{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d8e98dbe",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "d8e98dbe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import zipfile\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup envoirement"
      ],
      "metadata": {
        "id": "G6wFF41DtQ4b"
      },
      "id": "G6wFF41DtQ4b"
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device in uso:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGMl6Pu2rSYk",
        "outputId": "880ebfbc-6645-4f97-c05e-8427c503e2b8"
      },
      "id": "kGMl6Pu2rSYk",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device in uso: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbghhrvbrkuF",
        "outputId": "f96df1b8-0bda-43bc-a183-d7946a315214"
      },
      "id": "rbghhrvbrkuF",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data loading"
      ],
      "metadata": {
        "id": "UtEsNOoRtWa2"
      },
      "id": "UtEsNOoRtWa2"
    },
    {
      "cell_type": "code",
      "source": [
        "zip_path = \"/content/drive/MyDrive/AML_MistakeDetection_DATA/features/gopro/segments/1s/video/omnivore.zip\"\n",
        "extract_dir = \"/content/omnivore_extracted\"\n",
        "\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "    z.extractall(extract_dir)\n",
        "\n",
        "extract_dir = \"/content/omnivore_extracted/omnivore\"\n",
        "\n",
        "print(\"Extracted files:\", len(os.listdir(extract_dir)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfekzBNAtZ_S",
        "outputId": "6708b2fd-7f99-4528-e353-0294d7c43749"
      },
      "id": "EfekzBNAtZ_S",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted files: 384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"/content/drive/MyDrive/AML_MistakeDetection_DATA/annotation_json/complete_step_annotations.json\") as f:\n",
        "    annotations = json.load(f)"
      ],
      "metadata": {
        "id": "G7sq4nHfy-AZ"
      },
      "id": "G7sq4nHfy-AZ",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VideoFeatureDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.from_numpy(X).float()\n",
        "        self.y = torch.from_numpy(y).long()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]"
      ],
      "metadata": {
        "id": "e8N7MZHSuh4N"
      },
      "id": "e8N7MZHSuh4N",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_labels_for_npz(npz_file, annotations):\n",
        "    # es: \"10_3_360.mp4_1s_1s.npz\"\n",
        "    base = os.path.basename(npz_file)\n",
        "    activity, attempt = base.split(\"_\")[:2]  # \"10\", \"3\"\n",
        "    recording_id = f\"{activity}_{attempt}\"\n",
        "\n",
        "    # carica feature\n",
        "    data = np.load(npz_file)\n",
        "    arr = data[list(data.keys())[0]]  # shape (N, 400)\n",
        "    N = arr.shape[0]\n",
        "\n",
        "    labels = np.zeros(N, dtype=np.int64)  # default: no-error = 0\n",
        "\n",
        "    # trova annotation di questo recording\n",
        "    info = annotations[recording_id]\n",
        "    steps = info[\"steps\"]\n",
        "\n",
        "    # assegnazione label per ogni secondo\n",
        "    for step in steps:\n",
        "        has_error = int(step[\"has_errors\"])  # True→1, False→0\n",
        "        start = step[\"start_time\"]\n",
        "        end   = step[\"end_time\"]\n",
        "\n",
        "        if start == -1 or end == -1 or has_error == 0:\n",
        "            continue\n",
        "\n",
        "        for sec in range(int(start), int(end) + 1, 1):\n",
        "            sec_start = sec\n",
        "            sec_end   = sec + 1\n",
        "\n",
        "            # check overlap\n",
        "            if sec_start >= start and sec_end <= end:\n",
        "                labels[sec] = has_error\n",
        "\n",
        "    return arr, labels"
      ],
      "metadata": {
        "id": "TAUlHVSAykFa"
      },
      "id": "TAUlHVSAykFa",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_X = []\n",
        "all_y = []\n",
        "\n",
        "extract_dir = \"/content/omnivore_extracted/omnivore\"\n",
        "\n",
        "for f in sorted(os.listdir(extract_dir)):\n",
        "    if f.endswith(\".npz\"):\n",
        "        X, y = get_labels_for_npz(os.path.join(extract_dir, f), annotations)\n",
        "        all_X.append(X)\n",
        "        all_y.append(y)\n",
        "\n",
        "X = np.concatenate(all_X, axis=0)\n",
        "y = np.concatenate(all_y, axis=0)\n",
        "\n",
        "print(X.shape, y.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6VeoKhMuHuZ",
        "outputId": "fc55de05-7735-4aa7-af82-8accfe1d9694"
      },
      "id": "w6VeoKhMuHuZ",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(340320, 1024) (340320,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    shuffle=True,\n",
        ")"
      ],
      "metadata": {
        "id": "DZDtlZE8up0H"
      },
      "id": "DZDtlZE8up0H",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = VideoFeatureDataset(X_train, y_train)\n",
        "test_dataset  = VideoFeatureDataset(X_test,  y_test)"
      ],
      "metadata": {
        "id": "WprZqP5g-zGI"
      },
      "id": "WprZqP5g-zGI",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 512\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "FQ-eX8oeuwVg"
      },
      "id": "FQ-eX8oeuwVg",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP"
      ],
      "metadata": {
        "id": "SigNHU8ctaWJ"
      },
      "id": "SigNHU8ctaWJ"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MLPCapitainCook(nn.Module):\n",
        "    def __init__(self, in_features: int, p: float = 0.5) -> None:\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_features, 256)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p)       # Dropout layer con probabilità p\n",
        "        self.fc2 = nn.Linear(256, 1)       # Output logit (senza sigmoid)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)                # Applica Dropout solo in TRAIN\n",
        "        x = self.fc2(x)                    # Output logit\n",
        "        return x                           # no Sigmoid qui\n"
      ],
      "metadata": {
        "id": "O74QGf1Qo2sK"
      },
      "id": "O74QGf1Qo2sK",
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MLPCapitainCook(1024).to(device)"
      ],
      "metadata": {
        "id": "UWoFzaKgp-I_"
      },
      "id": "UWoFzaKgp-I_",
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.0001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
        "# count classi\n",
        "neg = (y == 1).sum()   # classe “1” diventa negativa\n",
        "pos = (y == 0).sum()   # classe “0” diventa positiva\n",
        "\n",
        "# pos_weight = quanto pesa la classe “positiva” = classe 0\n",
        "pos_weight_value = neg / pos\n",
        "pos_weight = torch.tensor([pos_weight_value], device=device)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "epochs = 50"
      ],
      "metadata": {
        "id": "6FdOuKJopr3m"
      },
      "id": "6FdOuKJopr3m",
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    # -------------------------\n",
        "    #        TRAIN\n",
        "    # -------------------------\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device).float()\n",
        "\n",
        "        outputs = model(inputs)            # [B, 1]\n",
        "        outputs = outputs.squeeze(1)       # [B]\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "    # -------------------------\n",
        "    #        EVAL\n",
        "    # -------------------------\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device).float()\n",
        "\n",
        "            outputs = model(inputs).squeeze(1)  # logits\n",
        "\n",
        "            # same loss as train\n",
        "            val_loss = criterion(outputs, labels)\n",
        "            total_val_loss += val_loss.item()\n",
        "\n",
        "            # convert logits → probabilities → binary predictions\n",
        "            probs = torch.sigmoid(outputs)\n",
        "            preds = (probs >= 0.5).long()\n",
        "\n",
        "            all_preds.append(preds.cpu())\n",
        "            all_targets.append(labels.cpu())\n",
        "\n",
        "    # concat\n",
        "    all_preds = torch.cat(all_preds).numpy()\n",
        "    all_targets = torch.cat(all_targets).numpy()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(test_loader)\n",
        "    acc = accuracy_score(all_targets, all_preds)\n",
        "    f1  = f1_score(all_targets, all_preds, zero_division=0)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} \"\n",
        "          f\"- Train Loss: {avg_train_loss:.4f} \"\n",
        "          f\"- Val Loss: {avg_val_loss:.4f} \"\n",
        "          f\"- Acc: {acc:.4f} \"\n",
        "          f\"- F1: {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "id": "4u_a6jPgq7OI",
        "outputId": "85a092e6-0b00-47bd-cd64-8c35bc46500a"
      },
      "id": "4u_a6jPgq7OI",
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50 - Train Loss: 0.2603 - Val Loss: 0.2469 - Acc: 0.7560 - F1: 0.0000\n",
            "Epoch 2/50 - Train Loss: 0.2432 - Val Loss: 0.2389 - Acc: 0.7566 - F1: 0.0055\n",
            "Epoch 3/50 - Train Loss: 0.2357 - Val Loss: 0.2312 - Acc: 0.7595 - F1: 0.0307\n",
            "Epoch 4/50 - Train Loss: 0.2283 - Val Loss: 0.2240 - Acc: 0.7658 - F1: 0.0830\n",
            "Epoch 5/50 - Train Loss: 0.2217 - Val Loss: 0.2177 - Acc: 0.7693 - F1: 0.1090\n",
            "Epoch 6/50 - Train Loss: 0.2159 - Val Loss: 0.2119 - Acc: 0.7737 - F1: 0.1427\n",
            "Epoch 7/50 - Train Loss: 0.2108 - Val Loss: 0.2076 - Acc: 0.7786 - F1: 0.1778\n",
            "Epoch 8/50 - Train Loss: 0.2061 - Val Loss: 0.2033 - Acc: 0.7834 - F1: 0.2119\n",
            "Epoch 9/50 - Train Loss: 0.2024 - Val Loss: 0.1995 - Acc: 0.7859 - F1: 0.2278\n",
            "Epoch 10/50 - Train Loss: 0.1985 - Val Loss: 0.1963 - Acc: 0.7926 - F1: 0.2738\n",
            "Epoch 11/50 - Train Loss: 0.1954 - Val Loss: 0.1934 - Acc: 0.7949 - F1: 0.2888\n",
            "Epoch 12/50 - Train Loss: 0.1924 - Val Loss: 0.1913 - Acc: 0.7936 - F1: 0.2778\n",
            "Epoch 13/50 - Train Loss: 0.1896 - Val Loss: 0.1886 - Acc: 0.7975 - F1: 0.3023\n",
            "Epoch 14/50 - Train Loss: 0.1872 - Val Loss: 0.1858 - Acc: 0.8018 - F1: 0.3276\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3046166210.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}