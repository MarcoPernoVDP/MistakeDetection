{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8e98dbe",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "d8e98dbe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import zipfile\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup environement"
      ],
      "metadata": {
        "id": "G6wFF41DtQ4b"
      },
      "id": "G6wFF41DtQ4b"
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device in uso:\", device)"
      ],
      "metadata": {
        "id": "kGMl6Pu2rSYk"
      },
      "id": "kGMl6Pu2rSYk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbghhrvbrkuF",
        "outputId": "759086f8-70e5-455a-cb1c-d2e2b42a993a"
      },
      "id": "rbghhrvbrkuF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data loading"
      ],
      "metadata": {
        "id": "UtEsNOoRtWa2"
      },
      "id": "UtEsNOoRtWa2"
    },
    {
      "cell_type": "code",
      "source": [
        "zip_path = \"/content/drive/MyDrive/AML_MistakeDetection_DATA/features/gopro/segments/1s/video/omnivore.zip\"\n",
        "extract_dir = \"/content/omnivore_extracted\"\n",
        "\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "    z.extractall(extract_dir)\n",
        "\n",
        "extract_dir = \"/content/omnivore_extracted/omnivore\"\n",
        "\n",
        "print(\"Extracted files:\", len(os.listdir(extract_dir)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfekzBNAtZ_S",
        "outputId": "df7f41c8-69d3-421e-e22a-acc737d48448"
      },
      "id": "EfekzBNAtZ_S",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted files: 384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"/content/drive/MyDrive/AML_MistakeDetection_DATA/annotation_json/complete_step_annotations.json\") as f:\n",
        "    annotations = json.load(f)"
      ],
      "metadata": {
        "id": "G7sq4nHfy-AZ"
      },
      "id": "G7sq4nHfy-AZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VideoFeatureDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.from_numpy(X).float()\n",
        "        self.y = torch.from_numpy(y).long()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]"
      ],
      "metadata": {
        "id": "e8N7MZHSuh4N"
      },
      "id": "e8N7MZHSuh4N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_labels_for_npz(npz_file, annotations):\n",
        "    # es: \"10_3_360.mp4_1s_1s.npz\"\n",
        "    base = os.path.basename(npz_file)\n",
        "    activity, attempt = base.split(\"_\")[:2]  # \"10\", \"3\"\n",
        "    recording_id = f\"{activity}_{attempt}\"\n",
        "\n",
        "    # carica feature\n",
        "    data = np.load(npz_file)\n",
        "    arr = data[list(data.keys())[0]]  # shape (N, 1024)\n",
        "    N = arr.shape[0]\n",
        "\n",
        "    labels = np.zeros(N, dtype=np.int64)  # default: normal = no-error = 0\n",
        "\n",
        "    # trova annotation di questo recording\n",
        "    info = annotations[recording_id]\n",
        "    steps = info[\"steps\"]\n",
        "\n",
        "    # assegnazione label per ogni secondo\n",
        "    for step in steps:\n",
        "        has_error = int(step[\"has_errors\"])  # True→1, False→0\n",
        "        start = step[\"start_time\"]\n",
        "        end   = step[\"end_time\"]\n",
        "\n",
        "        if start == -1 or end == -1 or has_error == 0:\n",
        "            continue\n",
        "\n",
        "        for sec in range(int(start), int(end) + 1, 1):\n",
        "            sec_start = sec\n",
        "            sec_end   = sec + 1\n",
        "\n",
        "            # check overlap\n",
        "            if sec_start >= start and sec_end <= end: # i secondi ai bordi avranno sempre il valore di default (norml = no-error = 0)\n",
        "                labels[sec] = has_error\n",
        "\n",
        "    return arr, labels"
      ],
      "metadata": {
        "id": "TAUlHVSAykFa"
      },
      "id": "TAUlHVSAykFa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_X = []\n",
        "all_y = []\n",
        "\n",
        "extract_dir = \"/content/omnivore_extracted/omnivore\"\n",
        "\n",
        "for f in sorted(os.listdir(extract_dir)):\n",
        "    if f.endswith(\".npz\"):\n",
        "        X, y = get_labels_for_npz(os.path.join(extract_dir, f), annotations)\n",
        "        all_X.append(X)\n",
        "        all_y.append(y)\n",
        "\n",
        "X = np.concatenate(all_X, axis=0)\n",
        "y = np.concatenate(all_y, axis=0)\n",
        "\n",
        "print(X.shape, y.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6VeoKhMuHuZ",
        "outputId": "0e6bff9c-7933-4fa4-d77a-e2d3972eecc0"
      },
      "id": "w6VeoKhMuHuZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(340320, 1024) (340320,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# \"Step (S)\" data splits (come usato nel paper CaptainCook4D)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "# implementare anche \"Recording (R)\" data splits (come fatto nel paper) per vederne differenze"
      ],
      "metadata": {
        "id": "DZDtlZE8up0H"
      },
      "id": "DZDtlZE8up0H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = VideoFeatureDataset(X_train, y_train)\n",
        "test_dataset  = VideoFeatureDataset(X_test,  y_test)"
      ],
      "metadata": {
        "id": "WprZqP5g-zGI"
      },
      "id": "WprZqP5g-zGI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 512\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "FQ-eX8oeuwVg"
      },
      "id": "FQ-eX8oeuwVg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP (Version 1)"
      ],
      "metadata": {
        "id": "SigNHU8ctaWJ"
      },
      "id": "SigNHU8ctaWJ"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MLPCapitainCook(nn.Module):\n",
        "    def __init__(self, in_features: int, p: float = 0.5) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features, 256)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p)       # Dropout layer con probabilità p\n",
        "        self.fc2 = nn.Linear(256, 1)       # Output logit (senza sigmoid)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)                # Applica Dropout solo in TRAIN\n",
        "        x = self.fc2(x)                    # Output logit\n",
        "        return x                           # no Sigmoid qui\n"
      ],
      "metadata": {
        "id": "O74QGf1Qo2sK"
      },
      "id": "O74QGf1Qo2sK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MLPCapitainCook(1024).to(device)"
      ],
      "metadata": {
        "id": "UWoFzaKgp-I_"
      },
      "id": "UWoFzaKgp-I_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.0001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
        "# count classi\n",
        "num_error = (y == 1).sum()   # classe “1” = \"error\"\n",
        "num_normal = (y == 0).sum()   # classe “0” = \"normal\"\n",
        "\n",
        "# pos_weight = quanto pesa la classe “positiva” = classe \"1\"\n",
        "#pos_weight_value = num_normal/num_error\n",
        "pos_weight_value = 1.5\n",
        "print(pos_weight_value)\n",
        "pos_weight = torch.tensor([pos_weight_value], device=device)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "epochs = 50"
      ],
      "metadata": {
        "id": "6FdOuKJopr3m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f888829f-fbfc-47d3-b01f-88e429b1457f"
      },
      "id": "6FdOuKJopr3m",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    # -------------------------\n",
        "    #        TRAIN\n",
        "    # -------------------------\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device).float()\n",
        "\n",
        "        outputs = model(inputs)            # [B, 1]\n",
        "        outputs = outputs.squeeze(1)       # [B]\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "    # -------------------------\n",
        "    #        EVAL\n",
        "    # -------------------------\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device).float()\n",
        "\n",
        "            outputs = model(inputs).squeeze(1)  # logits\n",
        "\n",
        "            # same loss as train\n",
        "            val_loss = criterion(outputs, labels)\n",
        "            total_val_loss += val_loss.item()\n",
        "\n",
        "            # convert logits → probabilities → binary predictions\n",
        "            probs = torch.sigmoid(outputs)\n",
        "            preds = (probs >= 0.5).long()\n",
        "\n",
        "            all_preds.append(preds.cpu())\n",
        "            all_targets.append(labels.cpu())\n",
        "\n",
        "    # concat\n",
        "    all_preds = torch.cat(all_preds).numpy()\n",
        "    all_targets = torch.cat(all_targets).numpy()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(test_loader)\n",
        "    acc = accuracy_score(all_targets, all_preds)\n",
        "    f1  = f1_score(all_targets, all_preds, zero_division=0)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} \"\n",
        "          f\"- Train Loss: {avg_train_loss:.4f} \"\n",
        "          f\"- Val Loss: {avg_val_loss:.4f} \"\n",
        "          f\"- Acc: {acc:.4f} \"\n",
        "          f\"- F1: {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4u_a6jPgq7OI",
        "outputId": "b0b8b41c-0c65-4729-92b1-8f51632af365"
      },
      "id": "4u_a6jPgq7OI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50 - Train Loss: 0.6710 - Val Loss: 0.6475 - Acc: 0.7680 - F1: 0.2537\n",
            "Epoch 2/50 - Train Loss: 0.6327 - Val Loss: 0.6185 - Acc: 0.7798 - F1: 0.3424\n",
            "Epoch 3/50 - Train Loss: 0.6049 - Val Loss: 0.5917 - Acc: 0.7903 - F1: 0.4095\n",
            "Epoch 4/50 - Train Loss: 0.5822 - Val Loss: 0.5715 - Acc: 0.7972 - F1: 0.4712\n",
            "Epoch 5/50 - Train Loss: 0.5638 - Val Loss: 0.5534 - Acc: 0.8049 - F1: 0.5094\n",
            "Epoch 6/50 - Train Loss: 0.5471 - Val Loss: 0.5384 - Acc: 0.8108 - F1: 0.5248\n",
            "Epoch 7/50 - Train Loss: 0.5331 - Val Loss: 0.5260 - Acc: 0.8173 - F1: 0.5480\n",
            "Epoch 8/50 - Train Loss: 0.5204 - Val Loss: 0.5156 - Acc: 0.8179 - F1: 0.5578\n",
            "Epoch 9/50 - Train Loss: 0.5097 - Val Loss: 0.5056 - Acc: 0.8226 - F1: 0.5754\n",
            "Epoch 10/50 - Train Loss: 0.4993 - Val Loss: 0.4971 - Acc: 0.8278 - F1: 0.5817\n",
            "Epoch 11/50 - Train Loss: 0.4901 - Val Loss: 0.4878 - Acc: 0.8301 - F1: 0.5992\n",
            "Epoch 12/50 - Train Loss: 0.4821 - Val Loss: 0.4823 - Acc: 0.8329 - F1: 0.6030\n",
            "Epoch 13/50 - Train Loss: 0.4750 - Val Loss: 0.4744 - Acc: 0.8342 - F1: 0.6142\n",
            "Epoch 14/50 - Train Loss: 0.4674 - Val Loss: 0.4697 - Acc: 0.8350 - F1: 0.6147\n",
            "Epoch 15/50 - Train Loss: 0.4600 - Val Loss: 0.4637 - Acc: 0.8386 - F1: 0.6189\n",
            "Epoch 16/50 - Train Loss: 0.4536 - Val Loss: 0.4585 - Acc: 0.8412 - F1: 0.6237\n",
            "Epoch 17/50 - Train Loss: 0.4479 - Val Loss: 0.4535 - Acc: 0.8412 - F1: 0.6372\n",
            "Epoch 18/50 - Train Loss: 0.4408 - Val Loss: 0.4477 - Acc: 0.8445 - F1: 0.6416\n",
            "Epoch 19/50 - Train Loss: 0.4358 - Val Loss: 0.4443 - Acc: 0.8460 - F1: 0.6380\n",
            "Epoch 20/50 - Train Loss: 0.4312 - Val Loss: 0.4399 - Acc: 0.8471 - F1: 0.6496\n",
            "Epoch 21/50 - Train Loss: 0.4267 - Val Loss: 0.4362 - Acc: 0.8500 - F1: 0.6544\n",
            "Epoch 22/50 - Train Loss: 0.4203 - Val Loss: 0.4347 - Acc: 0.8507 - F1: 0.6463\n",
            "Epoch 23/50 - Train Loss: 0.4167 - Val Loss: 0.4311 - Acc: 0.8521 - F1: 0.6507\n",
            "Epoch 24/50 - Train Loss: 0.4114 - Val Loss: 0.4273 - Acc: 0.8533 - F1: 0.6537\n",
            "Epoch 25/50 - Train Loss: 0.4076 - Val Loss: 0.4224 - Acc: 0.8531 - F1: 0.6648\n",
            "Epoch 26/50 - Train Loss: 0.4037 - Val Loss: 0.4208 - Acc: 0.8559 - F1: 0.6619\n",
            "Epoch 27/50 - Train Loss: 0.3991 - Val Loss: 0.4178 - Acc: 0.8558 - F1: 0.6671\n",
            "Epoch 28/50 - Train Loss: 0.3955 - Val Loss: 0.4145 - Acc: 0.8571 - F1: 0.6726\n",
            "Epoch 29/50 - Train Loss: 0.3921 - Val Loss: 0.4112 - Acc: 0.8576 - F1: 0.6765\n",
            "Epoch 30/50 - Train Loss: 0.3876 - Val Loss: 0.4113 - Acc: 0.8581 - F1: 0.6697\n",
            "Epoch 31/50 - Train Loss: 0.3837 - Val Loss: 0.4055 - Acc: 0.8600 - F1: 0.6827\n",
            "Epoch 32/50 - Train Loss: 0.3803 - Val Loss: 0.4025 - Acc: 0.8600 - F1: 0.6903\n",
            "Epoch 33/50 - Train Loss: 0.3763 - Val Loss: 0.4017 - Acc: 0.8615 - F1: 0.6886\n",
            "Epoch 34/50 - Train Loss: 0.3748 - Val Loss: 0.3983 - Acc: 0.8620 - F1: 0.6917\n",
            "Epoch 35/50 - Train Loss: 0.3713 - Val Loss: 0.3971 - Acc: 0.8618 - F1: 0.6902\n",
            "Epoch 36/50 - Train Loss: 0.3687 - Val Loss: 0.3948 - Acc: 0.8624 - F1: 0.6908\n",
            "Epoch 37/50 - Train Loss: 0.3649 - Val Loss: 0.3927 - Acc: 0.8632 - F1: 0.6979\n",
            "Epoch 38/50 - Train Loss: 0.3619 - Val Loss: 0.3921 - Acc: 0.8638 - F1: 0.6942\n",
            "Epoch 39/50 - Train Loss: 0.3591 - Val Loss: 0.3915 - Acc: 0.8646 - F1: 0.6910\n",
            "Epoch 40/50 - Train Loss: 0.3578 - Val Loss: 0.3882 - Acc: 0.8648 - F1: 0.6988\n",
            "Epoch 41/50 - Train Loss: 0.3545 - Val Loss: 0.3877 - Acc: 0.8663 - F1: 0.6988\n",
            "Epoch 42/50 - Train Loss: 0.3509 - Val Loss: 0.3864 - Acc: 0.8659 - F1: 0.6977\n",
            "Epoch 43/50 - Train Loss: 0.3493 - Val Loss: 0.3850 - Acc: 0.8674 - F1: 0.7001\n",
            "Epoch 44/50 - Train Loss: 0.3459 - Val Loss: 0.3831 - Acc: 0.8669 - F1: 0.7018\n",
            "Epoch 45/50 - Train Loss: 0.3418 - Val Loss: 0.3820 - Acc: 0.8677 - F1: 0.7031\n",
            "Epoch 46/50 - Train Loss: 0.3403 - Val Loss: 0.3825 - Acc: 0.8680 - F1: 0.6995\n",
            "Epoch 47/50 - Train Loss: 0.3389 - Val Loss: 0.3784 - Acc: 0.8698 - F1: 0.7097\n",
            "Epoch 48/50 - Train Loss: 0.3373 - Val Loss: 0.3777 - Acc: 0.8679 - F1: 0.7102\n",
            "Epoch 49/50 - Train Loss: 0.3335 - Val Loss: 0.3764 - Acc: 0.8692 - F1: 0.7094\n",
            "Epoch 50/50 - Train Loss: 0.3309 - Val Loss: 0.3773 - Acc: 0.8698 - F1: 0.7081\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}