\section{Method}
\label{sec:method}

We present our approach for mistake detection in procedural activities through two tasks: reproducing supervised baselines (Task 1) and developing a multimodal graph-based extension (Task 2).

\subsection{Task 1: Supervised Error Recognition Baseline}
\label{subsec:task1}
We reproduce the CaptainCook4D benchmark~\cite{captaincook4d} baseline, formulated as binary video classification to predict whether a segment contains an error based solely on visual cues.

We evaluate three temporal aggregation architectures on pre-extracted visual features:
\begin{enumerate}
    \item \textbf{MLP Baseline:} Multi-Layer Perceptron with LayerNorm, followed by Linear, ReLU, and Dropout ($p=0.5$) layers mapping features to mistake probability.
    \item \textbf{Transformer Baseline:} 2 encoder layers with 8 attention heads and feature dimension 1024. Uses sinusoidal Positional Encoding and global mean pooling.
    \item \textbf{LSTM Baseline:} 2 stacked LSTM layers (hidden size 1024) using final hidden state $h_T$ for classification.
\end{enumerate}

These baselines detect visually obvious errors but struggle with semantic deviations requiring procedural knowledge.

\subsection{Task 2: Multimodal Mistake Detection Extension}
\label{subsec:task2}
We propose a multimodal architecture aligning visual execution with textual recipe steps through four stages: Feature Extraction, Visual Step Segmentation, Multimodal Alignment, and Graph-based Classification.

\subsubsection{Feature Extraction and Encoding}
We employ \textbf{EgoVLP}~\cite{egovlp} for video and text encoding, leveraging its egocentric dataset pre-training:
\begin{itemize}
    \item \textbf{Video Features:} EgoVLP video encoder processes 1-second clips, yielding visual embeddings $V \in \mathbb{R}^{T_v \times D}$, where $D=768$.
    \item \textbf{Text Features:} EgoVLP text encoder produces textual embeddings $T \in \mathbb{R}^{N_s \times D}$ for $N_s$ recipe steps.
\end{itemize}

\subsubsection{Visual Step Segmentation (HiERO)}
\textbf{HiERO} performs zero-shot hierarchical localization via agglomerative clustering on frame-level embeddings $V$, grouping visually similar clips into segments. This generates visual step proposals $V_{step} \in \mathbb{R}^{M \times D}$ representing detected actions.

\subsubsection{Multimodal Alignment (Hungarian Matching)}
We associate visual segments with textual steps via optimal assignment. Computing a cost matrix $C$ based on cosine distance between $V_{step}$ and $T$, the \textbf{Hungarian Algorithm} minimizes total matching cost, producing pairs $\{(v_i, t_j)\}$ that assign visual realizations to recipe instructions.

\subsubsection{Graph-Based Error Detection (DAGNN)}
We model the aligned procedure as a directed graph where nodes represent recipe steps with multimodal features $x_i \in \mathbb{R}^{1536}$ (concatenating text embedding $t_j$ and aligned visual embedding $v_i$). Edges are defined sequentially ($i \rightarrow i+1$) to represent expected temporal flow.

The DAGNN architecture consists of: (1) projection block (Linear → BatchNorm → ReLU) reducing dimensionality from 1536 to 128, (2) DAGNN layer with LayerNorm and Dropout ($p=0.5$) propagating context across steps, (3) Global Mean Pooling aggregating node features, (4) MLP classifier outputting binary prediction (0: Correct, 1: Mistake).

This formulation lets the network reason over procedural dependencies and detect structural deviations (e.g., omitted steps, incorrect order) that pure video-based classifiers overlook.

