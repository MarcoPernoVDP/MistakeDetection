%-------------------------------------------------------------------------
\section{Related Work}
\label{sec:related}

Our work spans egocentric video understanding, procedural activity recognition, multimodal learning, and graph neural networks.

\subsection{Egocentric Video Understanding}
Large-scale egocentric datasets (EPIC-KITCHENS, Ego4D, Captain Cook 4D~\cite{captaincook4d}) enable action recognition and temporal segmentation. Vision-language models like EgoVLP~\cite{egovlp} learn joint embeddings through contrastive learning, providing strong zero-shot capabilities. We leverage EgoVLP for semantic visual features aligned with recipe instructions.

\subsection{Procedural Activity Recognition}
Procedural understanding requires modeling causal and temporal constraints. HiERO~\cite{hiero} proposes weakly supervised step discovery via feature clustering and temporal coherence. Our experiments show zero-shot localization quality critically impacts error detection. Our graph-based approach explicitly encodes recipe structure through multimodal alignment.

Our approach unifies graph neural networks, multimodal embeddings, and optimal matching for procedural error detection respecting causal structure.
