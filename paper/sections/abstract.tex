%%%%%%%%% ABSTRACT
\begin{abstract}
   Detecting procedural mistakes in egocentric cooking videos is crucial for developing intelligent assistive systems and improving instructional content. However, this task is challenging due to the semantic gap between visual observations and recipe instructions, as well as the inherent temporal and causal structure of cooking procedures. We propose a novel approach based on Directed Acyclic Graph Neural Networks (DAGNNs) that explicitly models the procedural structure of recipes while integrating multimodal information from both video and text. Our method leverages EgoVLP visual embeddings and recipe text embeddings, aligned through Hungarian matching to establish semantic correspondences between observed actions and recipe steps. The DAGNN architecture processes the resulting multimodal graph representation, where nodes encode both visual and textual features, preserving the causal dependencies inherent in cooking procedures. We introduce a learnable projection layer that maps the concatenated 1536-dimensional embeddings to a compact 256-dimensional space before graph convolution operations. Experiments on the Captain Cook 4D dataset reveal significant challenges in this task. Our approach achieves performance below that of a dummy baseline system, likely due to the limited dataset size and the inherent limitations of the zero-shot unsupervised HiERO model used for video step embeddings generation. These results highlight the difficulty of procedural error detection and suggest that supervised approaches with larger datasets and improved step localization methods are necessary for achieving robust performance in this challenging domain.
\end{abstract}
