%%%%%%%%% ABSTRACT
\begin{abstract}
   Detecting procedural mistakes in egocentric cooking videos is crucial for developing intelligent assistive systems and improving instructional content. However, this task is challenging due to the semantic gap between visual observations and recipe instructions, as well as the inherent temporal and causal structure of cooking procedures. We propose a novel approach based on Directed Acyclic Graph Neural Networks (DAGNNs) that explicitly models the procedural structure of recipes while integrating multimodal information from both video and text. Our method leverages EgoVLP visual embeddings and recipe text embeddings, aligned through Hungarian matching to establish semantic correspondences between observed actions and recipe steps. The DAGNN architecture processes the resulting multimodal graph representation, where nodes encode both visual and textual features, preserving the causal dependencies inherent in cooking procedures. We introduce a learnable projection layer that maps the concatenated 1536-dimensional embeddings to a compact 256-dimensional space before graph convolution operations. Extensive experiments on the Captain Cook 4D dataset demonstrate the effectiveness of our approach in detecting procedural errors, achieving promising results in binary mistake classification. Our method provides an interpretable framework for understanding action-recipe alignments and identifying deviations from correct procedures, paving the way for more robust procedural error detection systems.
\end{abstract}
