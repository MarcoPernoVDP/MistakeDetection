%-------------------------------------------------------------------------
\section{Conclusion}
\label{sec:conclusion}

This work investigated procedural error detection in egocentric cooking videos through supervised baselines (Task 1) and a novel graph-based multimodal framework (Task 2).

For Task 1, we successfully reproduced Captain Cook 4D benchmark results across MLP and Transformer architectures, uncovering threshold documentation inconsistencies. Our LSTM baseline (F1=0.511) demonstrated intermediate performance between MLPs (F1=0.243) and Transformers (F1=0.554), confirming that temporal modeling benefits error detection, though self-attention mechanisms better capture long-range dependencies.

For Task 2, we proposed a DAGNN framework modeling recipe structure through graph convolution while integrating EgoVLP visual embeddings and recipe text via Hungarian matching. Comparing against a sequential Transformer baseline using the same HiERO-extracted features, DAGNN achieved modest improvements (F1: 0.578 vs 0.503 for 8 steps; 0.587 vs 0.547 for max+1), demonstrating that explicit graph structure provides incremental benefits. However, high standard deviations (F1 std > 0.15) reveal substantial variability across recipe contexts.

Our analysis identified three critical bottlenecks: (\textit{i}) \textbf{step localization quality}—zero-shot HiERO produces noisy segmentation with ambiguous visual-textual alignments (Figure~\ref{fig:hungarian_similarity_matrix}); (\textit{ii}) \textbf{limited training data}—few videos per LOO fold is insufficient for robust graph representations; (\textit{iii}) \textbf{modality gap}—simple feature concatenation fails to bridge visual-textual domains with limited data.

Future work could explore: (\textit{1}) using supervised step localization methods instead of zero-shot approaches; (\textit{2}) evaluating on larger datasets to better assess model generalization; (\textit{3}) experimenting with different feature fusion strategies beyond simple concatenation. While our graph-based approach did not achieve breakthroughs, we provide transparent analysis of failure modes. We hope this honest reporting of both successes and challenges contributes to understanding the complexity of procedural error detection in egocentric videos.

