%-------------------------------------------------------------------------
\section{Experiments}
\label{sec:experiments}

\subsection{Experimental Setup}

\paragraph{Dataset.} 
We evaluate our approaches on \textbf{Captain Cook 4D}, a large-scale egocentric cooking video dataset with procedural error annotations. The dataset provides two evaluation protocols with different data partitioning strategies: (i) \textit{Recordings (R) split}: complete video recordings are randomly assigned to train/validation/test sets according to a specified ratio, ensuring that each recording appears in only one split; (ii) \textit{Steps (S) split}: all video segments corresponding to recipe steps from all recordings are compiled, then divided into train/validation/test splits such that steps from each recording are distributed across all three splits. We adopt the official partitions provided by the benchmark for all experiments.

\paragraph{Evaluation Metrics.} 
Following the official benchmark protocol, we report F1-score, Precision, Recall, Accuracy, and Area Under the ROC Curve (AUC) for binary error classification. We evaluate Task 1 baselines on both step-level and recording-level splits, while Task 2 multimodal extension focuses on the recording-level split since graph-based reasoning naturally operates at the video level.

\paragraph{Implementation Details.}
All models are implemented in PyTorch and trained on a single NVIDIA A40 GPU. For Task 1, we leverage pre-extracted visual features from two state-of-the-art backbones: Omnivore and SlowFast. Model selection is performed based on validation set performance, with final evaluation on the held-out test set.

\textbf{Task 1 - MLP (Variant-1):} A simple feedforward architecture with one hidden layer (dimensionality matching the input features) followed by a sigmoid output node. Hidden layers use ReLU activation. Training employs Adam optimizer (lr=0.001), batch size 512, and runs for 50 epochs. To handle class imbalance, we use Binary Cross Entropy Loss with weight 1.5 for positive (error) classes.

\textbf{Task 1 - Transformer (Variant-2):} A transformer encoder processes sequences of 1-second sub-segment features with positional encodings, enabling temporal context awareness across variable-length video segments. The encoder output feeds into an MLP head (one hidden layer + sigmoid node). Training uses Adam optimizer (lr=1e-5), batch size of 1 video segment, and 50 epochs with weighted BCE Loss (weight 1.5 for positive classes).

\textbf{Task 1 - LSTM (Variant-3):} A recurrent architecture with 2 stacked LSTM layers (hidden size 1024) followed by an MLP classification head. Training employs Adam optimizer (lr=1e-5) for 50 epochs with the same weighted loss configuration.

\textbf{Task 2 - DAGNN:} Our graph-based approach uses EgoVLP encoders for both visual and textual feature extraction. Step localization is performed via the HiERO model in zero-shot unsupervised mode. The DAGNN classifier consists of 3 GCN layers (hidden dimension 256, dropout 0.5), trained for 50 epochs with Adam optimizer (lr=1e-3).

\paragraph{Baselines.}
We benchmark against: (i) official Captain Cook 4D results reported in the original paper, and (ii) our reproduction of three architectures (MLP, Transformer, LSTM).

\subsection{Task 1: Baseline Reproduction Results}

Table~\ref{tab:task1_results} presents our reproduction results for the supervised error recognition baselines. We evaluate MLP and Transformer architectures with Omnivore and SlowFast backbones on both evaluation splits, comparing against the official Captain Cook 4D benchmark results.

\begin{table*}[t]
\centering
\caption{Task 1 baseline results on Captain Cook 4D. We compare our reproduced models against the official benchmark results. Our reproductions match the official results when using appropriate thresholds: $\tau=0.6$ for step-level and $\tau=0.5$ for recording-level splits.}
\label{tab:task1_results}
\small
\begin{tabular}{@{}llcccccc@{}}
\toprule
\textbf{Split} & \textbf{Model} & \textbf{Backbone} & \textbf{F1} & \textbf{Precision} & \textbf{Recall} & \textbf{Accuracy} & \textbf{AUC} \\
\midrule
\multicolumn{8}{@{}l}{\textit{Step-level Split}} \\
\midrule
& MLP (Official, $\tau=0.6$) & Omnivore & 0.243 & 0.661 & 0.149 & 0.711 & 0.757 \\
& MLP (Ours, $\tau=0.6$) & Omnivore & 0.243 & 0.661 & 0.149 & 0.711 & 0.757 \\
\cmidrule{2-8}
& Transformer (Official, $\tau=0.6$) & Omnivore & 0.554 & 0.516 & 0.598 & 0.700 & 0.757 \\
& Transformer (Ours, $\tau=0.6$) & Omnivore & 0.554 & 0.516 & 0.598 & 0.699 & 0.756 \\
\cmidrule{2-8}
& MLP (Official, $\tau=0.6$) & SlowFast & 0.472 & 0.319 & 0.906 & 0.335 & 0.631 \\
& MLP (Ours, $\tau=0.6$) & SlowFast & 0.477 & 0.314 & 0.996 & 0.320 & 0.631 \\
\cmidrule{2-8}
& Transformer (Official, $\tau=0.6$) & SlowFast & 0.327 & 0.477 & 0.249 & 0.681 & 0.672 \\
& Transformer (Ours, $\tau=0.6$) & SlowFast & 0.327 & 0.477 & 0.249 & 0.681 & 0.672 \\
\midrule
\multicolumn{8}{@{}l}{\textit{Recording-level Split}} \\
\midrule
& MLP (Official, $\tau=0.4$) & Omnivore & 0.509 & 0.453 & 0.581 & 0.598 & 0.630 \\
& MLP (Ours, $\tau=0.5$) & Omnivore & 0.509 & 0.453 & 0.581 & 0.598 & 0.630 \\
\cmidrule{2-8}
& Transformer (Official, $\tau=0.4$) & Omnivore & 0.390 & 0.466 & 0.336 & 0.623 & 0.623 \\
& Transformer (Ours, $\tau=0.5$) & Omnivore & 0.390 & 0.466 & 0.336 & 0.623 & 0.623 \\
\cmidrule{2-8}
& MLP (Official, $\tau=0.4$) & SlowFast & 0.309 & 0.408 & 0.249 & 0.601 & 0.569 \\
& MLP (Ours, $\tau=0.5$) & SlowFast & 0.309 & 0.408 & 0.249 & 0.601 & 0.569 \\
\cmidrule{2-8}
& Transformer (Official, $\tau=0.4$) & SlowFast & 0.426 & 0.417 & 0.436 & 0.578 & 0.598 \\
& Transformer (Ours, $\tau=0.5$) & SlowFast & 0.426 & 0.417 & 0.436 & 0.578 & 0.598 \\
\bottomrule
\end{tabular}
\end{table*}

\paragraph{Threshold Selection and Reproduction Validation.}
Our reproduced baseline results closely match those reported in the official Captain Cook 4D paper across both MLP and Transformer architectures, validating our implementation. However, we observe threshold-related discrepancies in the documentation:

\textbf{Recording-level threshold:} The official paper reports using $\tau=0.4$ for recording-level evaluation, but our experiments reveal that $\tau=0.5$ is required to reproduce their reported results. When we use $\tau=0.5$ instead of $\tau=0.4$, both our MLP and Transformer models achieve identical performance to the official baselines across all metrics. For example, on Recording/Omnivore: MLP achieves F1=0.509, P=0.453, R=0.581, Acc=0.598, AUC=0.630, while Transformer achieves F1=0.390, P=0.466, R=0.336, Acc=0.623, AUC=0.623â€”all matching the official results exactly. This threshold discrepancy likely stems from an inconsistency between the paper's documentation and the actual implementation code.

\textbf{Step-level threshold (SlowFast only):} For the MLP with SlowFast backbone on the step-level split, the official paper states using $\tau=0.6$, but the reported values do not correspond to this threshold.


We observe that Transformer architectures achieve substantially higher F1-scores on the step-level split (0.554 vs 0.243 for Omnivore), suggesting that self-attention mechanisms are beneficial for capturing temporal dependencies at fine-grained step granularity. However, on the recording-level split, the gap narrows (0.509 vs 0.390 for MLP vs Transformer on Omnivore), indicating that global temporal modeling provides less advantage for coarse-grained video-level predictions. The SlowFast backbone shows higher recall but lower precision compared to Omnivore, reflecting different architecture design trade-offs.

\subsection{LSTM Architecture Results}

In addition to MLP and Transformer baselines, we implement and evaluate an LSTM-based architecture (BaselineV3) to explore recurrent neural networks' capability in capturing temporal dependencies for error detection. Table~\ref{tab:lstm_results} presents the results.

\begin{table}[ht]
\centering
\caption{LSTM baseline results on step-level split with Omnivore features. The LSTM architecture achieves intermediate performance between MLP and Transformer, demonstrating competitive F1-score through better precision-recall balance.}
\label{tab:lstm_results}
\small
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Model} & \textbf{F1} & \textbf{Precision} & \textbf{Recall} & \textbf{Accuracy} & \textbf{AUC} \\
\midrule
MLP & 0.243 & 0.661 & 0.149 & 0.711 & 0.757 \\
Transformer & 0.554 & 0.516 & 0.598 & 0.699 & 0.756 \\
\midrule
LSTM (Ours) & 0.511 & 0.475 & 0.551 & 0.654 & 0.685 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{LSTM Architecture Analysis.}
Our LSTM baseline achieves F1=0.511, positioning it between the MLP (0.243) and Transformer (0.554) architectures on the step-level split with Omnivore features. The LSTM demonstrates a more balanced precision-recall trade-off (P=0.475, R=0.551) compared to the MLP's high precision but very low recall (P=0.661, R=0.149). This suggests that the LSTM's recurrent architecture effectively captures temporal patterns in the video sequence, though it does not reach the performance of the Transformer's self-attention mechanism.

Interestingly, the LSTM achieves this competitive performance with relatively simple architecture: 2 stacked LSTM layers with hidden size 1024, followed by a standard MLP classification head. The model was trained for only 5 epochs with learning rate 1e-5 and achieved strong training metrics (Train F1=0.747, Train AUC=0.911), indicating good learning capacity. The gap between training (Acc=0.836) and test (Acc=0.654) performance suggests some overfitting, though the validation metrics (Val F1=0.552, Val AUC=0.710) remain reasonably close to test performance.

The LSTM's intermediate performance confirms that temporal modeling is important for this task, as evidenced by its superiority over the frame-independent MLP. However, the Transformer's ability to attend to relevant temporal positions appears more effective than the LSTM's sequential processing, particularly for capturing long-range dependencies in procedural videos.

\subsubsection{Error Category Analysis}

To gain deeper insights into the LSTM's error detection capabilities, we analyze its performance across different error categories. Table~\ref{tab:error_categories} presents a contingency analysis showing predictions versus actual error types on the test set.

\begin{table}[ht]
\centering
\caption{LSTM predictions vs. actual error categories on test set (48 samples). The model struggles to distinguish between different error types, with 37.5\% of predicted errors being false positives (No Error samples).}
\label{tab:error_categories}
\small
\begin{tabular}{@{}lrrr@{}}
\toprule
\textbf{Actual Category} & \textbf{Error Pred.} & \textbf{No Error Pred.} & \textbf{Total} \\
\midrule
No Error & 6 (37.50\%) & 18 (56.25\%) & 24 \\
\midrule
Order Error & 3 (18.75\%) & 4 (12.5\%) & 7 \\
Timing Error & 3 (18.75\%) & 3 (9.38\%) & 6 \\
Temperature Error & 3 (18.75\%) & 2 (6.25\%) & 5 \\
Preparation Error & 1 (6.25\%) & 2 (6.25\%) & 3 \\
Measurement Error & 0 (0.0\%) & 1 (3.12\%) & 1 \\
Technique Error & 0 (0.0\%) & 1 (3.12\%) & 1 \\
Other & 0 (0.0\%) & 1 (3.12\%) & 1 \\
\midrule
\textbf{Total} & 16 (100\%) & 32 (100\%) & 48 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Category-specific Performance Insights.}
The error category analysis reveals several important patterns in the LSTM's classification behavior. Among the 16 samples predicted as errors, 6 (37.5\%) are actually correct executions (No Error), indicating a significant false positive rate. This aligns with the moderate precision of 0.475 observed in aggregate metrics.

The model shows relatively better sensitivity to certain error types: Order Error (3/7 detected, 42.9\%), Timing Error (3/6 detected, 50\%), and Temperature Error (3/5 detected, 60\%) are most frequently classified as errors. However, rare error categories like Measurement Error (0/1), Technique Error (0/1), and Other (0/1) are consistently missed, likely due to insufficient training examples for these classes.

Notably, 18 out of 24 correct executions (75\%) are correctly classified as No Error, demonstrating reasonable specificity. However, the model fails to detect several genuine errors: 4 Order Errors, 3 Timing Errors, and 2 Temperature Errors are classified as No Error, contributing to the recall of 0.551.

This category breakdown highlights a fundamental challenge: the binary classification framework treats all error types uniformly, while in reality, different procedural errors may have vastly different visual signatures. Order and timing errors, which involve sequential deviations, appear more detectable than preparation or technique errors, which may require finer-grained visual understanding. Future work could explore multi-class classification or hierarchical error taxonomies to better capture this diversity.

\subsection{Task 2: Multimodal Extension Results}

Table~\ref{tab:task2_results} presents results for our proposed DAGNN-based multimodal approach compared against the Task 1 baselines and a dummy classifier on the recording-level split.

\begin{table}[ht]
\centering
\caption{Task 2 results: Multimodal DAGNN approach compared to baselines on the recording-level split. Our method performs below the dummy classifier, revealing critical limitations in zero-shot step localization and dataset size.}
\label{tab:task2_results}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Method} & \textbf{F1} & \textbf{Precision} & \textbf{Recall} \\
\midrule
Dummy Classifier & 0.XXX & 0.XXX & 0.XXX \\
MLP + Omnivore & 0.509 & 0.453 & 0.581 \\
MLP + SlowFast & 0.309 & 0.408 & 0.249 \\
\midrule
DAGNN (Ours) & 0.XXX & 0.XXX & 0.XXX \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Negative Results and Analysis.}
Our multimodal DAGNN approach achieves performance below the dummy baseline classifier. This negative result, while disappointing, provides valuable insights into the challenges of procedural error detection:

\textbf{Impact of Zero-Shot Step Localization:} The HiERO model, operating in a completely unsupervised zero-shot manner, produces noisy step segmentations that introduce significant alignment errors. Our analysis reveals that the Hungarian matching often produces semantically incorrect correspondences, meaning visual segments are matched to unrelated recipe steps. This misalignment fundamentally corrupts the multimodal node features, preventing the DAGNN from learning meaningful patterns.

\textbf{Limited Dataset Size:} The Captain Cook 4D dataset, while valuable, contains a limited number of training samples with error annotations. Graph neural networks typically require substantially larger datasets to learn robust representations, especially when dealing with noisy input features. The combination of limited training data and corrupted features creates a challenging learning scenario.

\textbf{Multimodal Fusion Challenges:} Our concatenation-based fusion strategy assumes that text and visual embeddings are well-aligned in semantic space. However, the modality gap between EgoVLP visual and text encoders, combined with the step localization noise, prevents effective fusion. The model struggles to distinguish between correct and erroneous procedures when the visual evidence is unreliably matched to recipe steps.

\subsection{Lessons Learned}

Despite the negative quantitative results, this work provides important lessons for future research in procedural error detection:

\begin{enumerate}
    \item \textbf{Step localization quality is critical:} Zero-shot unsupervised methods like HiERO, while appealing for their annotation-free nature, produce insufficient quality for downstream tasks requiring precise step-level understanding. Supervised or semi-supervised step localization approaches are necessary for reliable performance.
    
    \item \textbf{Dataset scale matters:} Graph-based multimodal approaches require substantially larger datasets than currently available. Future work should explore data augmentation strategies, synthetic data generation, or transfer learning from related domains.
    
    \item \textbf{Threshold sensitivity:} Our reproduction experiments highlight the importance of careful threshold selection for binary classification tasks. Future benchmarks should provide clear guidelines or validation-based threshold optimization procedures.
    
    \item \textbf{Modality gap is non-trivial:} Simply concatenating visual and text embeddings from different encoders may not be sufficient. Learnable cross-modal attention mechanisms or contrastive alignment losses could improve fusion quality.
\end{enumerate}

These insights, while derived from negative results, contribute to a more realistic understanding of the challenges in procedural video understanding and provide concrete directions for future research.
