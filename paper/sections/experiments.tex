%-------------------------------------------------------------------------
\section{Experiments}
\label{sec:experiments}

\subsection{Experimental Setup}

\paragraph{Dataset.} 
We evaluate our approaches on \textbf{Captain Cook 4D}, a large-scale egocentric cooking video dataset with procedural error annotations. The dataset provides two evaluation protocols with different data partitioning strategies: (i) \textit{Recordings (R) split}: complete video recordings are randomly assigned to train/validation/test sets according to a specified ratio, ensuring that each recording appears in only one split; (ii) \textit{Steps (S) split}: all video segments corresponding to recipe steps from all recordings are compiled, then divided into train/validation/test splits such that steps from each recording are distributed across all three splits. We adopt the official partitions provided by the benchmark for all experiments.

\paragraph{Evaluation Metrics.} 
Following the official benchmark protocol, we report F1-score, Precision, Recall, Accuracy, and Area Under the ROC Curve (AUC) for binary error classification. We evaluate Task 1 baselines on both step-level and recording-level splits, while Task 2 multimodal extension focuses on the recording-level split since graph-based reasoning naturally operates at the video level.

\paragraph{Implementation Details.}
All models are implemented in PyTorch and trained on a single NVIDIA A40 GPU. For Task 1, we leverage pre-extracted visual features from two state-of-the-art backbones: Omnivore and SlowFast. Model selection is performed based on validation set performance, with final evaluation on the held-out test set.

\textbf{Task 1 - MLP (Variant-1):} A simple feedforward architecture with one hidden layer (dimensionality matching the input features) followed by a sigmoid output node. Hidden layers use ReLU activation. Training employs Adam optimizer (lr=0.001), batch size 512, and runs for 50 epochs. To handle class imbalance, we use Binary Cross Entropy Loss with weight 1.5 for positive (error) classes.

\textbf{Task 1 - Transformer (Variant-2):} A transformer encoder processes sequences of 1-second sub-segment features with positional encodings, enabling temporal context awareness across variable-length video segments. The encoder output feeds into an MLP head (one hidden layer + sigmoid node). Training uses Adam optimizer (lr=1e-5), batch size of 1 video segment, and 50 epochs with weighted BCE Loss (weight 1.5 for positive classes).

\textbf{Task 1 - LSTM (Variant-3):} A recurrent architecture with 2 stacked LSTM layers (hidden size 1024) followed by an MLP classification head. Training employs Adam optimizer (lr=1e-5) for 50 epochs with the same weighted loss configuration.

\textbf{Task 2 - DAGNN:} Our graph-based approach uses EgoVLP encoders for both visual and textual feature extraction. Step localization is performed via the HiERO model in zero-shot unsupervised mode. The DAGNN classifier consists of 3 GCN layers (hidden dimension 256, dropout 0.5), trained for 50 epochs with Adam optimizer (lr=1e-3).

\paragraph{Baselines.}
We benchmark against: (i) official Captain Cook 4D results reported in the original paper, and (ii) our reproduction of three architectures (MLP, Transformer, LSTM).

\subsection{Task 1: Baseline Reproduction Results}

Table~\ref{tab:task1_results} presents our reproduction results for the supervised error recognition baselines. We evaluate MLP and Transformer architectures with Omnivore and SlowFast backbones on both evaluation splits, comparing against the official Captain Cook 4D benchmark results.

\begin{table*}[t]
\centering
\caption{Task 1 baseline results on Captain Cook 4D. We compare our reproduced models against the official benchmark results. Our reproductions match the official results when using appropriate thresholds: $\tau=0.6$ for step-level and $\tau=0.5$ for recording-level splits.}
\label{tab:task1_results}
\small
\begin{tabular}{@{}llcccccc@{}}
\toprule
\textbf{Split} & \textbf{Model} & \textbf{Backbone} & \textbf{F1} & \textbf{Precision} & \textbf{Recall} & \textbf{Accuracy} & \textbf{AUC} \\
\midrule
\multicolumn{8}{@{}l}{\textit{Step-level Split}} \\
\midrule
& MLP (Official, $\tau=0.6$) & Omnivore & 0.243 & 0.661 & 0.149 & 0.711 & 0.757 \\
& MLP (Ours, $\tau=0.6$) & Omnivore & 0.243 & 0.661 & 0.149 & 0.711 & 0.757 \\
\cmidrule{2-8}
& Transformer (Official, $\tau=0.6$) & Omnivore & 0.554 & 0.516 & 0.598 & 0.700 & 0.757 \\
& Transformer (Ours, $\tau=0.6$) & Omnivore & 0.554 & 0.516 & 0.598 & 0.699 & 0.756 \\
\cmidrule{2-8}
& MLP (Official, $\tau=0.6$) & SlowFast & 0.472 & 0.319 & 0.906 & 0.335 & 0.631 \\
& MLP (Ours, $\tau=0.6$) & SlowFast & 0.477 & 0.314 & 0.996 & 0.320 & 0.631 \\
\cmidrule{2-8}
& Transformer (Official, $\tau=0.6$) & SlowFast & 0.327 & 0.477 & 0.249 & 0.681 & 0.672 \\
& Transformer (Ours, $\tau=0.6$) & SlowFast & 0.327 & 0.477 & 0.249 & 0.681 & 0.672 \\
\midrule
\multicolumn{8}{@{}l}{\textit{Recording-level Split}} \\
\midrule
& MLP (Official, $\tau=0.4$) & Omnivore & 0.509 & 0.453 & 0.581 & 0.598 & 0.630 \\
& MLP (Ours, $\tau=0.5$) & Omnivore & 0.509 & 0.453 & 0.581 & 0.598 & 0.630 \\
\cmidrule{2-8}
& Transformer (Official, $\tau=0.4$) & Omnivore & 0.390 & 0.466 & 0.336 & 0.623 & 0.623 \\
& Transformer (Ours, $\tau=0.5$) & Omnivore & 0.390 & 0.466 & 0.336 & 0.623 & 0.623 \\
\cmidrule{2-8}
& MLP (Official, $\tau=0.4$) & SlowFast & 0.309 & 0.408 & 0.249 & 0.601 & 0.569 \\
& MLP (Ours, $\tau=0.5$) & SlowFast & 0.309 & 0.408 & 0.249 & 0.601 & 0.569 \\
\cmidrule{2-8}
& Transformer (Official, $\tau=0.4$) & SlowFast & 0.426 & 0.417 & 0.436 & 0.578 & 0.598 \\
& Transformer (Ours, $\tau=0.5$) & SlowFast & 0.426 & 0.417 & 0.436 & 0.578 & 0.598 \\
\bottomrule
\end{tabular}
\end{table*}

\paragraph{Threshold Selection and Reproduction Validation.}
Our reproduced baseline results closely match those reported in the official Captain Cook 4D paper across both MLP and Transformer architectures, validating our implementation. However, we observe threshold-related discrepancies in the documentation:

\textbf{Recording-level threshold:} The official paper reports using $\tau=0.4$ for recording-level evaluation, but our experiments reveal that $\tau=0.5$ is required to reproduce their reported results. When we use $\tau=0.5$ instead of $\tau=0.4$, both our MLP and Transformer models achieve identical performance to the official baselines across all metrics. For example, on Recording/Omnivore: MLP achieves F1=0.509, P=0.453, R=0.581, Acc=0.598, AUC=0.630, while Transformer achieves F1=0.390, P=0.466, R=0.336, Acc=0.623, AUC=0.623—all matching the official results exactly. This threshold discrepancy likely stems from an inconsistency between the paper's documentation and the actual implementation code.

\textbf{Step-level threshold (SlowFast only):} For the MLP with SlowFast backbone on the step-level split, the official paper states using $\tau=0.6$, but the reported values do not correspond to this threshold.


We observe that Transformer architectures achieve substantially higher F1-scores on the step-level split (0.554 vs 0.243 for Omnivore), suggesting that self-attention mechanisms are beneficial for capturing temporal dependencies at fine-grained step granularity. However, on the recording-level split, the gap narrows (0.509 vs 0.390 for MLP vs Transformer on Omnivore), indicating that global temporal modeling provides less advantage for coarse-grained video-level predictions. The SlowFast backbone shows higher recall but lower precision compared to Omnivore, reflecting different architecture design trade-offs.

\subsection{LSTM Architecture Results}

In addition to MLP and Transformer baselines, we implement and evaluate an LSTM-based architecture (BaselineV3) to explore recurrent neural networks' capability in capturing temporal dependencies for error detection. Table~\ref{tab:lstm_results} presents the results.

\begin{table}[ht]
\centering
\caption{LSTM baseline results on step-level split with Omnivore features. The LSTM architecture achieves intermediate performance between MLP and Transformer, demonstrating competitive F1-score through better precision-recall balance.}
\label{tab:lstm_results}
\small
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Model} & \textbf{F1} & \textbf{Precision} & \textbf{Recall} & \textbf{Accuracy} & \textbf{AUC} \\
\midrule
MLP & 0.243 & 0.661 & 0.149 & 0.711 & 0.757 \\
Transformer & 0.554 & 0.516 & 0.598 & 0.699 & 0.756 \\
\midrule
LSTM (Ours) & 0.511 & 0.475 & 0.551 & 0.654 & 0.685 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{LSTM Architecture Analysis.}
Our LSTM baseline achieves F1=0.511, positioning it between the MLP (0.243) and Transformer (0.554) architectures on the step-level split with Omnivore features. The LSTM demonstrates a more balanced precision-recall trade-off (P=0.475, R=0.551) compared to the MLP's high precision but very low recall (P=0.661, R=0.149). This suggests that the LSTM's recurrent architecture effectively captures temporal patterns in the video sequence, though it does not reach the performance of the Transformer's self-attention mechanism.

Interestingly, the LSTM achieves this competitive performance with relatively simple architecture: 2 stacked LSTM layers with hidden size 1024, followed by a standard MLP classification head. The model was trained for only 5 epochs with learning rate 1e-5 and achieved strong training metrics (Train F1=0.747, Train AUC=0.911), indicating good learning capacity. The gap between training (Acc=0.836) and test (Acc=0.654) performance suggests some overfitting, though the validation metrics (Val F1=0.552, Val AUC=0.710) remain reasonably close to test performance.

The LSTM's intermediate performance confirms that temporal modeling is important for this task, as evidenced by its superiority over the frame-independent MLP. However, the Transformer's ability to attend to relevant temporal positions appears more effective than the LSTM's sequential processing, particularly for capturing long-range dependencies in procedural videos.

\subsubsection{Error Category Analysis}

To gain deeper insights into the LSTM's error detection capabilities, we analyze its performance across different error categories. Table~\ref{tab:error_categories} presents a contingency analysis showing predictions versus actual error types on the test set.

\begin{table}[ht]
\centering
\caption{LSTM predictions vs. actual error categories on test set (48 samples). The model struggles to distinguish between different error types, with 37.5\% of predicted errors being false positives (No Error samples).}
\label{tab:error_categories}
\small
\begin{tabular}{@{}lrrr@{}}
\toprule
\textbf{Actual Category} & \textbf{Error Pred.} & \textbf{No Error Pred.} & \textbf{Total} \\
\midrule
No Error & 6 (37.50\%) & 18 (56.25\%) & 24 \\
\midrule
Order Error & 3 (18.75\%) & 4 (12.5\%) & 7 \\
Timing Error & 3 (18.75\%) & 3 (9.38\%) & 6 \\
Temperature Error & 3 (18.75\%) & 2 (6.25\%) & 5 \\
Preparation Error & 1 (6.25\%) & 2 (6.25\%) & 3 \\
Measurement Error & 0 (0.0\%) & 1 (3.12\%) & 1 \\
Technique Error & 0 (0.0\%) & 1 (3.12\%) & 1 \\
Other & 0 (0.0\%) & 1 (3.12\%) & 1 \\
\midrule
\textbf{Total} & 16 (100\%) & 32 (100\%) & 48 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Category-specific Performance Insights.}
The error category analysis reveals several important patterns in the LSTM's classification behavior. Among the 16 samples predicted as errors, 6 (37.5\%) are actually correct executions (No Error), indicating a significant false positive rate. This aligns with the moderate precision of 0.475 observed in aggregate metrics.

The model shows relatively better sensitivity to certain error types: Order Error (3/7 detected, 42.9\%), Timing Error (3/6 detected, 50\%), and Temperature Error (3/5 detected, 60\%) are most frequently classified as errors. However, rare error categories like Measurement Error (0/1), Technique Error (0/1), and Other (0/1) are consistently missed, likely due to insufficient training examples for these classes.

Notably, 18 out of 24 correct executions (75\%) are correctly classified as No Error, demonstrating reasonable specificity. However, the model fails to detect several genuine errors: 4 Order Errors, 3 Timing Errors, and 2 Temperature Errors are classified as No Error, contributing to the recall of 0.551.

This category breakdown highlights a fundamental challenge: the binary classification framework treats all error types uniformly, while in reality, different procedural errors may have vastly different visual signatures. Order and timing errors, which involve sequential deviations, appear more detectable than preparation or technique errors, which may require finer-grained visual understanding. Future work could explore multi-class classification or hierarchical error taxonomies to better capture this diversity.

\subsection{Task 2: Multimodal Graph-Based Extension}

\paragraph{Motivation and Approach.}
While Task 1 baselines operate on isolated video segments, real-world procedural error detection requires understanding relationships between sequential steps. To address this limitation, we develop Task 2 as a multimodal graph-based extension that:
\begin{itemize}
    \item \textbf{Leverages recipe structure:} Models cooking procedures as directed acyclic graphs (DAGs) where nodes represent recipe steps and edges encode temporal dependencies
    \item \textbf{Integrates multimodal information:} Combines visual evidence from video with textual recipe descriptions through feature concatenation
    \item \textbf{Performs video-level reasoning:} Operates at the recording (video) level rather than individual segments, enabling holistic error assessment
\end{itemize}

\paragraph{Pipeline Overview.}
Our Task 2 pipeline consists of four main stages implemented across multiple notebooks:

\textbf{Stage 1 - Feature Extraction:} We extract multimodal embeddings using EgoVLP~\cite{egovlp}, a state-of-the-art egocentric vision-language model pre-trained on large-scale first-person video datasets. For each video, the EgoVLP visual encoder produces 256-dimensional embeddings capturing appearance and motion patterns. Similarly, we encode recipe step descriptions using the EgoVLP text encoder, generating 256-dimensional semantic embeddings for each procedural instruction. These embeddings are then processed by HiERO, which projects them to 768 dimensions through its internal projection layers.

\textbf{Stage 2 - Step Localization:} To align visual observations with recipe instructions, we employ HiERO~\cite{hiero}, a hierarchical event recognition model. HiERO temporally segments each cooking video into candidate step intervals, producing a sequence of video segments corresponding to hypothesized recipe step executions. We evaluate two HiERO configurations with different supervision strategies:
\begin{itemize}
    \item \textbf{Fixed 8 steps (unsupervised):} Operates in zero-shot unsupervised mode, forcing HiERO to segment each video into exactly 8 steps without using any recipe-specific information. This provides uniform graph structure across all videos but may not adapt well to recipes with varying complexity.
    \item \textbf{Max+1 steps (supervised):} Operates in supervised mode by leveraging recipe-specific information. For each recipe, HiERO segments the video into exactly (max+1) steps, where max is the actual number of steps prescribed in that specific recipe. The additional "+1" step provides HiERO with a margin of error, allowing it to detect an extra potential step that may account for procedural variations, errors, or transitional actions not explicitly listed in the recipe.
\end{itemize}

\textbf{Stage 3 - Hungarian Matching:} We apply the Hungarian algorithm to find optimal bipartite matching between HiERO-projected visual step embeddings (768-dim) and textual step embeddings (768-dim), minimizing cumulative cosine distance to establish semantic correspondences.

Figure~\ref{fig:hungarian_similarity_matrix} shows an example similarity matrix for a Coffee recipe (max+1 configuration). As expected, one video step remains unmatched. However, the matrix reveals significant ambiguity: many video steps show comparable similarity to multiple textual steps, indicating poor temporal clustering by HiERO. This uncertainty introduces noise into downstream node features.

\begin{figure}[t]
\centering
\includegraphics[width=0.8\linewidth]{figures/figure_2}
\caption{Hungarian matching similarity matrix for Coffee recipe execution (max+1 configuration). Ambiguous matches reveal challenges in visual-textual alignment with zero-shot step localization.}
\label{fig:hungarian_similarity_matrix}
\end{figure}

\textbf{Stage 4 - Graph Construction and Classification:} For each video, we construct a directed acyclic graph where:
\begin{itemize}
    \item \textbf{Nodes:} Represent matched recipe steps, with features formed by concatenating visual embeddings (from matched video segment) and textual embeddings (from recipe step description), resulting in 1536-dimensional node features
    \item \textbf{Edges:} Encode the sequential dependency structure defined by the recipe (e.g., "dice onions" $\rightarrow$ "sauté onions")
\end{itemize}

\paragraph{DAGNN Architecture.}
We implement a Directed Acyclic Graph Neural Network (DAGNN) for video-level error classification:

\begin{enumerate}
    \item \textbf{Projection Layer:} Maps concatenated 1536-dimensional features to a 128-dimensional hidden space via MLP with ReLU activation and LayerNorm for stability
    \item \textbf{Graph Convolution:} Single GCN layer propagates information along graph edges, enabling each node to aggregate context from neighboring steps
    \item \textbf{Global Pooling:} Mean pooling over all nodes produces a single 128-dimensional video-level representation
    \item \textbf{Binary Classifier:} Two-layer MLP (128 $\rightarrow$ 64 $\rightarrow$ 1) with ReLU and dropout (0.4) outputs binary error prediction
\end{enumerate}


\paragraph{Experimental Setup.}
Due to the limited dataset size (384 annotated videos across 24 recipes), we employ Leave-One-Out Cross-Validation (LOO) to maximize training data while ensuring complete recipe-level separation between train and test sets. Each of the 24 unique recipes serves as test set once, with the remaining 23 recipes forming the training set, preventing data leakage from multiple recordings of the same recipe.

We evaluate both HiERO configurations with the following training setup:
\begin{itemize}
    \item \textbf{8 steps configuration:} 15 epochs, batch size 32, hidden dimension 256, single GCN layer, dropout 0.3
    \item \textbf{Max+1 steps configuration:} 20 epochs, batch size 32, hidden dimension 256, single GCN layer, dropout 0.3
\end{itemize}

Both configurations use Adam optimizer (lr=1e-4, weight decay 1e-6), Binary Cross-Entropy loss with positive class weight 0.75 to handle class imbalance, and gradient clipping (max norm 1.0). Model selection is based on training loss, with the best checkpoint used for final evaluation.

\paragraph{Results.}
Table~\ref{tab:task2_dagnn_results} presents results for both DAGNN and Transformer baseline across both HiERO configurations using Leave-One-Out cross-validation. To isolate the contribution of graph-based reasoning, we compare DAGNN against a Transformer baseline that processes the same HiERO-extracted features but treats them as sequential temporal data without explicit graph structure. The Transformer uses self-attention over step sequences with the same training setup (batch size 8, Adam optimizer lr=1e-5, BCE loss with pos\_weight 0.75), trained for 30 epochs (8 steps) and 100 epochs (max+1 steps).

\begin{table*}[t]
\centering
\caption{Task 2 Leave-One-Out Cross-Validation results comparing DAGNN (graph-based) with Transformer baseline (sequential). Values show mean ± standard deviation across 24 folds (one per recipe). High variance indicates substantial performance variability across different recipe contexts.}
\label{tab:task2_dagnn_results}
\small
\begin{tabular}{@{}llccccc@{}}
\toprule
\textbf{Model} & \textbf{HiERO Config} & \textbf{Accuracy} & \textbf{F1} & \textbf{Precision} & \textbf{Recall} & \textbf{AUC} \\
\midrule
\multicolumn{7}{@{}l}{\textit{Transformer Baseline (Sequential)}} \\
\midrule
Transformer & 8 steps & 0.508 $\pm$ 0.152 & 0.503 $\pm$ 0.288 & 0.465 $\pm$ 0.270 & 0.610 $\pm$ 0.379 & 0.533 $\pm$ 0.166 \\
Transformer & Max+1 steps & 0.516 $\pm$ 0.128 & 0.547 $\pm$ 0.162 & 0.590 $\pm$ 0.181 & 0.549 $\pm$ 0.193 & 0.516 $\pm$ 0.197 \\
\midrule
\multicolumn{7}{@{}l}{\textit{DAGNN (Graph-Based)}} \\
\midrule
DAGNN & 8 steps & 0.546 $\pm$ 0.127 & 0.578 $\pm$ 0.194 & 0.620 $\pm$ 0.195 & 0.643 $\pm$ 0.298 & 0.549 $\pm$ 0.149 \\
DAGNN & Max+1 steps & 0.515 $\pm$ 0.133 & 0.587 $\pm$ 0.155 & 0.591 $\pm$ 0.184 & 0.658 $\pm$ 0.233 & 0.517 $\pm$ 0.180 \\
\bottomrule
\end{tabular}
\end{table*}

\paragraph{Performance Analysis.}
Comparing DAGNN with the Transformer baseline reveals modest improvements from graph-based reasoning. For the 8-step configuration, DAGNN achieves higher accuracy (0.546 vs 0.508), F1 (0.578 vs 0.503), and precision (0.620 vs 0.465) compared to the Transformer, while maintaining comparable recall and AUC. The max+1 configuration shows similar trends, with DAGNN achieving marginally better F1 (0.587 vs 0.547) and recall (0.658 vs 0.549). Notably, DAGNN exhibits slightly lower variance in F1 for both configurations (0.194 vs 0.288 for 8 steps; 0.155 vs 0.162 for max+1), suggesting that explicit graph structure provides somewhat more stable predictions compared to pure sequential modeling.

However, both approaches suffer from high standard deviations across all metrics, indicating that neither architecture consistently generalizes across different recipe contexts. The large variance (e.g., recall std > 0.23 for DAGNN, > 0.37 for Transformer on 8 steps) stems from limited training data (~16 videos per LOO fold), noisy HiERO segmentation varying by recipe complexity, and for DAGNN specifically, ambiguous Hungarian matching producing inconsistent multimodal node features. While graph-based modeling offers incremental gains, the fundamental challenge remains the quality of input features from zero-shot step localization.

\paragraph{Analysis and Challenges.}
Our multimodal graph-based approach faces several fundamental challenges that impact performance:

\textbf{Zero-Shot Step Localization Noise:} HiERO operates without supervision, producing temporally imprecise step boundaries. Misaligned segments corrupt the visual embeddings, introducing noise that propagates through the entire pipeline. Qualitative inspection reveals that HiERO occasionally merges multiple recipe steps into single segments or over-segments atomic actions.

\textbf{Hungarian Matching Errors:} While optimal for the assignment problem, Hungarian matching relies purely on embedding similarity and ignores temporal order. In error scenarios where steps are executed out-of-sequence, this semantic matching may incorrectly align video segments to recipe steps, fundamentally corrupting the node features used for graph construction.

\textbf{Limited Training Data:} With only 28 annotated videos distributed across 8 recipes, the dataset is insufficient for training deep graph neural networks. The LOO strategy leaves only ~20-24 training videos per fold, severely limiting the model's capacity to learn robust error patterns. This is exacerbated by class imbalance, with error samples being minority class.

\textbf{Modality Gap:} Despite using the same EgoVLP encoder for both visual and textual embeddings, the semantic spaces may not be perfectly aligned. Concatenating these embeddings assumes they are comparable, but the projection layer alone may be insufficient to bridge domain differences, especially given limited training data.

\textbf{Graph Structure Simplification:} Our DAG construction uses only sequential edges from the recipe, ignoring richer dependencies (e.g., ingredient sharing, tool reuse). More sophisticated graph structures might capture procedural relationships better, but would require additional supervision or domain knowledge.

\subsection{Lessons Learned and Future Directions}

This work provides valuable insights for future research in procedural video understanding:

\begin{enumerate}
    \item \textbf{Step localization quality is critical:} Zero-shot methods like HiERO, while annotation-free, produce insufficient precision for error detection tasks requiring fine-grained step alignment. Future work should explore weakly-supervised or few-shot step localization, or develop robust error detection methods that are invariant to segmentation noise.
    
    \item \textbf{Dataset scale matters:} Graph-based multimodal approaches require substantially more training data. Promising directions include: (i) data augmentation via temporal transformations or synthetic errors, (ii) pre-training on unlabeled cooking videos with self-supervised objectives, (iii) transfer learning from related procedural domains (e.g., assembly, surgery).
    
    \item \textbf{Cross-validation strategy impacts results:} LOO provides recipe-level generalization estimates but suffers from high variance due to single-recipe test sets. Stratified K-Fold allows larger test sets but may overestimate performance by including same-recipe videos in train/test. Both perspectives are valuable for understanding model robustness.
    
    \item \textbf{Improved multimodal fusion:} Simple concatenation may not suffice. Future architectures should explore: (i) cross-modal attention mechanisms to dynamically weight visual vs. textual evidence, (ii) contrastive learning to align visual and text embeddings, (iii) gated fusion to allow adaptive integration based on input characteristics.
    
    \item \textbf{Interpretability matters:} Graph-based models offer potential for interpretability through attention visualization over graph structure. Understanding which step transitions or node features contribute to error predictions could provide actionable insights for users and improve model debugging.
\end{enumerate}

These lessons, while stemming from a challenging problem setting, provide concrete directions for advancing procedural error detection in egocentric video understanding.
