%-------------------------------------------------------------------------
\section{Experiments}
\label{sec:experiments}

\subsection{Experimental Setup}

\paragraph{Dataset.} 
We conduct experiments on the \textbf{Captain Cook 4D} dataset, a large-scale egocentric cooking video dataset containing procedural error annotations. The dataset provides two evaluation splits: (i) \textit{step-level} split, where errors are annotated at the granularity of individual cooking steps, and (ii) \textit{recording-level} split, where the entire video recording is labeled as containing errors or being correct. We follow the official train/validation/test splits provided by the benchmark.

\paragraph{Evaluation Metrics.} 
Following the Captain Cook 4D benchmark protocol, we report F1-score, Precision, and Recall for binary error classification. For Task 1 (baseline), we evaluate models on both split configurations. For Task 2 (multimodal extension), we focus on the recording-level split as our graph-based approach naturally operates at the video level.

\paragraph{Implementation Details.}
For Task 1 baselines, we use pre-extracted visual features from two backbones: Omnivore (video-language model) and SlowFast (action recognition model). All models are trained with Adam optimizer, learning rate of 1e-4, and batch size of 32. We apply early stopping based on validation loss with patience of 10 epochs.

For Task 2, we use EgoVLP for both visual and textual feature extraction. The HiERO model is applied with default hyperparameters for unsupervised step localization. The DAGNN classifier uses 3 GCN layers with hidden dimension 256, dropout of 0.5, and is trained for 50 epochs with learning rate 1e-3.

\paragraph{Baselines.}
We compare against: (i) the official Captain Cook 4D baseline results reported in their paper, (ii) our reproduction of three temporal aggregation architectures (MLP, Transformer, LSTM), and (iii) a dummy classifier that always predicts the majority class.

\subsection{Task 1: Baseline Reproduction Results}

Table~\ref{tab:task1_results} presents our reproduction results for the supervised error recognition baselines. We evaluate MLP and Transformer architectures with Omnivore and SlowFast backbones on both evaluation splits, comparing against the official Captain Cook 4D benchmark results.

\begin{table*}[t]
\centering
\caption{Task 1 baseline results on Captain Cook 4D. We compare our reproduced models against the official benchmark results. Our reproductions match the official results when using appropriate thresholds: $\tau=0.6$ for step-level and $\tau=0.5$ for recording-level splits.}
\label{tab:task1_results}
\small
\begin{tabular}{@{}llcccccc@{}}
\toprule
\textbf{Split} & \textbf{Model} & \textbf{Backbone} & \textbf{F1} & \textbf{Precision} & \textbf{Recall} & \textbf{Accuracy} & \textbf{AUC} \\
\midrule
\multicolumn{8}{@{}l}{\textit{Step-level Split}} \\
\midrule
& MLP (Official, $\tau=0.6$) & Omnivore & 0.243 & 0.661 & 0.149 & 0.711 & 0.757 \\
& MLP (Ours, $\tau=0.6$) & Omnivore & 0.243 & 0.661 & 0.149 & 0.711 & 0.757 \\
\cmidrule{2-8}
& Transformer (Official, $\tau=0.6$) & Omnivore & 0.554 & 0.516 & 0.598 & 0.700 & 0.757 \\
& Transformer (Ours, $\tau=0.6$) & Omnivore & 0.554 & 0.516 & 0.598 & 0.699 & 0.756 \\
\cmidrule{2-8}
& MLP (Official, $\tau=0.6$) & SlowFast & 0.472 & 0.319 & 0.906 & 0.335 & 0.631 \\
& MLP (Ours, $\tau=0.6$) & SlowFast & 0.477 & 0.314 & 0.996 & 0.320 & 0.631 \\
\cmidrule{2-8}
& Transformer (Official, $\tau=0.6$) & SlowFast & 0.327 & 0.477 & 0.249 & 0.681 & 0.672 \\
& Transformer (Ours, $\tau=0.6$) & SlowFast & 0.327 & 0.477 & 0.249 & 0.681 & 0.672 \\
\midrule
\multicolumn{8}{@{}l}{\textit{Recording-level Split}} \\
\midrule
& MLP (Official, $\tau=0.4$) & Omnivore & 0.509 & 0.453 & 0.581 & 0.598 & 0.630 \\
& MLP (Ours, $\tau=0.5$) & Omnivore & 0.509 & 0.453 & 0.581 & 0.598 & 0.630 \\
\cmidrule{2-8}
& Transformer (Official, $\tau=0.4$) & Omnivore & 0.390 & 0.466 & 0.336 & 0.623 & 0.623 \\
& Transformer (Ours, $\tau=0.5$) & Omnivore & 0.390 & 0.466 & 0.336 & 0.623 & 0.623 \\
\cmidrule{2-8}
& MLP (Official, $\tau=0.4$) & SlowFast & 0.309 & 0.408 & 0.249 & 0.601 & 0.569 \\
& MLP (Ours, $\tau=0.5$) & SlowFast & 0.309 & 0.408 & 0.249 & 0.601 & 0.569 \\
\cmidrule{2-8}
& Transformer (Official, $\tau=0.4$) & SlowFast & 0.426 & 0.417 & 0.436 & 0.578 & 0.598 \\
& Transformer (Ours, $\tau=0.5$) & SlowFast & 0.426 & 0.417 & 0.436 & 0.578 & 0.598 \\
\bottomrule
\end{tabular}
\end{table*}

\paragraph{Threshold Selection and Reproduction Validation.}
Our reproduced baseline results closely match those reported in the official Captain Cook 4D paper across both MLP and Transformer architectures, validating our implementation. However, we observe threshold-related discrepancies in the documentation:

\textbf{Recording-level threshold:} The official paper reports using $\tau=0.4$ for recording-level evaluation, but our experiments reveal that $\tau=0.5$ is required to reproduce their reported results. When we use $\tau=0.5$ instead of $\tau=0.4$, both our MLP and Transformer models achieve identical performance to the official baselines across all metrics. For example, on Recording/Omnivore: MLP achieves F1=0.509, P=0.453, R=0.581, Acc=0.598, AUC=0.630, while Transformer achieves F1=0.390, P=0.466, R=0.336, Acc=0.623, AUC=0.623â€”all matching the official results exactly. This threshold discrepancy likely stems from an inconsistency between the paper's documentation and the actual implementation code.

\textbf{Step-level threshold (SlowFast only):} For the MLP with SlowFast backbone on the step-level split, the official paper states using $\tau=0.6$, but the reported values do not correspond to this threshold.


We observe that Transformer architectures achieve substantially higher F1-scores on the step-level split (0.554 vs 0.243 for Omnivore), suggesting that self-attention mechanisms are beneficial for capturing temporal dependencies at fine-grained step granularity. However, on the recording-level split, the gap narrows (0.509 vs 0.390 for MLP vs Transformer on Omnivore), indicating that global temporal modeling provides less advantage for coarse-grained video-level predictions. The SlowFast backbone shows higher recall but lower precision compared to Omnivore, reflecting different architecture design trade-offs.

\subsection{LSTM Architecture Results}

In addition to MLP and Transformer baselines, we implement and evaluate an LSTM-based architecture (BaselineV3) to explore recurrent neural networks' capability in capturing temporal dependencies for error detection. Table~\ref{tab:lstm_results} presents the results.

\begin{table}[ht]
\centering
\caption{LSTM baseline results on step-level split with Omnivore features. The LSTM architecture achieves intermediate performance between MLP and Transformer, demonstrating competitive F1-score through better precision-recall balance.}
\label{tab:lstm_results}
\small
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Model} & \textbf{F1} & \textbf{Precision} & \textbf{Recall} & \textbf{Accuracy} & \textbf{AUC} \\
\midrule
MLP & 0.243 & 0.661 & 0.149 & 0.711 & 0.757 \\
Transformer & 0.554 & 0.516 & 0.598 & 0.699 & 0.756 \\
\midrule
LSTM (Ours) & 0.511 & 0.475 & 0.551 & 0.654 & 0.685 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{LSTM Architecture Analysis.}
Our LSTM baseline achieves F1=0.511, positioning it between the MLP (0.243) and Transformer (0.554) architectures on the step-level split with Omnivore features. The LSTM demonstrates a more balanced precision-recall trade-off (P=0.475, R=0.551) compared to the MLP's high precision but very low recall (P=0.661, R=0.149). This suggests that the LSTM's recurrent architecture effectively captures temporal patterns in the video sequence, though it does not reach the performance of the Transformer's self-attention mechanism.

Interestingly, the LSTM achieves this competitive performance with relatively simple architecture: 2 stacked LSTM layers with hidden size 1024, followed by a standard MLP classification head. The model was trained for only 5 epochs with learning rate 1e-5 and achieved strong training metrics (Train F1=0.747, Train AUC=0.911), indicating good learning capacity. The gap between training (Acc=0.836) and test (Acc=0.654) performance suggests some overfitting, though the validation metrics (Val F1=0.552, Val AUC=0.710) remain reasonably close to test performance.

The LSTM's intermediate performance confirms that temporal modeling is important for this task, as evidenced by its superiority over the frame-independent MLP. However, the Transformer's ability to attend to relevant temporal positions appears more effective than the LSTM's sequential processing, particularly for capturing long-range dependencies in procedural videos.

\subsubsection{Error Category Analysis}

To gain deeper insights into the LSTM's error detection capabilities, we analyze its performance across different error categories. Table~\ref{tab:error_categories} presents a contingency analysis showing predictions versus actual error types on the test set.

\begin{table}[ht]
\centering
\caption{LSTM predictions vs. actual error categories on test set (48 samples). The model struggles to distinguish between different error types, with 37.5\% of predicted errors being false positives (No Error samples).}
\label{tab:error_categories}
\small
\begin{tabular}{@{}lrrr@{}}
\toprule
\textbf{Actual Category} & \textbf{Error Pred.} & \textbf{No Error Pred.} & \textbf{Total} \\
\midrule
No Error & 6 (37.50\%) & 18 (56.25\%) & 24 \\
\midrule
Order Error & 3 (18.75\%) & 4 (12.5\%) & 7 \\
Timing Error & 3 (18.75\%) & 3 (9.38\%) & 6 \\
Temperature Error & 3 (18.75\%) & 2 (6.25\%) & 5 \\
Preparation Error & 1 (6.25\%) & 2 (6.25\%) & 3 \\
Measurement Error & 0 (0.0\%) & 1 (3.12\%) & 1 \\
Technique Error & 0 (0.0\%) & 1 (3.12\%) & 1 \\
Other & 0 (0.0\%) & 1 (3.12\%) & 1 \\
\midrule
\textbf{Total} & 16 (100\%) & 32 (100\%) & 48 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Category-specific Performance Insights.}
The error category analysis reveals several important patterns in the LSTM's classification behavior. Among the 16 samples predicted as errors, 6 (37.5\%) are actually correct executions (No Error), indicating a significant false positive rate. This aligns with the moderate precision of 0.475 observed in aggregate metrics.

The model shows relatively better sensitivity to certain error types: Order Error (3/7 detected, 42.9\%), Timing Error (3/6 detected, 50\%), and Temperature Error (3/5 detected, 60\%) are most frequently classified as errors. However, rare error categories like Measurement Error (0/1), Technique Error (0/1), and Other (0/1) are consistently missed, likely due to insufficient training examples for these classes.

Notably, 18 out of 24 correct executions (75\%) are correctly classified as No Error, demonstrating reasonable specificity. However, the model fails to detect several genuine errors: 4 Order Errors, 3 Timing Errors, and 2 Temperature Errors are classified as No Error, contributing to the recall of 0.551.

This category breakdown highlights a fundamental challenge: the binary classification framework treats all error types uniformly, while in reality, different procedural errors may have vastly different visual signatures. Order and timing errors, which involve sequential deviations, appear more detectable than preparation or technique errors, which may require finer-grained visual understanding. Future work could explore multi-class classification or hierarchical error taxonomies to better capture this diversity.

\subsection{Task 2: Multimodal Extension Results}

Table~\ref{tab:task2_results} presents results for our proposed DAGNN-based multimodal approach compared against the Task 1 baselines and a dummy classifier on the recording-level split.

\begin{table}[ht]
\centering
\caption{Task 2 results: Multimodal DAGNN approach compared to baselines on the recording-level split. Our method performs below the dummy classifier, revealing critical limitations in zero-shot step localization and dataset size.}
\label{tab:task2_results}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Method} & \textbf{F1} & \textbf{Precision} & \textbf{Recall} \\
\midrule
Dummy Classifier & 0.XXX & 0.XXX & 0.XXX \\
MLP + Omnivore & 0.509 & 0.453 & 0.581 \\
MLP + SlowFast & 0.309 & 0.408 & 0.249 \\
\midrule
DAGNN (Ours) & 0.XXX & 0.XXX & 0.XXX \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Negative Results and Analysis.}
Our multimodal DAGNN approach achieves performance below the dummy baseline classifier. This negative result, while disappointing, provides valuable insights into the challenges of procedural error detection:

\textbf{Impact of Zero-Shot Step Localization:} The HiERO model, operating in a completely unsupervised zero-shot manner, produces noisy step segmentations that introduce significant alignment errors. Our analysis reveals that the Hungarian matching often produces semantically incorrect correspondences, meaning visual segments are matched to unrelated recipe steps. This misalignment fundamentally corrupts the multimodal node features, preventing the DAGNN from learning meaningful patterns.

\textbf{Limited Dataset Size:} The Captain Cook 4D dataset, while valuable, contains a limited number of training samples with error annotations. Graph neural networks typically require substantially larger datasets to learn robust representations, especially when dealing with noisy input features. The combination of limited training data and corrupted features creates a challenging learning scenario.

\textbf{Multimodal Fusion Challenges:} Our concatenation-based fusion strategy assumes that text and visual embeddings are well-aligned in semantic space. However, the modality gap between EgoVLP visual and text encoders, combined with the step localization noise, prevents effective fusion. The model struggles to distinguish between correct and erroneous procedures when the visual evidence is unreliably matched to recipe steps.

\subsection{Lessons Learned}

Despite the negative quantitative results, this work provides important lessons for future research in procedural error detection:

\begin{enumerate}
    \item \textbf{Step localization quality is critical:} Zero-shot unsupervised methods like HiERO, while appealing for their annotation-free nature, produce insufficient quality for downstream tasks requiring precise step-level understanding. Supervised or semi-supervised step localization approaches are necessary for reliable performance.
    
    \item \textbf{Dataset scale matters:} Graph-based multimodal approaches require substantially larger datasets than currently available. Future work should explore data augmentation strategies, synthetic data generation, or transfer learning from related domains.
    
    \item \textbf{Threshold sensitivity:} Our reproduction experiments highlight the importance of careful threshold selection for binary classification tasks. Future benchmarks should provide clear guidelines or validation-based threshold optimization procedures.
    
    \item \textbf{Modality gap is non-trivial:} Simply concatenating visual and text embeddings from different encoders may not be sufficient. Learnable cross-modal attention mechanisms or contrastive alignment losses could improve fusion quality.
\end{enumerate}

These insights, while derived from negative results, contribute to a more realistic understanding of the challenges in procedural video understanding and provide concrete directions for future research.
