%-------------------------------------------------------------------------
\section{Experiments}
\label{sec:experiments}

\subsection{Experimental Setup}

\paragraph{Dataset.} 
We evaluate on \textbf{Captain Cook 4D}, an egocentric cooking video dataset with procedural error annotations. The dataset provides two evaluation protocols for the first task: (i) \textit{Recordings split}: videos assigned to train/validation/test ensuring each recording appears in only one split; (ii) \textit{Steps split}: video segments divided across splits such that steps from each recording are distributed across all splits. We adopt the official partitions for all experiments. For the second task, we focus on the recording-level split since graph-based reasoning operates at the video level.

\paragraph{Evaluation Metrics.} 
Following the official benchmark protocol, we report F1-score, Precision, Recall, Accuracy, and Area Under the ROC Curve (AUC) for binary error classification. We evaluate Task 1 baselines on both step-level and recording-level splits, while Task 2 multimodal extension focuses on the recording-level split since graph-based reasoning naturally operates at the video level.

\paragraph{Implementation Details.}
All models are implemented in PyTorch and trained on T4 GPU. For Task 1, we use pre-extracted features from Omnivore and SlowFast backbones.

\textbf{Task 1 - MLP:} Feedforward architecture with one hidden layer, ReLU activation, and sigmoid output. Trained with Adam (lr=0.001), batch size 512, 50 epochs, using weighted BCE Loss (weight 1.5 for errors) and processing 1-second sub-step.

\textbf{Task 1 - Transformer:} Encoder with positional encodings processing all 1s segments of a step, feeding into MLP head, producing labels at step level. Trained with Adam (lr=1e-5), batch size 1, 50 epochs, weighted BCE Loss.

\textbf{Task 1 - LSTM:} 2 stacked LSTM layers (hidden size 1024) with MLP head. Trained with Adam (lr=1e-5), 5 epochs, weighted BCE Loss.

\textbf{Task 2 - DAGNN:} Uses EgoVLP encoders and HiERO step localization. DAGNN with single GCN layer (hidden dim 256, dropout 0.3), trained 15-20 epochs with Adam (lr=1e-4).


\subsection{Task 1: Baseline Reproduction Results}

Table~\ref{tab:task1_results} presents our reproduction results for the supervised error recognition baselines. We evaluate MLP and Transformer architectures with Omnivore and SlowFast backbones on both evaluation splits, comparing against the official Captain Cook 4D benchmark results.

\begin{table*}[t]
\centering
\caption{Task 1 baseline results on Captain Cook 4D. We compare our reproduced models against the official benchmark results. Our reproductions match the official results when using appropriate thresholds: $\tau=0.6$ for step-level and $\tau=0.5$ for recording-level splits.}
\label{tab:task1_results}
\small
\begin{tabular}{@{}llcccccc@{}}
\toprule
\textbf{Split} & \textbf{Model} & \textbf{Backbone} & \textbf{F1} & \textbf{Precision} & \textbf{Recall} & \textbf{Accuracy} & \textbf{AUC} \\
\midrule
\multicolumn{8}{@{}l}{\textit{Step-level Split}} \\
\midrule
& MLP (Official, $\tau=0.6$) & Omnivore & 0.243 & 0.661 & 0.149 & 0.711 & 0.757 \\
& MLP (Ours, $\tau=0.6$) & Omnivore & 0.243 & 0.661 & 0.149 & 0.711 & 0.757 \\
\cmidrule{2-8}
& Transformer (Official, $\tau=0.6$) & Omnivore & 0.554 & 0.516 & 0.598 & 0.700 & 0.757 \\
& Transformer (Ours, $\tau=0.6$) & Omnivore & 0.554 & 0.516 & 0.598 & 0.699 & 0.756 \\
\cmidrule{2-8}
& MLP (Official, $\tau=0.6$) & SlowFast & 0.472 & 0.319 & 0.906 & 0.335 & 0.631 \\
& MLP (Ours, $\tau=0.6$) & SlowFast & 0.477 & 0.314 & 0.996 & 0.320 & 0.631 \\
\cmidrule{2-8}
& Transformer (Official, $\tau=0.6$) & SlowFast & 0.327 & 0.477 & 0.249 & 0.681 & 0.672 \\
& Transformer (Ours, $\tau=0.6$) & SlowFast & 0.327 & 0.477 & 0.249 & 0.681 & 0.672 \\
\midrule
\multicolumn{8}{@{}l}{\textit{Recording-level Split}} \\
\midrule
& MLP (Official, $\tau=0.4$) & Omnivore & 0.509 & 0.453 & 0.581 & 0.598 & 0.630 \\
& MLP (Ours, $\tau=0.5$) & Omnivore & 0.509 & 0.453 & 0.581 & 0.598 & 0.630 \\
\cmidrule{2-8}
& Transformer (Official, $\tau=0.4$) & Omnivore & 0.390 & 0.466 & 0.336 & 0.623 & 0.623 \\
& Transformer (Ours, $\tau=0.5$) & Omnivore & 0.390 & 0.466 & 0.336 & 0.623 & 0.623 \\
\cmidrule{2-8}
& MLP (Official, $\tau=0.4$) & SlowFast & 0.309 & 0.408 & 0.249 & 0.601 & 0.569 \\
& MLP (Ours, $\tau=0.5$) & SlowFast & 0.309 & 0.408 & 0.249 & 0.601 & 0.569 \\
\cmidrule{2-8}
& Transformer (Official, $\tau=0.4$) & SlowFast & 0.426 & 0.417 & 0.436 & 0.578 & 0.598 \\
& Transformer (Ours, $\tau=0.5$) & SlowFast & 0.426 & 0.417 & 0.436 & 0.578 & 0.598 \\
\bottomrule
\end{tabular}
\end{table*}

\paragraph{Threshold Selection and Reproduction Validation.}
Our reproduced results closely match official Captain Cook 4D benchmarks, validating our implementation. However, we found threshold discrepancies: the official paper reports $\tau=0.4$ for recording-level evaluation, but $\tau=0.5$ is required to reproduce their results. Both our MLP and Transformer models achieve identical performance to official baselines using $\tau=0.5$.

Transformers achieve substantially higher F1-scores on step-level splits (0.554 vs 0.243 for Omnivore), indicating self-attention benefits for temporal dependencies. On recording-level splits, the gap narrows (0.509 vs 0.390), suggesting less advantage for video-level predictions.

\subsection{LSTM Architecture Results}

In addition to MLP and Transformer baselines, we implement and evaluate an LSTM-based architecture (BaselineV3) to explore recurrent neural networks' capability in capturing temporal dependencies for error detection.  We choose the LSTM which produced the lower loss on the test dataset after each epoch. Table~\ref{tab:lstm_results} presents the results.

\begin{table*}[t]
\centering
\caption{LSTM baseline results on both evaluation splits with Omnivore features. The LSTM architecture achieves intermediate performance between MLP and Transformer on step-level split, while showing competitive results on recording-level split.}
\label{tab:lstm_results}
\small
\begin{tabular}{@{}llcccccc@{}}
\toprule
\textbf{Split} & \textbf{Model} & \textbf{Backbone} & \textbf{F1} & \textbf{Precision} & \textbf{Recall} & \textbf{Accuracy} & \textbf{AUC} \\
\midrule
\multicolumn{8}{@{}l}{\textit{Step-level Split}} \\
\midrule
& MLP & Omnivore & 0.243 & 0.661 & 0.149 & 0.711 & 0.757 \\
& Transformer & Omnivore & 0.554 & 0.516 & 0.598 & 0.699 & 0.756 \\
& LSTM (Ours) & Omnivore & 0.511 & 0.475 & 0.551 & 0.654 & 0.685 \\
\midrule
\multicolumn{8}{@{}l}{\textit{Recording-level Split}} \\
\midrule
& MLP & Omnivore & 0.509 & 0.453 & 0.581 & 0.598 & 0.630 \\
& Transformer & Omnivore & 0.390 & 0.466 & 0.336 & 0.623 & 0.623 \\
& LSTM (Ours) & Omnivore & 0.438 & 0.383 & 0.513 & 0.614 & 0.621 \\
\bottomrule
\end{tabular}
\end{table*}

\paragraph{LSTM Architecture Analysis.}
Our LSTM baseline achieves F1=0.511 on step-level split, intermediate between MLP (0.243) and Transformer (0.554) with Omnivore features. The LSTM demonstrates balanced precision-recall (P=0.475, R=0.551) compared to MLP's high precision but low recall (P=0.661, R=0.149), confirming that recurrent architectures effectively capture temporal patterns. However, Transformers remain superior on step-level split, likely due to better long-range dependency modeling through self-attention.

The LSTM model occupies a middle ground in terms of performance, sitting between the simpler MLP and the more complex Transformer architecture. It demonstrates a balanced precision-recall trade-off and, notably, maintains a more stable recall across different data splits compared to its counterparts. Anyways, a simple Dummy System outperform our LSTM model - 69\% accuracy.
\subsubsection{Error Category Analysis}

To gain deeper insights into the LSTM's error detection capabilities, we analyze its performance across different error categories. Table~\ref{tab:error_categories} presents a contingency analysis showing predictions versus actual error types on the test set.

\begin{table}[ht]
\centering
\caption{LSTM predictions vs. actual error categories on test set (1206 samples). The model struggles to distinguish between different error types, with 44.1\% of predicted errors being false positives (No Error samples).}
\label{tab:error_categories}
\small
\begin{tabular}{@{}lrrr@{}}
\toprule
\textbf{Actual Category} & \textbf{Error Pred.} & \textbf{No Error Pred.} & \textbf{Total} \\
\midrule
No Error & 213 & 507 & 720 \\
\midrule
Measurement Error & 40  & 25  & 65 \\
Missing Step & 1  & 0  & 1 \\
Order Error & 97 & 66  & 163 \\
Other & 1  & 0  & 1 \\
Preparation Error & 44  & 37  & 81 \\
Technique Error & 60 & 42  & 102 \\
Temperature Error & 9  & 13  & 22 \\
Timing Error & 18  & 33  & 51 \\
\midrule
\textbf{Total} & 483 & 723 & 1206 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Category-specific Performance Insights.}
Among the 483 samples predicted as errors, 213 are correct executions, indicating substantial false positives. The model shows variable performance across error categories: it better detects Measurement, Order, and Technique errors, while struggling with Temperature and Timing errors. The improved detection rates for previously challenging categories (Measurement, Technique) suggest the model has learned discriminative features, though temporal errors (Timing, Temperature) remain difficult to identify. This highlights a fundamental challenge: binary classification treats all error types uniformly, while different procedural errors have vastly different visual signatures requiring specialized feature representations.

\subsection{Task 2: Multimodal Graph-Based Extension}

\paragraph{Motivation and Approach.}
While Task 1 baselines operate on isolated video segments, real-world procedural error detection requires understanding relationships between sequential steps. To address this limitation, we develop Task 2 as a multimodal graph-based extension that:
\begin{itemize}
    \item \textbf{Leverages recipe structure:} Models cooking procedures as directed acyclic graphs (DAGs) where nodes represent recipe steps and edges encode temporal dependencies
    \item \textbf{Integrates multimodal information:} Combines visual evidence from video with textual recipe descriptions through feature concatenation
    \item \textbf{Performs video-level reasoning:} Operates at the recording (video) level rather than individual segments, enabling holistic error assessment
\end{itemize}

\paragraph{Pipeline Overview.}
Our Task 2 pipeline consists of four main stages implemented across multiple notebooks:

\textbf{Stage 1 - Feature Extraction:} We extract multimodal embeddings using EgoVLP~\cite{egovlp}, a state-of-the-art egocentric vision-language model pre-trained on large-scale first-person video datasets. For each video, the EgoVLP visual encoder produces 256-dimensional embeddings capturing appearance and motion patterns. Similarly, we encode recipe step descriptions using the EgoVLP text encoder, generating 256-dimensional semantic embeddings for each procedural instruction. These embeddings are then processed by HiERO, which projects them to 768 dimensions through its internal projection layers.

\textbf{Stage 2 - Step Localization:} To align visual observations with recipe instructions, we employ HiERO~\cite{hiero}, a hierarchical event recognition model. HiERO temporally segments each cooking video into candidate step intervals, producing a sequence of video segments corresponding to hypothesized recipe step executions. We evaluate two HiERO configurations with different supervision strategies:
\begin{itemize}
    \item \textbf{Fixed 8 steps:} Forcing HiERO to segment each video into exactly 8 steps without using any recipe-specific information. This provides uniform graph structure across all videos but may not adapt well to recipes with varying complexity.
    \item \textbf{Max+1 steps:} For each recipe, HiERO segments the video into exactly (max+1) steps, where max is the actual number of steps prescribed in that specific recipe. The additional "+1" step provides HiERO with a margin of error, allowing it to detect an extra potential step that may account for procedural variations, errors, or transitional actions not explicitly listed in the recipe.
\end{itemize}

\textbf{Stage 3 - Hungarian Matching:} We apply the Hungarian algorithm to find optimal bipartite matching between HiERO-projected visual step embeddings (768-dim) and textual step embeddings (768-dim), minimizing cumulative cosine distance to establish semantic correspondences.

Figure~\ref{fig:hungarian_similarity_matrix} shows an example similarity matrix for a Coffee recipe (max+1 configuration). As expected, one video step remains unmatched. However, the matrix reveals significant ambiguity: many video steps show comparable similarity to multiple textual steps, indicating poor temporal clustering by HiERO. This uncertainty introduces noise into downstream node features.

\begin{figure}[t]
\centering
\includegraphics[width=0.8\linewidth]{figures/figure_2}
\caption{Hungarian matching similarity matrix for Coffee recipe execution (max+1 configuration). Ambiguous matches reveal challenges in visual-textual alignment with zero-shot step localization.}
\label{fig:hungarian_similarity_matrix}
\end{figure}

\textbf{Stage 4 - Graph Construction and Classification:} For each video, we construct a directed acyclic graph where:
\begin{itemize}
    \item \textbf{Nodes:} Represent matched recipe steps, with features formed by concatenating visual embeddings (from matched video segment) and textual embeddings (from recipe step description), resulting in 1536-dimensional node features
    \item \textbf{Edges:} Encode the sequential dependency structure defined by the recipe (e.g., "dice onions" $\rightarrow$ "sauté onions")
\end{itemize}

\paragraph{DAGNN Architecture.}
We implement a Directed Acyclic Graph Neural Network (DAGNN) for video-level error classification:

\begin{enumerate}
    \item \textbf{Projection Layer:} Maps concatenated 1536-dimensional features to a 128-dimensional hidden space via MLP with ReLU activation and LayerNorm for stability
    \item \textbf{Graph Convolution:} Single GCN layer propagates information along graph edges, enabling each node to aggregate context from neighboring steps
    \item \textbf{Global Pooling:} Mean pooling over all nodes produces a single 128-dimensional video-level representation
    \item \textbf{Binary Classifier:} Two-layer MLP (128 $\rightarrow$ 64 $\rightarrow$ 1) with ReLU and dropout outputs binary error prediction
\end{enumerate}


\paragraph{Experimental Setup.}
Due to the limited dataset size (384 annotated videos across 24 recipes), we employ Leave-One-Out Cross-Validation (LOO) to maximize training data while ensuring complete recipe-level separation between train and test sets. Each of the 24 unique recipes serves as test set once, with the remaining 23 recipes forming the training set, preventing data leakage from multiple recordings of the same recipe.

We evaluate both HiERO configurations with the following training setup:
\begin{itemize}
    \item \textbf{8 steps configuration:} 15 epochs, batch size 32, hidden dimension 256, single GCN layer, dropout 0.3
    \item \textbf{Max+1 steps configuration:} 20 epochs, batch size 32, hidden dimension 256, single GCN layer, dropout 0.3
\end{itemize}

Both configurations use Adam optimizer (lr=1e-4, weight decay 1e-6), Binary Cross-Entropy loss with positive class weight 0.75 to handle class imbalance, and gradient clipping (max norm 1.0). Model selection is based on training loss, with the best checkpoint used for final evaluation.

\paragraph{Results.}
Table~\ref{tab:task2_dagnn_results} presents results for both DAGNN and Transformer baseline across both HiERO configurations using Leave-One-Out cross-validation. To isolate the contribution of graph-based reasoning, we compare DAGNN against a Transformer baseline that processes the same HiERO-extracted features but treats them as sequential temporal data without explicit graph structure. The Transformer uses self-attention over step sequences with Adam optimizer (lr=1e-5) and BCE loss (pos\_weight 0.75), trained for 30 epochs with batch size 8 (8 steps configuration) and 100 epochs with batch size 32 (max+1 steps configuration).

\begin{table*}[t]
\centering
\caption{Task 2 Leave-One-Out Cross-Validation results comparing DAGNN (graph-based) with Transformer baseline (sequential). Values show mean ± standard deviation across 24 folds (one per recipe). High variance indicates substantial performance variability across different recipe contexts.}
\label{tab:task2_dagnn_results}
\small
\begin{tabular}{@{}llccccc@{}}
\toprule
\textbf{Model} & \textbf{HiERO Config} & \textbf{Accuracy} & \textbf{F1} & \textbf{Precision} & \textbf{Recall} & \textbf{AUC} \\
\midrule
\multicolumn{7}{@{}l}{\textit{Transformer Baseline (Sequential)}} \\
\midrule
Transformer & 8 steps & 0.521 $\pm$ 0.144 & 0.505 $\pm$ 0.265 & 0.544 $\pm$ 0.272 & 0.587 $\pm$ 0.380 & 0.534 $\pm$ 0.172 \\
Transformer & Max+1 steps & 0.494 $\pm$ 0.134 & 0.513 $\pm$ 0.239 & 0.494 $\pm$ 0.210 & 0.575 $\pm$ 0.301 & 0.500 $\pm$ 0.144 \\
\midrule
\multicolumn{7}{@{}l}{\textit{DAGNN (Graph-Based)}} \\
\midrule
DAGNN & 8 steps & 0.546 $\pm$ 0.127 & 0.578 $\pm$ 0.194 & 0.620 $\pm$ 0.195 & 0.643 $\pm$ 0.298 & 0.549 $\pm$ 0.149 \\
DAGNN & Max+1 steps & 0.515 $\pm$ 0.133 & 0.587 $\pm$ 0.155 & 0.591 $\pm$ 0.184 & 0.658 $\pm$ 0.233 & 0.517 $\pm$ 0.180 \\
\bottomrule
\end{tabular}
\end{table*}

\paragraph{Performance Analysis.}
Comparing DAGNN with the Transformer baseline reveals modest improvements from graph-based reasoning. For the 8-step configuration, DAGNN achieves higher accuracy (0.546 vs 0.521), F1 (0.578 vs 0.505), and precision (0.620 vs 0.544) compared to the Transformer, while maintaining comparable recall and AUC. The max+1 configuration shows similar trends, with DAGNN achieving marginally better F1 (0.587 vs 0.513) and recall (0.658 vs 0.575). Notably, DAGNN exhibits slightly lower variance in F1 for both configurations (0.194 vs 0.265 for 8 steps; 0.155 vs 0.239 for max+1), suggesting that explicit graph structure provides somewhat more stable predictions compared to pure sequential modeling.

However, both approaches suffer from high standard deviations across all metrics, indicating that neither architecture consistently generalizes across different recipe contexts. The large variance (e.g., recall std $>$ 0.298 for DAGNN, $>$ 0.380 for Transformer on 8 steps) stems from limited training data (~16 videos per LOO fold), noisy HiERO segmentation varying by recipe complexity, and for DAGNN specifically, ambiguous Hungarian matching producing inconsistent multimodal node features. While graph-based modeling offers incremental gains, the fundamental challenge remains the quality of input features from zero-shot step localization.

\paragraph{Analysis and Challenges.}
Our multimodal graph-based approach faces several fundamental challenges that impact performance:

\textbf{Zero-Shot Step Localization Noise:} HiERO operates with weakly-supervision, producing temporally imprecise step boundaries. Misaligned segments corrupt the visual embeddings, introducing noise that propagates through the entire pipeline. Qualitative inspection reveals that HiERO occasionally merges multiple recipe steps into single segments or over-segments atomic actions.

\textbf{Hungarian Matching Errors:} While optimal for the assignment problem, Hungarian matching relies purely on embedding similarity and ignores temporal order. In error scenarios where steps are executed out-of-sequence, this semantic matching may incorrectly align video segments to recipe steps, fundamentally corrupting the node features used for graph construction.  A new approach is to use 1-to-N matching instead of strict 1-to-1 Hungarian matching, allowing multiple video segments to correspond to a single recipe step, potentially mitigating segmentation errors.

\textbf{Limited Training Data:} With only 384 recipe videos distrubuted across 24 recipes, the dataset is insufficient for training deep graph neural networks. The LOO strategy leaves only ~16 training videos per fold, severely limiting the model's capacity to learn robust error patterns. This is exacerbated by class imbalance, with no-error samples being minority class.

\textbf{Modality Gap:} Despite using the same EgoVLP encoder for both visual and textual embeddings, the semantic spaces may not be perfectly aligned. Concatenating these embeddings assumes they are comparable, but the projection layer alone may be insufficient to bridge domain differences, especially given limited training data.

\textbf{Graph Structure Simplification:} Our DAG construction uses only sequential edges from the recipe, ignoring richer dependencies (e.g., ingredient sharing, tool reuse). More sophisticated graph structures might capture procedural relationships better, but would require additional supervision or domain knowledge.

\subsection{Lessons Learned and Future Directions}

This work provides valuable insights for future research in procedural video understanding:

\begin{enumerate}
    \item \textbf{Step localization quality is critical:} Zero-shot methods like HiERO, while annotation-free, produce insufficient precision for error detection tasks requiring fine-grained step alignment. Future work should explore supervised or few-shot step localization, or develop robust error detection methods that are invariant to segmentation noise.
    
    \item \textbf{Dataset scale matters:} Graph-based multimodal approaches require substantially more training data. Promising directions include: (i) data augmentation via temporal transformations or synthetic errors, (ii) pre-training on unlabeled cooking videos with self-supervised objectives, (iii) transfer learning from related procedural domains (e.g., assembly, surgery).
    
    \item \textbf{Cross-validation strategy impacts results:} LOO provides recipe-level generalization estimates but suffers from high variance due to single-recipe test sets. Stratified K-Fold allows larger test sets but may overestimate performance by including same-recipe videos in train/test. Both perspectives are valuable for understanding model robustness.
    
    \item \textbf{Improved multimodal fusion:} Simple concatenation may not suffice. Future architectures should explore: (i) cross-modal attention mechanisms to dynamically weight visual vs. textual evidence, (ii) contrastive learning to align visual and text embeddings, (iii) gated fusion to allow adaptive integration based on input characteristics.
    
    \item \textbf{Interpretability matters:} Graph-based models offer potential for interpretability through attention visualization over graph structure. Understanding which step transitions or node features contribute to error predictions could provide actionable insights for users and improve model debugging.
\end{enumerate}

These lessons, while stemming from a challenging problem setting, provide concrete directions for advancing procedural error detection in egocentric video understanding.
