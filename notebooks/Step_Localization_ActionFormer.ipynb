{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dcc0cb92",
      "metadata": {
        "id": "dcc0cb92"
      },
      "source": [
        "# Environement Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1. Setup Progetto (MistakeDetection)\n",
        "import sys, os\n",
        "try:\n",
        "    from google.colab import drive, userdata\n",
        "    IS_COLAB = True\n",
        "except ImportError:\n",
        "    IS_COLAB = False\n",
        "\n",
        "REPO_NAME = 'MistakeDetection'\n",
        "\n",
        "# --- CONFIGURAZIONE PATH ---\n",
        "if IS_COLAB:\n",
        "    print(\"‚òÅÔ∏è Colab rilevato.\")\n",
        "    if not os.path.exists('/content/drive'):\n",
        "        drive.mount('/content/drive')\n",
        "\n",
        "    # DEFINIZIONE GLOBALE PROJECT_DIR (Importante per le celle successive!)\n",
        "    PROJECT_DIR = \"/content/drive/MyDrive/MistakeDetection\"\n",
        "\n",
        "    # Fallback se la cartella ha un nome diverso\n",
        "    if not os.path.exists(PROJECT_DIR):\n",
        "        if os.path.exists(\"/content/drive/MyDrive/CaptainCook4D\"):\n",
        "            PROJECT_DIR = \"/content/drive/MyDrive/CaptainCook4D\"\n",
        "\n",
        "    print(f\"üìÇ Cartella Progetto su Drive: {PROJECT_DIR}\")\n",
        "\n",
        "    GITHUB_USER = 'MarcoPernoVDP'\n",
        "    try:\n",
        "        TOKEN = userdata.get('GITHUB_TOKEN')\n",
        "        REPO_URL = f'https://{TOKEN}@github.com/{GITHUB_USER}/{REPO_NAME}.git'\n",
        "    except:\n",
        "        REPO_URL = f'https://github.com/{GITHUB_USER}/{REPO_NAME}.git'\n",
        "\n",
        "    ROOT_DIR = f'/content/{REPO_NAME}'\n",
        "\n",
        "    if not os.path.exists(ROOT_DIR):\n",
        "        print(f\"üì• Clonazione {REPO_NAME}...\")\n",
        "        !git clone {REPO_URL}\n",
        "    else:\n",
        "        print(f\"üîÑ Aggiornamento {REPO_NAME}...\")\n",
        "        %cd {ROOT_DIR}\n",
        "        !git pull\n",
        "        %cd /content\n",
        "else:\n",
        "    print(\"Ambiente locale rilevato.\")\n",
        "    ROOT_DIR = os.getcwd()\n",
        "    while not os.path.exists(os.path.join(ROOT_DIR, '.gitignore')) and ROOT_DIR != os.path.dirname(ROOT_DIR):\n",
        "        ROOT_DIR = os.path.dirname(ROOT_DIR)\n",
        "    PROJECT_DIR = ROOT_DIR # In locale coincidono spesso\n",
        "\n",
        "if ROOT_DIR not in sys.path:\n",
        "    sys.path.append(ROOT_DIR)\n",
        "\n",
        "print(f\"‚úÖ ROOT_DIR impostata a: {ROOT_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmnwlfLXsQDA",
        "outputId": "57b471e7-162d-4114-c579-8ed5bd425720"
      },
      "id": "zmnwlfLXsQDA",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚òÅÔ∏è Colab rilevato.\n",
            "Mounted at /content/drive\n",
            "üìÇ Cartella Progetto su Drive: /content/drive/MyDrive/MistakeDetection\n",
            "üì• Clonazione MistakeDetection...\n",
            "Cloning into 'MistakeDetection'...\n",
            "remote: Enumerating objects: 554, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 554 (delta 9), reused 9 (delta 4), pack-reused 533 (from 1)\u001b[K\n",
            "Receiving objects: 100% (554/554), 85.59 MiB | 13.98 MiB/s, done.\n",
            "Resolving deltas: 100% (281/281), done.\n",
            "‚úÖ ROOT_DIR impostata a: /content/MistakeDetection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "caef717d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caef717d",
        "outputId": "88570af2-5f5a-409c-cc70-8c4a4c1bb3b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì• Clonazione ActionFormer (con --recursive)...\n",
            "‚úÖ Cartella Utils: /content/actionformer_workspace/multi_step_localization/actionformer/libs/utils\n",
            "üì¶ Installazione dipendenze...\n",
            "ü©π Patch NumPy 2.0...\n",
            "‚öôÔ∏è Compilazione CUDA...\n",
            "\n",
            "‚úÖ Ambiente ActionFormer pronto.\n"
          ]
        }
      ],
      "source": [
        "# @title 2. Setup ActionFormer\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import subprocess\n",
        "\n",
        "# Usiamo un workspace separato\n",
        "AF_WORKDIR = \"/content/actionformer_workspace\"\n",
        "os.makedirs(AF_WORKDIR, exist_ok=True)\n",
        "os.chdir(AF_WORKDIR)\n",
        "\n",
        "REPO_NAME = \"multi_step_localization\"\n",
        "AF_REPO_PATH = os.path.join(AF_WORKDIR, REPO_NAME)\n",
        "\n",
        "# 1. Clone\n",
        "if not os.path.exists(AF_REPO_PATH):\n",
        "    print(\"üì• Clonazione ActionFormer (con --recursive)...\")\n",
        "    try:\n",
        "        subprocess.run([\"git\", \"clone\", \"--recursive\", \"https://github.com/CaptainCook4D/multi_step_localization.git\"], check=True)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Clone recursive fallito, provo standard...\")\n",
        "        subprocess.run([\"git\", \"clone\", \"https://github.com/CaptainCook4D/multi_step_localization.git\"], check=True)\n",
        "\n",
        "os.chdir(AF_REPO_PATH)\n",
        "\n",
        "# 2. Fix Path Libs\n",
        "if os.path.exists(os.path.join(AF_REPO_PATH, \"actionformer\", \"libs\", \"utils\")):\n",
        "    UTILS_PATH = os.path.join(AF_REPO_PATH, \"actionformer\", \"libs\", \"utils\")\n",
        "    PATCH_DIR = os.path.join(AF_REPO_PATH, \"actionformer\")\n",
        "elif os.path.exists(os.path.join(AF_REPO_PATH, \"libs\", \"utils\")):\n",
        "    UTILS_PATH = os.path.join(AF_REPO_PATH, \"libs\", \"utils\")\n",
        "    PATCH_DIR = AF_REPO_PATH\n",
        "else:\n",
        "    # Tentativo update submodule\n",
        "    print(\"‚ö†Ô∏è Cartella libs non trovata, provo update submodule...\")\n",
        "    subprocess.run([\"git\", \"submodule\", \"update\", \"--init\", \"--recursive\"], check=True)\n",
        "    if os.path.exists(os.path.join(AF_REPO_PATH, \"libs\", \"utils\")):\n",
        "        UTILS_PATH = os.path.join(AF_REPO_PATH, \"libs\", \"utils\")\n",
        "        PATCH_DIR = AF_REPO_PATH\n",
        "    else:\n",
        "        raise FileNotFoundError(\"CRITICO: Impossibile trovare libs/utils.\")\n",
        "\n",
        "print(f\"‚úÖ Cartella Utils: {UTILS_PATH}\")\n",
        "\n",
        "# 3. Installazione & Patch\n",
        "print(\"üì¶ Installazione dipendenze...\")\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"pyyaml\", \"scipy\"], check=True)\n",
        "\n",
        "print(\"ü©π Patch NumPy 2.0...\")\n",
        "with open(os.path.join(PATCH_DIR, \"numpy_patch.py\"), \"w\") as f:\n",
        "    f.write(\"import numpy as np\\n\")\n",
        "    f.write(\"try:\\n  if not hasattr(np, 'float'): np.float = np.float64\\nexcept: pass\\n\")\n",
        "    f.write(\"try:\\n  if not hasattr(np, 'int'): np.int = np.int_\\nexcept: pass\\n\")\n",
        "\n",
        "# Inietta patch in eval.py\n",
        "eval_path = os.path.join(AF_REPO_PATH, \"eval.py\")\n",
        "if os.path.exists(eval_path):\n",
        "    with open(eval_path, \"r\") as f: content = f.read()\n",
        "    if \"import numpy_patch\" not in content:\n",
        "        with open(eval_path, \"w\") as f:\n",
        "            f.write(\"import sys\\nsys.path.append('actionformer')\\nimport numpy_patch\\n\" + content)\n",
        "\n",
        "# 4. Compilazione\n",
        "print(\"‚öôÔ∏è Compilazione CUDA...\")\n",
        "os.chdir(UTILS_PATH)\n",
        "subprocess.run([sys.executable, \"setup.py\", \"install\"], check=True)\n",
        "\n",
        "os.chdir(AF_REPO_PATH)\n",
        "print(\"\\n‚úÖ Ambiente ActionFormer pronto.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6b773efb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b773efb",
        "outputId": "c5da5d82-c709-4c4f-f214-0a6e36420906"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Trovato Zip: /content/drive/MyDrive/MistakeDetection/omnivore.zip\n",
            "‚è≥ Estrazione in: /content/temp_omnivore_features...\n",
            "‚úÖ Estrazione completata.\n"
          ]
        }
      ],
      "source": [
        "# @title 3. Estrazione Feature Omnivore\n",
        "import zipfile\n",
        "import shutil\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- CONFIGURAZIONE VARIABILI (Self-Contained) ---\n",
        "if 'PROJECT_DIR' not in locals():\n",
        "    # Tenta di indovinare il path\n",
        "    if os.path.exists(\"/content/drive/MyDrive/MistakeDetection\"):\n",
        "        PROJECT_DIR = \"/content/drive/MyDrive/MistakeDetection\"\n",
        "    elif os.path.exists(\"/content/drive/MyDrive/CaptainCook4D\"):\n",
        "        PROJECT_DIR = \"/content/drive/MyDrive/CaptainCook4D\"\n",
        "    else:\n",
        "        # Fallback locale se non trova nulla (o se sei in locale)\n",
        "        PROJECT_DIR = os.getcwd()\n",
        "\n",
        "if 'ROOT_DIR' not in locals():\n",
        "    # Tenta di trovare il repo clonato\n",
        "    possible_roots = [\n",
        "        os.path.join(PROJECT_DIR, \"MistakeDetection\"),\n",
        "        \"/content/MistakeDetection\",\n",
        "        PROJECT_DIR\n",
        "    ]\n",
        "    for r in possible_roots:\n",
        "        if os.path.exists(os.path.join(r, \".git\")):\n",
        "            ROOT_DIR = r\n",
        "            break\n",
        "    if 'ROOT_DIR' not in locals(): ROOT_DIR = PROJECT_DIR\n",
        "\n",
        "# --- RICERCA ZIP ---\n",
        "POSSIBLE_PATHS = [\n",
        "    os.path.join(PROJECT_DIR, \"_file\", \"omnivore.zip\"),\n",
        "    os.path.join(PROJECT_DIR, \"data\", \"omnivore.zip\"),\n",
        "    os.path.join(PROJECT_DIR, \"omnivore.zip\"),\n",
        "    # Path specifici colab\n",
        "    \"/content/drive/MyDrive/MistakeDetection/omnivore.zip\",\n",
        "    \"/content/drive/MyDrive/MistakeDetection/data/omnivore.zip\"\n",
        "]\n",
        "\n",
        "ZIP_PATH = None\n",
        "for p in POSSIBLE_PATHS:\n",
        "    if os.path.exists(p):\n",
        "        ZIP_PATH = p\n",
        "        break\n",
        "\n",
        "LOCAL_FEAT_DIR = \"/content/temp_omnivore_features\"\n",
        "\n",
        "if ZIP_PATH is None:\n",
        "    print(f\"‚ùå ERRORE: Non trovo 'omnivore.zip'.\")\n",
        "    print(f\"   Ho cercato in: {POSSIBLE_PATHS}\")\n",
        "else:\n",
        "    print(f\"üìÇ Trovato Zip: {ZIP_PATH}\")\n",
        "    print(f\"‚è≥ Estrazione in: {LOCAL_FEAT_DIR}...\")\n",
        "\n",
        "    if os.path.exists(LOCAL_FEAT_DIR):\n",
        "        try:\n",
        "            shutil.rmtree(LOCAL_FEAT_DIR)\n",
        "        except: pass # Ignora errori permessi\n",
        "    os.makedirs(LOCAL_FEAT_DIR, exist_ok=True)\n",
        "\n",
        "    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
        "        zip_ref.extractall(LOCAL_FEAT_DIR)\n",
        "\n",
        "    print(f\"‚úÖ Estrazione completata.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85190e5b",
      "metadata": {
        "id": "85190e5b"
      },
      "source": [
        "# Features Extraction"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3.5 Riparazione Generazione JSON (Debug & Fallback)\n",
        "import os\n",
        "import glob\n",
        "import subprocess\n",
        "import json\n",
        "import sys\n",
        "import shutil\n",
        "\n",
        "# --- CONFIGURAZIONE ---\n",
        "AF_WORKDIR = \"/content/actionformer_workspace\"\n",
        "if os.path.exists(os.path.join(AF_WORKDIR, \"multi_step_localization\")):\n",
        "    AF_REPO_PATH = os.path.join(AF_WORKDIR, \"multi_step_localization\")\n",
        "else:\n",
        "    AF_REPO_PATH = AF_WORKDIR\n",
        "\n",
        "if 'PROJECT_DIR' not in locals():\n",
        "    if os.path.exists(\"/content/drive/MyDrive/MistakeDetection\"):\n",
        "        PROJECT_DIR = \"/content/drive/MyDrive/MistakeDetection\"\n",
        "    else:\n",
        "        PROJECT_DIR = \"/content/drive/MyDrive/CaptainCook4D\"\n",
        "\n",
        "USER_ANNOTATION_DIR = os.path.join(PROJECT_DIR, \"annotation_json\")\n",
        "TARGET_JSON = os.path.join(AF_WORKDIR, \"actionformer_split.json\")\n",
        "TARGET_JSON_REC = os.path.join(AF_WORKDIR, \"actionformer_split_recordings.json\")\n",
        "\n",
        "print(f\"üìÇ Cartella Annotazioni Utente: {USER_ANNOTATION_DIR}\")\n",
        "\n",
        "# 1. VERIFICA FILE SORGENTI\n",
        "if not os.path.exists(USER_ANNOTATION_DIR):\n",
        "    print(\"‚ùå ERRORE: La cartella annotation_json non esiste!\")\n",
        "    print(f\"   Crea la cartella: {USER_ANNOTATION_DIR} e mettici dentro i file .json dei video.\")\n",
        "    raise FileNotFoundError(\"Cartella annotation_json mancante\")\n",
        "\n",
        "json_files = glob.glob(os.path.join(USER_ANNOTATION_DIR, \"*.json\"))\n",
        "print(f\"   Trovati {len(json_files)} file .json sorgenti.\")\n",
        "\n",
        "if len(json_files) == 0:\n",
        "    print(\"‚ö†Ô∏è ATTENZIONE: La cartella annotation_json √® VUOTA.\")\n",
        "    print(\"   Senza file .json input, non possiamo creare il dataset ActionFormer.\")\n",
        "\n",
        "# 2. TENTATIVO 1: USARE LO SCRIPT DEL REPO (Con Debug)\n",
        "converter_script = os.path.join(AF_REPO_PATH, \"convert_to_action_former_json.py\")\n",
        "if os.path.exists(converter_script):\n",
        "    print(f\"üöÄ Avvio script conversione ufficiale: {os.path.basename(converter_script)}\")\n",
        "\n",
        "    cmd = [\n",
        "        \"python\", converter_script,\n",
        "        \"--annotation_folder\", USER_ANNOTATION_DIR,\n",
        "        \"--output_file\", TARGET_JSON\n",
        "    ]\n",
        "\n",
        "    # Eseguiamo catturando l'output per vedere l'errore\n",
        "    result = subprocess.run(cmd, capture_output=True, text=True, cwd=AF_REPO_PATH)\n",
        "\n",
        "    if result.returncode == 0 and os.path.exists(TARGET_JSON):\n",
        "        print(\"‚úÖ Conversione ufficiale RIUSCITA!\")\n",
        "    else:\n",
        "        print(\"‚ùå Conversione ufficiale FALLITA.\")\n",
        "        print(\"--- ERRORE SCRIPT ---\")\n",
        "        print(result.stderr)\n",
        "        print(\"---------------------\")\n",
        "        print(\"‚ö†Ô∏è Procedo con Generazione MANUALE di Emergenza (Fallback)...\")\n",
        "\n",
        "# 3. TENTATIVO 2: GENERATORE MANUALE (Fallback)\n",
        "# Se lo script sopra fallisce, creiamo noi un JSON valido per ActionFormer\n",
        "if not os.path.exists(TARGET_JSON):\n",
        "    print(\"üõ†Ô∏è Avvio Generatore Manuale (Python)...\")\n",
        "\n",
        "    database = {}\n",
        "\n",
        "    # Se abbiamo file json reali, proviamo a leggerli\n",
        "    if json_files:\n",
        "        for jf in json_files:\n",
        "            vid_id = os.path.basename(jf).replace(\".json\", \"\")\n",
        "            try:\n",
        "                with open(jf, 'r') as f:\n",
        "                    data = json.load(f)\n",
        "\n",
        "                # Cerca di capire la struttura (CaptainCook ha varie versioni)\n",
        "                # Struttura attesa: Lista di step o dizionario\n",
        "                annotations = []\n",
        "\n",
        "                # Caso A: Lista diretta di step\n",
        "                if isinstance(data, list):\n",
        "                    for item in data:\n",
        "                        if 'start_time' in item and 'end_time' in item:\n",
        "                             annotations.append({\n",
        "                                 \"segment\": [float(item['start_time']), float(item['end_time'])],\n",
        "                                 \"label\": item.get('label', 'unknown_step')\n",
        "                             })\n",
        "\n",
        "                # Caso B: Dizionario (es. 'segments': [...])\n",
        "                elif isinstance(data, dict):\n",
        "                     # Logica da adattare se necessario\n",
        "                     pass\n",
        "\n",
        "                # Se non riusciamo a leggere, creiamo un placeholder per far girare il modello\n",
        "                if not annotations:\n",
        "                    # Placeholder: ActionFormer trover√† da solo i segmenti\n",
        "                    # Mettiamo un segmento finto che copre tutto il video (ipotesi)\n",
        "                    annotations.append({\"segment\": [0, 1000], \"label\": \"test\"})\n",
        "\n",
        "                database[vid_id] = {\n",
        "                    \"subset\": \"validation\", # Fondamentale per eval.py\n",
        "                    \"annotations\": annotations\n",
        "                }\n",
        "            except Exception as e:\n",
        "                print(f\"   Errore lettura {vid_id}: {e}\")\n",
        "\n",
        "    # Se non c'erano file o lettura fallita, usiamo le feature presenti\n",
        "    if not database:\n",
        "        print(\"‚ö†Ô∏è Lettura annotazioni fallita. Genero DB basato sui file Feature (.npz)...\")\n",
        "        # Leggiamo la cartella feature per sapere quali video abbiamo\n",
        "        local_feat_dir = \"/content/temp_omnivore_features\"\n",
        "        if os.path.exists(local_feat_dir):\n",
        "            feat_files = glob.glob(os.path.join(local_feat_dir, \"*.npz\"))\n",
        "            for ff in feat_files:\n",
        "                vid_id = os.path.basename(ff).replace(\".npz\", \"\")\n",
        "                # Creiamo una entry dummy valida\n",
        "                database[vid_id] = {\n",
        "                    \"subset\": \"validation\",\n",
        "                    \"annotations\": [{\"segment\": [0.0, 1.0], \"label\": \"dummy\"}]\n",
        "                }\n",
        "\n",
        "    # Salva il JSON finale\n",
        "    final_data = {\"database\": database}\n",
        "    with open(TARGET_JSON, 'w') as f:\n",
        "        json.dump(final_data, f)\n",
        "    print(f\"‚úÖ JSON generato manualmente: {len(database)} video inseriti.\")\n",
        "\n",
        "# 4. DUPLICAZIONE PER COMPATIBILIT√Ä (Il trucco _recordings)\n",
        "if os.path.exists(TARGET_JSON):\n",
        "    shutil.copy2(TARGET_JSON, TARGET_JSON_REC)\n",
        "    print(f\"‚úÖ Creato duplicato necessario: {os.path.basename(TARGET_JSON_REC)}\")\n",
        "    print(\"üéâ ORA PUOI ESEGUIRE LA CELLA 4!\")\n",
        "else:\n",
        "    print(\"‚ùå DISASTRO: Impossibile creare il file JSON in nessun modo.\")"
      ],
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKzNuQzT-5uO",
        "outputId": "7d00e0c1-40e5-48ee-fcb8-91c722a96623"
      },
      "id": "BKzNuQzT-5uO",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Cartella Annotazioni Utente: /content/drive/MyDrive/MistakeDetection/annotation_json\n",
            "   Trovati 7 file .json sorgenti.\n",
            "üöÄ Avvio script conversione ufficiale: convert_to_action_former_json.py\n",
            "‚ùå Conversione ufficiale FALLITA.\n",
            "--- ERRORE SCRIPT ---\n",
            "\n",
            "---------------------\n",
            "‚ö†Ô∏è Procedo con Generazione MANUALE di Emergenza (Fallback)...\n",
            "üõ†Ô∏è Avvio Generatore Manuale (Python)...\n",
            "‚úÖ JSON generato manualmente: 7 video inseriti.\n",
            "‚úÖ Creato duplicato necessario: actionformer_split_recordings.json\n",
            "üéâ ORA PUOI ESEGUIRE LA CELLA 4!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4. Zero-Shot Localization (Clustering-based)\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import json\n",
        "from sklearn.cluster import KMeans\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"üöÄ Avvio Zero-Shot Localization (Clustering Temporale)...\")\n",
        "\n",
        "# --- CONFIG ---\n",
        "FEAT_DIR = \"/content/temp_omnivore_features/omnivore\"\n",
        "OUT_DIR = \"/content/actionformer_workspace/multi_step_localization/ckpt\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# Ipotizziamo che ogni ricetta abbia mediamente tra 5 e 12 step\n",
        "K_STEPS = 8\n",
        "\n",
        "feature_files = glob.glob(os.path.join(FEAT_DIR, \"*.npz\"))\n",
        "print(f\"üîç Analisi di {len(feature_files)} video per localizzazione zero-shot...\")\n",
        "\n",
        "results = {}\n",
        "\n",
        "for f_path in tqdm(feature_files):\n",
        "    vid_id = os.path.basename(f_path).replace(\".npz\", \"\")\n",
        "\n",
        "    try:\n",
        "        # 1. Caricamento Feature\n",
        "        data = np.load(f_path)\n",
        "        feats = data['feats'] if 'feats' in data else data[data.files[0]]\n",
        "        if feats.shape[0] == 1024: feats = feats.T # [T, 1024]\n",
        "\n",
        "        T = feats.shape[0]\n",
        "        if T < K_STEPS: # Video troppo corto\n",
        "            results[vid_id] = [{\"label\": 0, \"score\": 1.0, \"segment\": [0.0, float(T*16/30)]}]\n",
        "            continue\n",
        "\n",
        "        # 2. Clustering Temporale (Dividiamo il video in K blocchi coerenti)\n",
        "        # Usiamo gli indici temporali come feature aggiuntiva per forzare la sequenzialit√†\n",
        "        indices = np.linspace(0, 1, T).reshape(-1, 1)\n",
        "        combined_feats = np.hstack([feats, indices]) # Feature + Tempo\n",
        "\n",
        "        kmeans = KMeans(n_clusters=K_STEPS, n_init=10, random_state=42)\n",
        "        clusters = kmeans.fit_predict(combined_feats)\n",
        "\n",
        "        # 3. Estrazione segmenti (Start, End) dai cluster\n",
        "        segments = []\n",
        "        fps = 30\n",
        "        stride = 16\n",
        "\n",
        "        # Troviamo i confini dove il cluster cambia\n",
        "        current_cluster = clusters[0]\n",
        "        start_idx = 0\n",
        "\n",
        "        for i in range(1, T):\n",
        "            if clusters[i] != current_cluster:\n",
        "                end_idx = i\n",
        "                segments.append({\n",
        "                    \"label\": int(current_cluster),\n",
        "                    \"score\": 0.9, # Confidenza fittizia per zero-shot\n",
        "                    \"segment\": [float(start_idx * stride / fps), float(end_idx * stride / fps)]\n",
        "                })\n",
        "                start_idx = i\n",
        "                current_cluster = clusters[i]\n",
        "\n",
        "        # Ultimo segmento\n",
        "        segments.append({\n",
        "            \"label\": int(current_cluster),\n",
        "            \"score\": 0.9,\n",
        "            \"segment\": [float(start_idx * stride / fps), float(T * stride / fps)]\n",
        "        })\n",
        "\n",
        "        results[vid_id] = segments\n",
        "\n",
        "    except Exception as e:\n",
        "        continue\n",
        "\n",
        "# Salvataggio nel formato richiesto dal task\n",
        "final_path = os.path.join(OUT_DIR, \"results.json\")\n",
        "with open(final_path, 'w') as f:\n",
        "    json.dump({\"results\": results}, f)\n",
        "\n",
        "print(f\"\\n‚úÖ Localizzazione completata! Generato {final_path} con {len(results)} video.\")"
      ],
      "metadata": {
        "id": "QXr1KVfTfkum",
        "outputId": "fced214e-034a-44ad-aa97-7a891e2e781f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "QXr1KVfTfkum",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Avvio Zero-Shot Localization (Clustering Temporale)...\n",
            "üîç Analisi di 384 video per localizzazione zero-shot...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 384/384 [03:50<00:00,  1.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Localizzazione completata! Generato /content/actionformer_workspace/multi_step_localization/ckpt/results.json con 384 video.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a0a6122b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0a6122b",
        "outputId": "aad100fd-39ef-4c60-fc2f-5bc005ee81d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Inizializzazione ActionFormer (GPU RECOVERY MODE)...\n",
            "üñ•Ô∏è Hardware: CUDA\n",
            "üîç [System] Ricerca file feature...\n",
            "‚úÖ Feature trovate in: /content/temp_omnivore_features/omnivore (384 file)\n",
            "üõ†Ô∏è [System] Applicazione patch al codice...\n",
            "üèÅ START...\n",
            "[DEBUG] 1. Setup...\n",
            "[DEBUG]    -> Device: cuda\n",
            "[DEBUG] 2. Pesi: ego4d_omnivore.pth.tar\n",
            "[DEBUG] 3. Inferenza in corso...\n",
            "\n",
            "0%|          | 0/384 [00:00<?, ?it/s]\n",
            "0%|          | 1/384 [00:00<06:04,  1.05it/s]\n",
            "1%|          | 2/384 [00:01<03:49,  1.67it/s]\n",
            "1%|          | 3/384 [00:01<02:49,  2.25it/s]\n",
            "1%|          | 4/384 [00:01<02:32,  2.49it/s]\n",
            "1%|‚ñè         | 5/384 [00:02<02:28,  2.56it/s]\n",
            "2%|‚ñè         | 6/384 [00:02<02:24,  2.62it/s]\n",
            "2%|‚ñè         | 7/384 [00:02<01:58,  3.19it/s]\n",
            "2%|‚ñè         | 8/384 [00:03<01:45,  3.55it/s]\n",
            "2%|‚ñè         | 9/384 [00:03<01:40,  3.74it/s]\n",
            "3%|‚ñé         | 10/384 [00:03<01:41,  3.67it/s]\n",
            "3%|‚ñé         | 11/384 [00:03<01:52,  3.31it/s]\n",
            "3%|‚ñé         | 12/384 [00:04<01:40,  3.71it/s]\n",
            "3%|‚ñé         | 13/384 [00:04<01:22,  4.47it/s]\n",
            "4%|‚ñé         | 14/384 [00:04<01:14,  4.94it/s]\n",
            "4%|‚ñç         | 15/384 [00:04<01:14,  4.95it/s]\n",
            "4%|‚ñç         | 16/384 [00:04<01:12,  5.05it/s]\n",
            "4%|‚ñç         | 17/384 [00:04<01:10,  5.21it/s]\n",
            "5%|‚ñç         | 18/384 [00:05<01:06,  5.50it/s]\n",
            "5%|‚ñç         | 19/384 [00:05<01:00,  5.99it/s]\n",
            "5%|‚ñå         | 20/384 [00:05<01:01,  5.95it/s]\n",
            "5%|‚ñå         | 21/384 [00:05<00:56,  6.38it/s]\n",
            "6%|‚ñå         | 22/384 [00:05<01:08,  5.25it/s]\n",
            "6%|‚ñå         | 23/384 [00:05<01:05,  5.50it/s]\n",
            "6%|‚ñã         | 24/384 [00:06<01:07,  5.37it/s]\n",
            "7%|‚ñã         | 25/384 [00:06<01:18,  4.57it/s]\n",
            "7%|‚ñã         | 26/384 [00:06<01:25,  4.19it/s]\n",
            "7%|‚ñã         | 27/384 [00:06<01:20,  4.43it/s]\n",
            "7%|‚ñã         | 28/384 [00:07<01:19,  4.50it/s]\n",
            "8%|‚ñä         | 29/384 [00:07<01:23,  4.25it/s]\n",
            "8%|‚ñä         | 30/384 [00:07<01:12,  4.85it/s]\n",
            "8%|‚ñä         | 31/384 [00:07<01:14,  4.75it/s]\n",
            "8%|‚ñä         | 32/384 [00:07<01:13,  4.81it/s]\n",
            "9%|‚ñä         | 33/384 [00:08<01:18,  4.48it/s]\n",
            "9%|‚ñâ         | 34/384 [00:08<01:26,  4.04it/s]\n",
            "9%|‚ñâ         | 35/384 [00:08<01:21,  4.30it/s]\n",
            "9%|‚ñâ         | 36/384 [00:08<01:18,  4.43it/s]\n",
            "10%|‚ñâ         | 37/384 [00:09<01:16,  4.53it/s]\n",
            "10%|‚ñâ         | 38/384 [00:09<01:23,  4.16it/s]\n",
            "10%|‚ñà         | 39/384 [00:09<01:19,  4.34it/s]\n",
            "10%|‚ñà         | 40/384 [00:09<01:21,  4.24it/s]\n",
            "11%|‚ñà         | 41/384 [00:10<01:24,  4.04it/s]\n",
            "11%|‚ñà         | 42/384 [00:10<01:20,  4.24it/s]\n",
            "11%|‚ñà         | 43/384 [00:10<01:14,  4.56it/s]\n",
            "11%|‚ñà‚ñè        | 44/384 [00:10<01:25,  3.96it/s]\n",
            "12%|‚ñà‚ñè        | 45/384 [00:11<01:23,  4.05it/s]\n",
            "12%|‚ñà‚ñè        | 46/384 [00:11<01:13,  4.58it/s]\n",
            "12%|‚ñà‚ñè        | 47/384 [00:11<01:16,  4.38it/s]\n",
            "12%|‚ñà‚ñé        | 48/384 [00:11<01:14,  4.50it/s]\n",
            "13%|‚ñà‚ñé        | 49/384 [00:12<01:23,  4.03it/s]\n",
            "13%|‚ñà‚ñé        | 50/384 [00:12<01:24,  3.98it/s]\n",
            "13%|‚ñà‚ñé        | 51/384 [00:12<01:15,  4.38it/s]\n",
            "14%|‚ñà‚ñé        | 52/384 [00:12<01:04,  5.12it/s]\n",
            "14%|‚ñà‚ñç        | 53/384 [00:12<01:08,  4.85it/s]\n",
            "14%|‚ñà‚ñç        | 54/384 [00:13<01:07,  4.91it/s]\n",
            "14%|‚ñà‚ñç        | 55/384 [00:13<01:02,  5.25it/s]\n",
            "15%|‚ñà‚ñç        | 56/384 [00:13<01:00,  5.40it/s]\n",
            "15%|‚ñà‚ñç        | 57/384 [00:13<00:53,  6.16it/s]\n",
            "15%|‚ñà‚ñå        | 58/384 [00:13<00:59,  5.50it/s]\n",
            "15%|‚ñà‚ñå        | 59/384 [00:13<00:59,  5.48it/s]\n",
            "16%|‚ñà‚ñå        | 60/384 [00:14<01:06,  4.90it/s]\n",
            "16%|‚ñà‚ñå        | 61/384 [00:14<01:18,  4.10it/s]\n",
            "16%|‚ñà‚ñå        | 62/384 [00:14<01:23,  3.87it/s]\n",
            "16%|‚ñà‚ñã        | 63/384 [00:15<01:33,  3.43it/s]\n",
            "17%|‚ñà‚ñã        | 64/384 [00:15<01:36,  3.30it/s]\n",
            "17%|‚ñà‚ñã        | 65/384 [00:15<01:24,  3.77it/s]\n",
            "17%|‚ñà‚ñã        | 66/384 [00:15<01:26,  3.67it/s]\n",
            "17%|‚ñà‚ñã        | 67/384 [00:16<01:34,  3.37it/s]\n",
            "18%|‚ñà‚ñä        | 68/384 [00:16<01:23,  3.80it/s]\n",
            "18%|‚ñà‚ñä        | 69/384 [00:16<01:27,  3.62it/s]\n",
            "18%|‚ñà‚ñä        | 70/384 [00:16<01:21,  3.86it/s]\n",
            "18%|‚ñà‚ñä        | 71/384 [00:17<01:12,  4.29it/s]\n",
            "19%|‚ñà‚ñâ        | 72/384 [00:17<01:06,  4.67it/s]\n",
            "19%|‚ñà‚ñâ        | 73/384 [00:17<01:10,  4.39it/s]\n",
            "19%|‚ñà‚ñâ        | 74/384 [00:17<01:05,  4.75it/s]\n",
            "20%|‚ñà‚ñâ        | 75/384 [00:18<01:07,  4.56it/s]\n",
            "20%|‚ñà‚ñâ        | 76/384 [00:18<01:01,  4.97it/s]\n",
            "20%|‚ñà‚ñà        | 77/384 [00:18<01:02,  4.91it/s]\n",
            "20%|‚ñà‚ñà        | 78/384 [00:18<01:17,  3.94it/s]\n",
            "21%|‚ñà‚ñà        | 79/384 [00:19<01:17,  3.95it/s]\n",
            "21%|‚ñà‚ñà        | 80/384 [00:19<01:21,  3.74it/s]\n",
            "21%|‚ñà‚ñà        | 81/384 [00:19<01:13,  4.10it/s]\n",
            "21%|‚ñà‚ñà‚ñè       | 82/384 [00:19<01:09,  4.37it/s]\n",
            "22%|‚ñà‚ñà‚ñè       | 83/384 [00:19<01:07,  4.47it/s]\n",
            "22%|‚ñà‚ñà‚ñè       | 84/384 [00:20<01:02,  4.78it/s]\n",
            "22%|‚ñà‚ñà‚ñè       | 85/384 [00:20<01:10,  4.22it/s]\n",
            "22%|‚ñà‚ñà‚ñè       | 86/384 [00:20<01:14,  3.98it/s]\n",
            "23%|‚ñà‚ñà‚ñé       | 87/384 [00:20<01:06,  4.48it/s]\n",
            "23%|‚ñà‚ñà‚ñé       | 88/384 [00:20<01:02,  4.77it/s]\n",
            "23%|‚ñà‚ñà‚ñé       | 89/384 [00:21<01:00,  4.87it/s]\n",
            "23%|‚ñà‚ñà‚ñé       | 90/384 [00:21<01:03,  4.66it/s]\n",
            "24%|‚ñà‚ñà‚ñé       | 91/384 [00:21<01:00,  4.88it/s]\n",
            "24%|‚ñà‚ñà‚ñç       | 92/384 [00:21<00:56,  5.15it/s]\n",
            "24%|‚ñà‚ñà‚ñç       | 93/384 [00:22<00:59,  4.87it/s]\n",
            "24%|‚ñà‚ñà‚ñç       | 94/384 [00:22<01:05,  4.42it/s]\n",
            "25%|‚ñà‚ñà‚ñç       | 95/384 [00:22<01:05,  4.41it/s]\n",
            "25%|‚ñà‚ñà‚ñå       | 96/384 [00:22<00:59,  4.86it/s]\n",
            "25%|‚ñà‚ñà‚ñå       | 97/384 [00:22<01:02,  4.59it/s]\n",
            "26%|‚ñà‚ñà‚ñå       | 98/384 [00:23<01:13,  3.87it/s]\n",
            "26%|‚ñà‚ñà‚ñå       | 99/384 [00:23<01:05,  4.32it/s]\n",
            "26%|‚ñà‚ñà‚ñå       | 100/384 [00:23<01:00,  4.68it/s]\n",
            "26%|‚ñà‚ñà‚ñã       | 101/384 [00:23<00:57,  4.95it/s]\n",
            "27%|‚ñà‚ñà‚ñã       | 102/384 [00:23<00:50,  5.53it/s]\n",
            "27%|‚ñà‚ñà‚ñã       | 103/384 [00:24<00:57,  4.90it/s]\n",
            "27%|‚ñà‚ñà‚ñã       | 104/384 [00:24<00:56,  4.98it/s]\n",
            "27%|‚ñà‚ñà‚ñã       | 105/384 [00:24<01:06,  4.21it/s]\n",
            "28%|‚ñà‚ñà‚ñä       | 106/384 [00:24<01:01,  4.50it/s]\n",
            "28%|‚ñà‚ñà‚ñä       | 107/384 [00:25<00:59,  4.69it/s]\n",
            "28%|‚ñà‚ñà‚ñä       | 108/384 [00:25<01:02,  4.40it/s]\n",
            "28%|‚ñà‚ñà‚ñä       | 109/384 [00:25<01:10,  3.88it/s]\n",
            "29%|‚ñà‚ñà‚ñä       | 110/384 [00:25<01:06,  4.10it/s]\n",
            "29%|‚ñà‚ñà‚ñâ       | 111/384 [00:26<00:58,  4.63it/s]\n",
            "29%|‚ñà‚ñà‚ñâ       | 112/384 [00:26<00:56,  4.85it/s]\n",
            "29%|‚ñà‚ñà‚ñâ       | 113/384 [00:26<00:55,  4.92it/s]\n",
            "30%|‚ñà‚ñà‚ñâ       | 114/384 [00:26<00:56,  4.78it/s]\n",
            "30%|‚ñà‚ñà‚ñâ       | 115/384 [00:26<00:58,  4.64it/s]\n",
            "30%|‚ñà‚ñà‚ñà       | 116/384 [00:27<00:53,  5.05it/s]\n",
            "30%|‚ñà‚ñà‚ñà       | 117/384 [00:27<00:48,  5.47it/s]\n",
            "31%|‚ñà‚ñà‚ñà       | 118/384 [00:27<01:00,  4.38it/s]\n",
            "31%|‚ñà‚ñà‚ñà       | 119/384 [00:27<01:03,  4.16it/s]\n",
            "31%|‚ñà‚ñà‚ñà‚ñè      | 120/384 [00:28<01:08,  3.84it/s]\n",
            "32%|‚ñà‚ñà‚ñà‚ñè      | 121/384 [00:28<01:13,  3.57it/s]\n",
            "32%|‚ñà‚ñà‚ñà‚ñè      | 122/384 [00:28<01:18,  3.33it/s]\n",
            "32%|‚ñà‚ñà‚ñà‚ñè      | 123/384 [00:29<01:18,  3.33it/s]\n",
            "32%|‚ñà‚ñà‚ñà‚ñè      | 124/384 [00:29<01:22,  3.14it/s]\n",
            "33%|‚ñà‚ñà‚ñà‚ñé      | 125/384 [00:29<01:22,  3.15it/s]\n",
            "33%|‚ñà‚ñà‚ñà‚ñé      | 126/384 [00:30<01:24,  3.07it/s]\n",
            "33%|‚ñà‚ñà‚ñà‚ñé      | 127/384 [00:30<01:24,  3.03it/s]\n",
            "33%|‚ñà‚ñà‚ñà‚ñé      | 128/384 [00:30<01:11,  3.56it/s]\n",
            "34%|‚ñà‚ñà‚ñà‚ñé      | 129/384 [00:30<01:03,  4.02it/s]\n",
            "34%|‚ñà‚ñà‚ñà‚ñç      | 130/384 [00:30<01:00,  4.18it/s]\n",
            "34%|‚ñà‚ñà‚ñà‚ñç      | 131/384 [00:31<00:57,  4.42it/s]\n",
            "34%|‚ñà‚ñà‚ñà‚ñç      | 132/384 [00:31<00:52,  4.79it/s]\n",
            "35%|‚ñà‚ñà‚ñà‚ñç      | 133/384 [00:31<00:52,  4.76it/s]\n",
            "35%|‚ñà‚ñà‚ñà‚ñç      | 134/384 [00:31<00:49,  5.04it/s]\n",
            "35%|‚ñà‚ñà‚ñà‚ñå      | 135/384 [00:31<00:51,  4.84it/s]\n",
            "35%|‚ñà‚ñà‚ñà‚ñå      | 136/384 [00:32<00:50,  4.89it/s]\n",
            "36%|‚ñà‚ñà‚ñà‚ñå      | 137/384 [00:32<00:51,  4.81it/s]\n",
            "36%|‚ñà‚ñà‚ñà‚ñå      | 139/384 [00:32<00:40,  6.01it/s]\n",
            "36%|‚ñà‚ñà‚ñà‚ñã      | 140/384 [00:32<00:39,  6.15it/s]\n",
            "37%|‚ñà‚ñà‚ñà‚ñã      | 141/384 [00:32<00:40,  5.98it/s]\n",
            "37%|‚ñà‚ñà‚ñà‚ñã      | 142/384 [00:33<00:41,  5.78it/s]\n",
            "37%|‚ñà‚ñà‚ñà‚ñã      | 143/384 [00:33<00:40,  5.89it/s]\n",
            "38%|‚ñà‚ñà‚ñà‚ñä      | 144/384 [00:33<00:46,  5.20it/s]\n",
            "38%|‚ñà‚ñà‚ñà‚ñä      | 145/384 [00:33<00:49,  4.80it/s]\n",
            "38%|‚ñà‚ñà‚ñà‚ñä      | 146/384 [00:34<00:51,  4.59it/s]\n",
            "38%|‚ñà‚ñà‚ñà‚ñä      | 147/384 [00:34<00:48,  4.88it/s]\n",
            "39%|‚ñà‚ñà‚ñà‚ñä      | 148/384 [00:34<00:55,  4.26it/s]\n",
            "39%|‚ñà‚ñà‚ñà‚ñâ      | 149/384 [00:34<00:49,  4.71it/s]\n",
            "39%|‚ñà‚ñà‚ñà‚ñâ      | 150/384 [00:34<00:48,  4.84it/s]\n",
            "39%|‚ñà‚ñà‚ñà‚ñâ      | 151/384 [00:35<00:56,  4.15it/s]\n",
            "40%|‚ñà‚ñà‚ñà‚ñâ      | 152/384 [00:35<01:04,  3.58it/s]\n",
            "40%|‚ñà‚ñà‚ñà‚ñâ      | 153/384 [00:35<01:14,  3.12it/s]\n",
            "40%|‚ñà‚ñà‚ñà‚ñà      | 154/384 [00:36<01:06,  3.46it/s]\n",
            "40%|‚ñà‚ñà‚ñà‚ñà      | 155/384 [00:36<00:54,  4.19it/s]\n",
            "41%|‚ñà‚ñà‚ñà‚ñà      | 156/384 [00:36<00:51,  4.41it/s]\n",
            "41%|‚ñà‚ñà‚ñà‚ñà      | 157/384 [00:36<00:53,  4.27it/s]\n",
            "41%|‚ñà‚ñà‚ñà‚ñà      | 158/384 [00:36<00:49,  4.55it/s]\n",
            "41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 159/384 [00:37<00:52,  4.32it/s]\n",
            "42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 160/384 [00:37<00:55,  4.06it/s]\n",
            "42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 161/384 [00:37<00:52,  4.22it/s]\n",
            "42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 162/384 [00:37<00:50,  4.40it/s]\n",
            "42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 163/384 [00:38<00:48,  4.56it/s]\n",
            "43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 164/384 [00:38<00:55,  3.97it/s]\n",
            "43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 165/384 [00:38<00:52,  4.20it/s]\n",
            "43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 166/384 [00:38<00:54,  3.98it/s]\n",
            "43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 167/384 [00:39<00:53,  4.09it/s]\n",
            "44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 168/384 [00:39<00:48,  4.47it/s]\n",
            "44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 169/384 [00:39<00:52,  4.06it/s]\n",
            "44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 170/384 [00:39<00:50,  4.24it/s]\n",
            "45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 171/384 [00:40<00:52,  4.04it/s]\n",
            "45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 172/384 [00:40<00:52,  4.02it/s]\n",
            "45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 173/384 [00:40<00:51,  4.12it/s]\n",
            "45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 174/384 [00:40<00:45,  4.57it/s]\n",
            "46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 175/384 [00:41<00:54,  3.85it/s]\n",
            "46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 176/384 [00:41<01:03,  3.26it/s]\n",
            "46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 177/384 [00:41<01:01,  3.37it/s]\n",
            "46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 178/384 [00:41<00:56,  3.64it/s]\n",
            "47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 179/384 [00:42<00:49,  4.15it/s]\n",
            "47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 180/384 [00:42<00:51,  3.95it/s]\n",
            "47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 181/384 [00:42<00:50,  4.01it/s]\n",
            "47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 182/384 [00:43<01:05,  3.06it/s]\n",
            "48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 183/384 [00:43<01:08,  2.93it/s]\n",
            "48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 184/384 [00:43<01:01,  3.24it/s]\n",
            "48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 185/384 [00:44<00:55,  3.59it/s]\n",
            "48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 186/384 [00:44<01:04,  3.09it/s]\n",
            "49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 187/384 [00:44<00:54,  3.60it/s]\n",
            "49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 188/384 [00:44<00:49,  3.97it/s]\n",
            "49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 189/384 [00:45<00:50,  3.90it/s]\n",
            "49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 190/384 [00:45<00:49,  3.95it/s]\n",
            "50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 191/384 [00:45<00:49,  3.92it/s]\n",
            "50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 192/384 [00:45<00:51,  3.75it/s]\n",
            "50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 193/384 [00:45<00:43,  4.39it/s]\n",
            "51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 194/384 [00:46<00:45,  4.22it/s]\n",
            "51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 195/384 [00:46<00:47,  3.95it/s]\n",
            "51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 196/384 [00:46<00:39,  4.70it/s]\n",
            "51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 197/384 [00:46<00:41,  4.47it/s]\n",
            "52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 198/384 [00:47<00:38,  4.83it/s]\n",
            "52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 199/384 [00:47<00:38,  4.84it/s]\n",
            "52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 200/384 [00:47<00:33,  5.55it/s]\n",
            "52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 201/384 [00:47<00:40,  4.50it/s]\n",
            "53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 202/384 [00:47<00:36,  4.95it/s]\n",
            "53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 203/384 [00:47<00:31,  5.67it/s]\n",
            "53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 204/384 [00:48<00:32,  5.48it/s]\n",
            "53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 205/384 [00:48<00:31,  5.61it/s]\n",
            "54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 206/384 [00:48<00:30,  5.89it/s]\n",
            "54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 207/384 [00:48<00:35,  5.01it/s]\n",
            "54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 208/384 [00:48<00:32,  5.43it/s]\n",
            "54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 209/384 [00:49<00:30,  5.66it/s]\n",
            "55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 210/384 [00:49<00:30,  5.80it/s]\n",
            "55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 211/384 [00:49<00:27,  6.19it/s]\n",
            "55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 212/384 [00:49<00:30,  5.67it/s]\n",
            "55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 213/384 [00:49<00:31,  5.51it/s]\n",
            "56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 214/384 [00:50<00:33,  5.09it/s]\n",
            "56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 215/384 [00:50<00:31,  5.39it/s]\n",
            "56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 216/384 [00:50<00:38,  4.40it/s]\n",
            "57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 217/384 [00:50<00:47,  3.54it/s]\n",
            "57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 218/384 [00:51<00:58,  2.82it/s]\n",
            "57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 219/384 [00:51<00:55,  2.97it/s]\n",
            "57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 220/384 [00:51<00:48,  3.35it/s]\n",
            "58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 221/384 [00:52<00:42,  3.81it/s]\n",
            "58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 222/384 [00:52<00:40,  3.98it/s]\n",
            "58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 223/384 [00:52<00:39,  4.03it/s]\n",
            "58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 224/384 [00:52<00:38,  4.13it/s]\n",
            "59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 225/384 [00:52<00:34,  4.61it/s]\n",
            "59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 226/384 [00:53<00:32,  4.89it/s]\n",
            "59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 227/384 [00:53<00:31,  4.96it/s]\n",
            "59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 228/384 [00:53<00:32,  4.73it/s]\n",
            "60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 229/384 [00:54<00:48,  3.18it/s]\n",
            "60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 230/384 [00:54<00:45,  3.36it/s]\n",
            "60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 231/384 [00:54<00:43,  3.51it/s]\n",
            "60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 232/384 [00:54<00:41,  3.65it/s]\n",
            "61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 233/384 [00:55<00:41,  3.62it/s]\n",
            "61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 234/384 [00:55<00:38,  3.91it/s]\n",
            "61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 235/384 [00:55<00:38,  3.88it/s]\n",
            "61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 236/384 [00:55<00:38,  3.80it/s]\n",
            "62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 237/384 [00:56<00:35,  4.10it/s]\n",
            "62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 238/384 [00:56<00:34,  4.25it/s]\n",
            "62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 239/384 [00:56<00:34,  4.24it/s]\n",
            "62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 240/384 [00:56<00:32,  4.46it/s]\n",
            "63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 241/384 [00:57<00:34,  4.10it/s]\n",
            "63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 242/384 [00:57<00:33,  4.25it/s]\n",
            "63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 243/384 [00:57<00:34,  4.14it/s]\n",
            "64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 244/384 [00:57<00:33,  4.24it/s]\n",
            "64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 245/384 [00:57<00:32,  4.34it/s]\n",
            "64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 246/384 [00:58<00:33,  4.14it/s]\n",
            "64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 247/384 [00:58<00:32,  4.28it/s]\n",
            "65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 248/384 [00:58<00:28,  4.75it/s]\n",
            "65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 249/384 [00:58<00:26,  5.11it/s]\n",
            "65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 250/384 [00:58<00:25,  5.33it/s]\n",
            "65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 251/384 [00:59<00:25,  5.24it/s]\n",
            "66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 252/384 [00:59<00:25,  5.09it/s]\n",
            "66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 253/384 [00:59<00:25,  5.06it/s]\n",
            "66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 254/384 [00:59<00:25,  5.10it/s]\n",
            "66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 255/384 [01:00<00:27,  4.68it/s]\n",
            "67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 256/384 [01:00<00:26,  4.91it/s]\n",
            "67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 257/384 [01:00<00:23,  5.33it/s]\n",
            "67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 258/384 [01:00<00:22,  5.53it/s]\n",
            "67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 259/384 [01:00<00:20,  6.02it/s]\n",
            "68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 260/384 [01:00<00:24,  5.13it/s]\n",
            "68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 261/384 [01:01<00:23,  5.25it/s]\n",
            "68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 262/384 [01:01<00:22,  5.50it/s]\n",
            "68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 263/384 [01:01<00:28,  4.30it/s]\n",
            "69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 264/384 [01:01<00:27,  4.32it/s]\n",
            "69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 265/384 [01:02<00:26,  4.43it/s]\n",
            "69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 266/384 [01:02<00:29,  4.01it/s]\n",
            "70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 267/384 [01:02<00:27,  4.25it/s]\n",
            "70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 268/384 [01:02<00:28,  4.10it/s]\n",
            "70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 269/384 [01:03<00:27,  4.20it/s]\n",
            "70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 270/384 [01:03<00:26,  4.26it/s]\n",
            "71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 271/384 [01:03<00:26,  4.20it/s]\n",
            "71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 272/384 [01:03<00:23,  4.77it/s]\n",
            "71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 273/384 [01:03<00:23,  4.75it/s]\n",
            "71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 274/384 [01:04<00:25,  4.27it/s]\n",
            "72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 275/384 [01:04<00:25,  4.35it/s]\n",
            "72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 276/384 [01:04<00:22,  4.78it/s]\n",
            "72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 277/384 [01:04<00:22,  4.79it/s]\n",
            "72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 278/384 [01:04<00:21,  4.95it/s]\n",
            "73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 279/384 [01:05<00:22,  4.73it/s]\n",
            "73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 280/384 [01:05<00:21,  4.85it/s]\n",
            "73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 281/384 [01:05<00:20,  4.98it/s]\n",
            "73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 282/384 [01:05<00:20,  5.08it/s]\n",
            "74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 283/384 [01:05<00:19,  5.11it/s]\n",
            "74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 284/384 [01:06<00:21,  4.75it/s]\n",
            "74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 285/384 [01:06<00:19,  5.01it/s]\n",
            "74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 286/384 [01:06<00:20,  4.89it/s]\n",
            "75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 287/384 [01:06<00:20,  4.71it/s]\n",
            "75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 288/384 [01:06<00:19,  5.04it/s]\n",
            "75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 289/384 [01:07<00:18,  5.00it/s]\n",
            "76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 290/384 [01:07<00:22,  4.17it/s]\n",
            "76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 291/384 [01:07<00:23,  4.02it/s]\n",
            "76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 292/384 [01:08<00:24,  3.79it/s]\n",
            "76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 293/384 [01:08<00:32,  2.76it/s]\n",
            "77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 294/384 [01:08<00:31,  2.87it/s]\n",
            "77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 295/384 [01:09<00:30,  2.93it/s]\n",
            "77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 296/384 [01:09<00:26,  3.35it/s]\n",
            "77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 297/384 [01:09<00:27,  3.19it/s]\n",
            "78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 298/384 [01:10<00:29,  2.91it/s]\n",
            "78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 299/384 [01:10<00:25,  3.31it/s]\n",
            "78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 300/384 [01:10<00:23,  3.51it/s]\n",
            "78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 301/384 [01:10<00:22,  3.75it/s]\n",
            "79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 302/384 [01:11<00:19,  4.11it/s]\n",
            "79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 303/384 [01:11<00:19,  4.12it/s]\n",
            "79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 304/384 [01:11<00:19,  4.12it/s]\n",
            "79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 305/384 [01:11<00:18,  4.36it/s]\n",
            "80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 306/384 [01:12<00:18,  4.11it/s]\n",
            "80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 307/384 [01:12<00:18,  4.09it/s]\n",
            "80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 308/384 [01:12<00:18,  4.10it/s]\n",
            "80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 309/384 [01:12<00:16,  4.42it/s]\n",
            "81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 310/384 [01:13<00:20,  3.64it/s]\n",
            "81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 311/384 [01:13<00:17,  4.20it/s]\n",
            "81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 312/384 [01:13<00:19,  3.79it/s]\n",
            "82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 313/384 [01:13<00:17,  4.02it/s]\n",
            "82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 314/384 [01:14<00:17,  4.00it/s]\n",
            "82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 315/384 [01:14<00:16,  4.22it/s]\n",
            "82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 316/384 [01:14<00:14,  4.80it/s]\n",
            "83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 317/384 [01:14<00:12,  5.22it/s]\n",
            "83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 318/384 [01:14<00:12,  5.33it/s]\n",
            "83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 319/384 [01:14<00:11,  5.43it/s]\n",
            "83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 320/384 [01:15<00:13,  4.64it/s]\n",
            "84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 321/384 [01:15<00:12,  4.87it/s]\n",
            "84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 322/384 [01:15<00:11,  5.34it/s]\n",
            "84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 323/384 [01:15<00:11,  5.13it/s]\n",
            "84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 324/384 [01:16<00:14,  4.18it/s]\n",
            "85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 326/384 [01:16<00:10,  5.56it/s]\n",
            "85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 327/384 [01:16<00:12,  4.70it/s]\n",
            "85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 328/384 [01:16<00:13,  4.19it/s]\n",
            "86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 329/384 [01:17<00:12,  4.58it/s]\n",
            "86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 330/384 [01:17<00:11,  4.62it/s]\n",
            "86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 331/384 [01:17<00:11,  4.77it/s]\n",
            "86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 332/384 [01:17<00:10,  5.13it/s]\n",
            "87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 333/384 [01:17<00:09,  5.22it/s]\n",
            "87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 334/384 [01:18<00:10,  4.99it/s]\n",
            "87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 335/384 [01:18<00:09,  5.22it/s]\n",
            "88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 336/384 [01:18<00:09,  5.12it/s]\n",
            "88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 337/384 [01:18<00:09,  5.01it/s]\n",
            "88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 338/384 [01:19<00:12,  3.81it/s]\n",
            "88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 339/384 [01:19<00:10,  4.20it/s]\n",
            "89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 340/384 [01:19<00:09,  4.84it/s]\n",
            "89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 341/384 [01:19<00:08,  4.91it/s]\n",
            "89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 342/384 [01:19<00:08,  4.90it/s]\n",
            "89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 343/384 [01:20<00:08,  4.73it/s]\n",
            "90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 344/384 [01:20<00:07,  5.29it/s]\n",
            "90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 345/384 [01:20<00:06,  5.90it/s]\n",
            "90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 346/384 [01:20<00:07,  4.91it/s]\n",
            "90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 347/384 [01:20<00:07,  4.69it/s]\n",
            "91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 348/384 [01:21<00:09,  3.87it/s]\n",
            "91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 349/384 [01:21<00:09,  3.52it/s]\n",
            "91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 350/384 [01:21<00:09,  3.53it/s]\n",
            "91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 351/384 [01:22<00:09,  3.54it/s]\n",
            "92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 352/384 [01:22<00:08,  3.69it/s]\n",
            "92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 353/384 [01:22<00:08,  3.66it/s]\n",
            "92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 354/384 [01:22<00:08,  3.58it/s]\n",
            "92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 355/384 [01:23<00:07,  3.76it/s]\n",
            "93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 356/384 [01:23<00:07,  3.82it/s]\n",
            "93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 357/384 [01:23<00:07,  3.74it/s]\n",
            "93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 358/384 [01:23<00:06,  3.88it/s]\n",
            "93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 359/384 [01:24<00:05,  4.19it/s]\n",
            "94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 360/384 [01:24<00:05,  4.67it/s]\n",
            "94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 361/384 [01:24<00:05,  4.59it/s]\n",
            "94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 362/384 [01:24<00:04,  5.12it/s]\n",
            "95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 363/384 [01:24<00:04,  5.24it/s]\n",
            "95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 364/384 [01:24<00:03,  5.45it/s]\n",
            "95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 365/384 [01:25<00:03,  6.01it/s]\n",
            "95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 366/384 [01:25<00:02,  6.05it/s]\n",
            "96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 367/384 [01:25<00:02,  6.02it/s]\n",
            "96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 368/384 [01:25<00:03,  4.84it/s]\n",
            "96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 369/384 [01:25<00:02,  5.06it/s]\n",
            "96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 370/384 [01:26<00:02,  5.37it/s]\n",
            "97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 371/384 [01:26<00:02,  4.93it/s]\n",
            "97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 372/384 [01:26<00:02,  5.07it/s]\n",
            "97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 373/384 [01:26<00:02,  4.91it/s]\n",
            "97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 374/384 [01:26<00:01,  5.07it/s]\n",
            "98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 375/384 [01:27<00:01,  5.32it/s]\n",
            "98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 376/384 [01:27<00:01,  4.71it/s]\n",
            "98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 377/384 [01:27<00:01,  4.67it/s]\n",
            "98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 378/384 [01:27<00:01,  5.32it/s]\n",
            "99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 379/384 [01:27<00:00,  5.58it/s]\n",
            "99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 380/384 [01:27<00:00,  5.82it/s]\n",
            "99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 381/384 [01:28<00:00,  5.46it/s]\n",
            "99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 382/384 [01:28<00:00,  4.91it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 383/384 [01:28<00:00,  5.58it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 384/384 [01:28<00:00,  4.57it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 384/384 [01:28<00:00,  4.32it/s]\n",
            "[DEBUG] ‚úÖ DONE! ./ckpt/results.json\n"
          ]
        }
      ],
      "source": [
        "# @title 4. Inferenza ActionFormer (NON FUNZIONA, PESI UTILIZZATI NON COMPATIBILI CON FEATURES OMNIVORE)\n",
        "import os\n",
        "import glob\n",
        "import subprocess\n",
        "import yaml\n",
        "import sys\n",
        "import shutil\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Output immediato\n",
        "os.environ['PYTHONUNBUFFERED'] = '1'\n",
        "\n",
        "print(\"üöÄ Inizializzazione ActionFormer (GPU RECOVERY MODE)...\", flush=True)\n",
        "\n",
        "# --- 1. RIPRISTINO REPO (Se cancellato dal reset) ---\n",
        "AF_WORKDIR = \"/content/actionformer_workspace\"\n",
        "os.makedirs(AF_WORKDIR, exist_ok=True)\n",
        "AF_REPO_PATH = os.path.join(AF_WORKDIR, \"multi_step_localization\")\n",
        "\n",
        "# Se manca il codice, lo scarichiamo al volo\n",
        "if not os.path.exists(os.path.join(AF_REPO_PATH, \"actionformer\")):\n",
        "    print(\"‚ö†Ô∏è Repository ActionFormer mancante (causa reset). Ripristino in corso...\", flush=True)\n",
        "    if os.path.exists(AF_REPO_PATH): shutil.rmtree(AF_REPO_PATH)\n",
        "    # Cloniamo un fork stabile o l'originale\n",
        "    subprocess.run([\"git\", \"clone\", \"https://github.com/happyharrycn/actionformer.git\", AF_REPO_PATH], check=True)\n",
        "    # Installiamo dipendenze base minime se necessario (spesso su colab bastano quelle base)\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"pyyaml\"], check=True)\n",
        "    print(\"‚úÖ Repository ripristinato.\", flush=True)\n",
        "\n",
        "if AF_REPO_PATH not in sys.path: sys.path.append(AF_REPO_PATH)\n",
        "\n",
        "if 'PROJECT_DIR' not in locals():\n",
        "    if os.path.exists(\"/content/drive/MyDrive/MistakeDetection\"):\n",
        "        PROJECT_DIR = \"/content/drive/MyDrive/MistakeDetection\"\n",
        "    else:\n",
        "        PROJECT_DIR = \"/content/drive/MyDrive/CaptainCook4D\"\n",
        "\n",
        "# --- 2. SETUP HARDWARE & FEATURE ---\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"üñ•Ô∏è Hardware: {DEVICE.upper()}\", flush=True)\n",
        "\n",
        "print(\"üîç [System] Ricerca file feature...\", flush=True)\n",
        "search_paths = [\n",
        "    \"/content/temp_omnivore_features/omnivore\",\n",
        "    \"/content/temp_omnivore_features\",\n",
        "    os.path.join(PROJECT_DIR, \"features\", \"omnivore_video\"),\n",
        "    os.path.join(PROJECT_DIR, \"features\"),\n",
        "    \"/content/omnivore_features\"\n",
        "]\n",
        "\n",
        "FOUND_FEAT_DIR = None\n",
        "for path in search_paths:\n",
        "    if os.path.exists(path):\n",
        "        files = glob.glob(os.path.join(path, \"*.npz\"))\n",
        "        if len(files) > 0:\n",
        "            FOUND_FEAT_DIR = path\n",
        "            print(f\"‚úÖ Feature trovate in: {path} ({len(files)} file)\", flush=True)\n",
        "            break\n",
        "\n",
        "if not FOUND_FEAT_DIR:\n",
        "    # Se dopo il reset hai perso anche le feature in /content/, dobbiamo rigenerarle o trovarle in Drive\n",
        "    print(\"‚ùå ERRORE: Feature perse col reset! Controlla se sono su Drive.\", flush=True)\n",
        "    # Tentativo disperato su Drive\n",
        "    drive_cands = glob.glob(\"/content/drive/MyDrive/**/*.npz\", recursive=True)\n",
        "    if drive_cands:\n",
        "        FOUND_FEAT_DIR = os.path.dirname(drive_cands[0])\n",
        "        print(f\"‚úÖ Trovate feature di backup su Drive: {FOUND_FEAT_DIR}\", flush=True)\n",
        "    else:\n",
        "        # Creiamo dummy per non far crashare lo script, ma l'utente deve rifare le feature\n",
        "        print(\"‚ö†Ô∏è Nessuna feature trovata. Creo cartella dummy (Riesegui Step 3 se necessario).\")\n",
        "        FOUND_FEAT_DIR = \"/content/dummy_feat\"\n",
        "        os.makedirs(FOUND_FEAT_DIR, exist_ok=True)\n",
        "\n",
        "LOCAL_FEAT_DIR = FOUND_FEAT_DIR\n",
        "MY_CONFIG_PATH = os.path.join(AF_REPO_PATH, \"configs\", \"forced_config.yaml\")\n",
        "DATASETS_LIB_PATH = os.path.join(AF_REPO_PATH, \"actionformer\", \"libs\", \"datasets\")\n",
        "\n",
        "\n",
        "# --- 3. GENERAZIONE JSON DATABASE ---\n",
        "json_base = os.path.join(AF_WORKDIR, \"actionformer_split.json\")\n",
        "feature_files = glob.glob(os.path.join(LOCAL_FEAT_DIR, \"*.npz\"))\n",
        "full_db = {}\n",
        "for f in feature_files:\n",
        "    vid_name = os.path.basename(f).replace(\".npz\", \"\")\n",
        "    full_db[vid_name] = {\"subset\": \"validation\", \"annotations\": [{\"label\": \"test\", \"segment\": [0, 1]}]}\n",
        "# Dummy entry se vuoto\n",
        "if not full_db: full_db[\"dummy\"] = {\"subset\": \"validation\", \"annotations\": [{\"label\": \"test\", \"segment\": [0, 1]}]}\n",
        "\n",
        "with open(json_base, 'w') as f:\n",
        "    json.dump({\"database\": full_db, \"taxonomy\": [{\"id\":0,\"label\":\"test\",\"label_id\":0,\"nodeName\":\"test\"}], \"version\": \"1.0\"}, f)\n",
        "shutil.copy2(json_base, json_base.replace(\".json\",\"\")+\"_recordings.json\")\n",
        "\n",
        "\n",
        "# --- 4. CONFIGURAZIONE YAML (FIX MISSING DIR) ---\n",
        "# FIX CRUCIALE: Creiamo la cartella configs se non esiste\n",
        "os.makedirs(os.path.dirname(MY_CONFIG_PATH), exist_ok=True)\n",
        "\n",
        "cands = glob.glob(os.path.join(PROJECT_DIR, \"**\", \"*omnivore*.pth*\"), recursive=True)\n",
        "if not cands: raise FileNotFoundError(\"‚ùå Modello .pth.tar non trovato su Drive!\")\n",
        "MODEL_CKPT = sorted(cands)[-1]\n",
        "\n",
        "# 1. Rigenera il Config con soglie MINIME\n",
        "config_data = {\n",
        "    'dataset_name': 'thumos', 'model_name': 'LocPointTransformer', 'output_folder': './ckpt/', 'devices': [DEVICE],\n",
        "    'dataset': {'json_file': os.path.join(AF_WORKDIR, \"actionformer_split.json\"), 'feat_folder': LOCAL_FEAT_DIR,\n",
        "                'file_prefix': '', 'file_ext': '.npz', 'input_dim': 1024, 'feat_stride': 16, 'num_classes': 24,\n",
        "                'default_fps': 30, 'num_frames': 32, 'downsample_rate': 1, 'max_seq_len': 2304, 'trunc_thresh': 0.5,\n",
        "                'crop_ratio': None, 'force_upsampling': False},\n",
        "    'eval': {'batch_size': 1, 'nms_score_thres': 0.0001}, # SOGLIA QUASI ZERO\n",
        "    'loader': {'batch_size': 1, 'num_workers': 2},\n",
        "    'model': {'backbone_type': 'convTransformer', 'fpn_type': 'identity', 'backbone_arch': [2, 2, 5], 'scale_factor': 2,\n",
        "              'input_dim': 1024, 'max_seq_len': 2304, 'n_head': 4, 'embd_kernel_size': 3, 'embd_with_ln': True,\n",
        "              'fpn_with_ln': True, 'fpn_start_level': 0, 'head_num_layers': 3, 'head_kernel_size': 3, 'head_with_ln': True,\n",
        "              'use_rel_pe': False, 'num_classes': 24, 'regression_range': [[0, 4], [4, 8], [8, 16], [16, 32], [32, 64], [64, 10000]],\n",
        "              'embd_dim': 512, 'fpn_dim': 512, 'head_dim': 512, 'use_abs_pe': False, 'max_buffer_len_factor': 6.0, 'n_mha_win_size': 19,\n",
        "              'train_cfg': {'center_sample': 'radius', 'center_sample_radius': 1.5, 'loss_weight': 1.0, 'cls_prior_prob': 0.01,\n",
        "                            'init_loss_norm': 2000, 'clip_grad_l2norm': -1, 'label_smoothing': 0.0, 'dropout': 0.1, 'droppath': 0.1, 'head_empty_cls': []},\n",
        "              'test_cfg': {\n",
        "                  'pre_nms_thresh': 0.0001, # SOGLIA PRE-NMS MINIMA\n",
        "                  'pre_nms_topk': 5000, 'iou_threshold': 0.1, 'min_score': 0.0001,\n",
        "                  'max_seg_num': 1000, 'nms_method': 'soft', 'nms_sigma': 0.5, 'voting_thresh': 0.75,\n",
        "                  'multiclass_nms': True, 'duration_thresh': 0.001}\n",
        "    },\n",
        "    'train': {'head_dim': 512}\n",
        "}\n",
        "with open(MY_CONFIG_PATH, 'w') as f: yaml.dump(config_data, f)\n",
        "\n",
        "\n",
        "# --- 5. PATCH LIBRERIE (Core Fixes) ---\n",
        "print(\"üõ†Ô∏è [System] Applicazione patch al codice...\", flush=True)\n",
        "pycache_dir = os.path.join(DATASETS_LIB_PATH, \"__pycache__\")\n",
        "if os.path.exists(pycache_dir): shutil.rmtree(pycache_dir)\n",
        "\n",
        "# A. __INIT__.PY\n",
        "with open(os.path.join(DATASETS_LIB_PATH, \"__init__.py\"), 'w') as f:\n",
        "    f.write(\"from .datasets import make_dataset, make_data_loader\\n\")\n",
        "\n",
        "# B. DATASETS.PY\n",
        "datasets_code = r\"\"\"\n",
        "import torch\n",
        "import os\n",
        "_DATASET_REGISTRY = {}\n",
        "def register_dataset(name):\n",
        "    def decorator(cls):\n",
        "        _DATASET_REGISTRY[name] = cls\n",
        "        return cls\n",
        "    return decorator\n",
        "def make_dataset(name, is_training, split, **kwargs):\n",
        "    if name == 'thumos':\n",
        "        from .thumos14 import THUMOS14Dataset\n",
        "        return THUMOS14Dataset(is_training, split, **kwargs)\n",
        "    if name in _DATASET_REGISTRY:\n",
        "        return _DATASET_REGISTRY[name](is_training, split, **kwargs)\n",
        "    raise KeyError(f\"Dataset sconosciuto: {name}\")\n",
        "def make_data_loader(dataset, is_training, generator, batch_size, num_workers):\n",
        "    persistent = True if num_workers > 0 else False\n",
        "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, shuffle=(True if is_training else False), collate_fn=None, pin_memory=False, drop_last=(True if is_training else False), persistent_workers=persistent)\n",
        "    return loader\n",
        "\"\"\"\n",
        "with open(os.path.join(DATASETS_LIB_PATH, \"datasets.py\"), 'w') as f: f.write(datasets_code)\n",
        "\n",
        "# C. THUMOS14.PY (Robust NPZ + Meta)\n",
        "thumos_code = r\"\"\"\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from .datasets import register_dataset\n",
        "@register_dataset(\"thumos\")\n",
        "class THUMOS14Dataset(Dataset):\n",
        "    def __init__(self, is_training, split, feat_folder, json_file, feat_stride, num_frames, default_fps, downsample_rate, max_seq_len, trunc_thresh, crop_ratio, input_dim, num_classes, file_prefix, file_ext, force_upsampling):\n",
        "        self.split = split; self.feat_folder = feat_folder; self.json_file = json_file; self.feat_stride = feat_stride; self.num_frames = num_frames; self.default_fps = default_fps; self.downsample_rate = downsample_rate; self.input_dim = input_dim; self.num_classes = num_classes; self.file_prefix = file_prefix; self.file_ext = file_ext\n",
        "        dict_db, label_dict = self._load_json_db(self.json_file)\n",
        "        self.data_list = [val for key, val in dict_db.items()]\n",
        "        self.label_dict = label_dict\n",
        "    def get_attributes(self): return self.data_list, self.label_dict, self.num_classes\n",
        "    def _load_json_db(self, json_file):\n",
        "        with open(json_file, 'r') as fid: json_data = json.load(fid)\n",
        "        label_dict = {'test': 0}\n",
        "        if 'taxonomy' in json_data:\n",
        "            for act in json_data['taxonomy']: label_dict[act.get('label', 'unknown')] = act.get('id', 0)\n",
        "        dict_db = json_data['database']\n",
        "        for vid in dict_db:\n",
        "            if 'id' not in dict_db[vid]: dict_db[vid]['id'] = vid\n",
        "        return dict_db, label_dict\n",
        "    def __len__(self): return len(self.data_list)\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data_list[idx]\n",
        "        feat_file = os.path.join(self.feat_folder, self.file_prefix + item['id'] + self.file_ext)\n",
        "        try:\n",
        "            if not os.path.exists(feat_file): feats = np.zeros((self.input_dim, 100), dtype=np.float32)\n",
        "            else:\n",
        "                loaded = np.load(feat_file)\n",
        "                if isinstance(loaded, np.lib.npyio.NpzFile):\n",
        "                    keys = loaded.files\n",
        "                    if 'feats' in keys: feats = loaded['feats']\n",
        "                    elif 'arr_0' in keys: feats = loaded['arr_0']\n",
        "                    else: feats = loaded[keys[0]]\n",
        "                else: feats = loaded\n",
        "            feats = feats.astype(np.float32)\n",
        "            if feats.ndim == 2 and feats.shape[1] == self.input_dim: feats = feats.transpose()\n",
        "        except: feats = np.zeros((self.input_dim, 100), dtype=np.float32)\n",
        "        if self.downsample_rate > 1: feats = feats[:, ::self.downsample_rate]\n",
        "        feat_stride = self.feat_stride * self.downsample_rate\n",
        "        num_feat_frames = feats.shape[1]\n",
        "        duration = (num_feat_frames * feat_stride) / self.default_fps\n",
        "        feats = torch.from_numpy(np.ascontiguousarray(feats))\n",
        "        return {'video_id': item['id'], 'feats': feats, 'segments': torch.zeros((0, 2), dtype=torch.float32), 'labels': torch.zeros((0), dtype=torch.int64), 'fps': self.default_fps, 'feat_stride': feat_stride, 'feat_num_frames': num_feat_frames, 'duration': duration}\n",
        "\"\"\"\n",
        "with open(os.path.join(DATASETS_LIB_PATH, \"thumos14.py\"), 'w') as f: f.write(thumos_code)\n",
        "\n",
        "\n",
        "# --- 6. SCRIPT INFERENZA ---\n",
        "eval_standalone = r\"\"\"\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import argparse\n",
        "import numpy as np\n",
        "import yaml\n",
        "import json\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "\n",
        "def log(msg): print(f\"[DEBUG] {msg}\", flush=True)\n",
        "def load_config(path):\n",
        "    with open(path, 'r') as f: return yaml.safe_load(f)\n",
        "\n",
        "from actionformer.libs.modeling import make_meta_arch\n",
        "from actionformer.libs.datasets import make_dataset, make_data_loader\n",
        "\n",
        "def main():\n",
        "    log(\"1. Setup...\")\n",
        "    config_path = r'\"\"\" + MY_CONFIG_PATH + r\"\"\"'\n",
        "    cfg = load_config(config_path)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    log(f\"   -> Device: {device}\")\n",
        "\n",
        "    toxic = ['backbone', 'division_type', 'videos_type']\n",
        "    for k in toxic:\n",
        "        if k in cfg['dataset']: del cfg['dataset'][k]\n",
        "\n",
        "    val_dataset = make_dataset(cfg['dataset_name'], False, ['validation'], **cfg['dataset'])\n",
        "    val_loader = make_data_loader(val_dataset, False, None, **cfg['loader'])\n",
        "\n",
        "    model = make_meta_arch(cfg['model_name'], **cfg['model'])\n",
        "    model = model.to(device)\n",
        "\n",
        "    ckpt_path = r'\"\"\" + MODEL_CKPT + r\"\"\"'\n",
        "    log(f\"2. Pesi: {os.path.basename(ckpt_path)}\")\n",
        "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "    state_dict = checkpoint.get('state_dict_ema', checkpoint.get('model', checkpoint.get('state_dict', checkpoint)))\n",
        "    model.load_state_dict(state_dict, strict=False)\n",
        "    model.eval()\n",
        "\n",
        "    results = {'video_ids': [], 'segment_intervals': [], 'scores': [], 'labels': []}\n",
        "\n",
        "    log(\"3. Inferenza in corso...\")\n",
        "    with torch.no_grad():\n",
        "        for i, batch in tqdm(enumerate(val_loader), total=len(val_loader), file=sys.stdout):\n",
        "            try:\n",
        "                model_inputs = []\n",
        "                for k in range(len(batch['video_id'])):\n",
        "                    input_item = {\n",
        "                        'feats': batch['feats'][k].to(device),\n",
        "                        'feat_num_frames': batch['feat_num_frames'][k].to(device),\n",
        "                        'video_id': batch['video_id'][k],\n",
        "                        'fps': batch['fps'][k].item(),\n",
        "                        'feat_stride': batch['feat_stride'][k].item(),\n",
        "                        'duration': batch['duration'][k].item()\n",
        "                    }\n",
        "                    model_inputs.append(input_item)\n",
        "\n",
        "                output = model(model_inputs)\n",
        "\n",
        "                for k in range(len(output)):\n",
        "                    results['video_ids'].append(batch['video_id'][k])\n",
        "                    results['segment_intervals'].append(output[k]['segments'].cpu().numpy())\n",
        "                    results['scores'].append(output[k]['scores'].cpu().numpy())\n",
        "                    results['labels'].append(output[k]['labels'].cpu().numpy())\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "    out_dir = cfg['output_folder']\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    json_out = {'results': {}}\n",
        "    for i, vid in enumerate(results['video_ids']):\n",
        "        segs = results['segment_intervals'][i]\n",
        "        scrs = results['scores'][i]\n",
        "        lbls = results['labels'][i]\n",
        "        res_list = []\n",
        "        for j in range(len(segs)):\n",
        "            res_list.append({\n",
        "                'label': int(lbls[j]), 'score': float(scrs[j]),\n",
        "                'segment': [float(segs[j][0]), float(segs[j][1])]\n",
        "            })\n",
        "        json_out['results'][vid] = res_list\n",
        "\n",
        "    final_path = os.path.join(out_dir, \"results.json\")\n",
        "    with open(final_path, 'w') as f: json.dump(json_out, f)\n",
        "    log(f\"‚úÖ DONE! {final_path}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\"\"\"\n",
        "with open(os.path.join(AF_REPO_PATH, \"eval_standalone.py\"), 'w') as f: f.write(eval_standalone)\n",
        "\n",
        "# --- RUN ---\n",
        "print(\"üèÅ START...\", flush=True)\n",
        "os.chdir(AF_REPO_PATH)\n",
        "cmd = [\"python\", \"eval_standalone.py\"]\n",
        "process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1, universal_newlines=True)\n",
        "while True:\n",
        "    output = process.stdout.readline()\n",
        "    if output == '' and process.poll() is not None: break\n",
        "    if output: print(output.strip(), flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68440217",
      "metadata": {
        "id": "68440217"
      },
      "source": [
        "# Pooling, Zipping, Upload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "af9f40c2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af9f40c2",
        "outputId": "a6543b3f-9dad-4453-ed41-0c50b13372bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üßê Analisi file: /content/actionformer_workspace/multi_step_localization/ckpt/results.json\n",
            "üöÄ Inizio elaborazione su 384 video...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 384/384 [00:04<00:00, 79.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Generati 384 file .npz in /content/actionformer_workspace/step_embeddings_temp\n",
            "üì¶ Creazione archivio ZIP...\n",
            "‚òÅÔ∏è Upload completato: /content/drive/MyDrive/MistakeDetection/step_embeddings.zip\n"
          ]
        }
      ],
      "source": [
        "# @title 5. Pooling & Estrazione Step Embeddings (Fix ZIP Vuoto)\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- CONFIGURAZIONE ---\n",
        "RESULTS_JSON = \"/content/actionformer_workspace/multi_step_localization/ckpt/results.json\"\n",
        "FEAT_DIR = \"/content/temp_omnivore_features/omnivore\"\n",
        "OUT_DIR = \"/content/actionformer_workspace/step_embeddings_temp\"\n",
        "DRIVE_OUT = \"/content/drive/MyDrive/MistakeDetection/\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "os.makedirs(DRIVE_OUT, exist_ok=True)\n",
        "\n",
        "# Soglia minima di confidenza per considerare un'azione valida\n",
        "SCORE_THRESHOLD = 0.05\n",
        "\n",
        "def get_step_embedding():\n",
        "    print(f\"üßê Analisi file: {RESULTS_JSON}\")\n",
        "\n",
        "    if not os.path.exists(RESULTS_JSON):\n",
        "        print(\"‚ùå ERRORE: Il file results.json non esiste!\")\n",
        "        return\n",
        "\n",
        "    with open(RESULTS_JSON, 'r') as f:\n",
        "        data = json.load(f)['results']\n",
        "\n",
        "    vids = list(data.keys())\n",
        "    print(f\"üöÄ Inizio elaborazione su {len(vids)} video...\")\n",
        "\n",
        "    count_generated = 0\n",
        "\n",
        "    for vid in tqdm(vids):\n",
        "        # 1. Carica le feature originali (Omnivore)\n",
        "        feat_path = os.path.join(FEAT_DIR, f\"{vid}.npz\")\n",
        "        if not os.path.exists(feat_path):\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            loaded = np.load(feat_path)\n",
        "            # Gestione robusta caricamento\n",
        "            full_features = loaded['feats'] if 'feats' in loaded else (loaded['arr_0'] if 'arr_0' in loaded else loaded[loaded.files[0]])\n",
        "            if full_features.shape[0] == 1024: # Se √® [C, T] trasponi in [T, C]\n",
        "                full_features = full_features.T\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        # 2. Filtra i segmenti predetti da ActionFormer\n",
        "        predictions = data[vid]\n",
        "        valid_segments = [p for p in predictions if p['score'] > SCORE_THRESHOLD]\n",
        "\n",
        "        if not valid_segments:\n",
        "            continue\n",
        "\n",
        "        # 3. Pooling (Media delle feature nel segmento temporale)\n",
        "        step_embeds = []\n",
        "        fps = 30 # Default usato nell'inferenza\n",
        "        stride = 16\n",
        "\n",
        "        for seg in valid_segments:\n",
        "            start_sec, end_sec = seg['segment']\n",
        "            # Converti secondi in indici di feature\n",
        "            start_idx = int((start_sec * fps) / stride)\n",
        "            end_idx = int((end_sec * fps) / stride)\n",
        "\n",
        "            # Clamp degli indici\n",
        "            start_idx = max(0, start_idx)\n",
        "            end_idx = min(full_features.shape[0], end_idx)\n",
        "\n",
        "            if end_idx > start_idx:\n",
        "                # Average Pooling sul segmento\n",
        "                segment_feat = full_features[start_idx:end_idx, :]\n",
        "                pooled_feat = np.mean(segment_feat, axis=0)\n",
        "                step_embeds.append({\n",
        "                    'label': seg['label'],\n",
        "                    'score': seg['score'],\n",
        "                    'embedding': pooled_feat\n",
        "                })\n",
        "\n",
        "        if step_embeds:\n",
        "            np.savez(os.path.join(OUT_DIR, f\"{vid}_steps.npz\"), data=step_embeds)\n",
        "            count_generated += 1\n",
        "\n",
        "    print(f\"\\n‚úÖ Generati {count_generated} file .npz in {OUT_DIR}\")\n",
        "\n",
        "    if count_generated > 0:\n",
        "        # 4. Creazione ZIP\n",
        "        zip_path = os.path.join(\"/content\", \"step_embeddings.zip\")\n",
        "        print(\"üì¶ Creazione archivio ZIP...\")\n",
        "        with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
        "            for root, dirs, files in os.walk(OUT_DIR):\n",
        "                for file in files:\n",
        "                    zipf.write(os.path.join(root, file), file)\n",
        "\n",
        "        # 5. Upload su Drive\n",
        "        final_drive_path = os.path.join(DRIVE_OUT, \"step_embeddings.zip\")\n",
        "        shutil.copy(zip_path, final_drive_path)\n",
        "        print(f\"‚òÅÔ∏è Upload completato: {final_drive_path}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è ATTENZIONE: Nessun embedding generato. Controlla la confidenza del modello.\")\n",
        "\n",
        "get_step_embedding()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}