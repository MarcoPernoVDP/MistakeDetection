{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dcc0cb92",
      "metadata": {
        "id": "dcc0cb92"
      },
      "source": [
        "# Environement Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1. Setup Progetto (MistakeDetection)\n",
        "import sys, os\n",
        "try:\n",
        "    from google.colab import drive, userdata\n",
        "    IS_COLAB = True\n",
        "except ImportError:\n",
        "    IS_COLAB = False\n",
        "\n",
        "REPO_NAME = 'MistakeDetection'\n",
        "\n",
        "# --- CONFIGURAZIONE PATH ---\n",
        "if IS_COLAB:\n",
        "    print(\"‚òÅÔ∏è Colab rilevato.\")\n",
        "    if not os.path.exists('/content/drive'):\n",
        "        drive.mount('/content/drive')\n",
        "\n",
        "    # DEFINIZIONE GLOBALE PROJECT_DIR (Importante per le celle successive!)\n",
        "    PROJECT_DIR = \"/content/drive/MyDrive/MistakeDetection\"\n",
        "\n",
        "    # Fallback se la cartella ha un nome diverso\n",
        "    if not os.path.exists(PROJECT_DIR):\n",
        "        if os.path.exists(\"/content/drive/MyDrive/CaptainCook4D\"):\n",
        "            PROJECT_DIR = \"/content/drive/MyDrive/CaptainCook4D\"\n",
        "\n",
        "    print(f\"üìÇ Cartella Progetto su Drive: {PROJECT_DIR}\")\n",
        "\n",
        "    GITHUB_USER = 'MarcoPernoVDP'\n",
        "    try:\n",
        "        TOKEN = userdata.get('GITHUB_TOKEN')\n",
        "        REPO_URL = f'https://{TOKEN}@github.com/{GITHUB_USER}/{REPO_NAME}.git'\n",
        "    except:\n",
        "        REPO_URL = f'https://github.com/{GITHUB_USER}/{REPO_NAME}.git'\n",
        "\n",
        "    ROOT_DIR = f'/content/{REPO_NAME}'\n",
        "\n",
        "    if not os.path.exists(ROOT_DIR):\n",
        "        print(f\"üì• Clonazione {REPO_NAME}...\")\n",
        "        !git clone {REPO_URL}\n",
        "    else:\n",
        "        print(f\"üîÑ Aggiornamento {REPO_NAME}...\")\n",
        "        %cd {ROOT_DIR}\n",
        "        !git pull\n",
        "        %cd /content\n",
        "else:\n",
        "    print(\"Ambiente locale rilevato.\")\n",
        "    ROOT_DIR = os.getcwd()\n",
        "    while not os.path.exists(os.path.join(ROOT_DIR, '.gitignore')) and ROOT_DIR != os.path.dirname(ROOT_DIR):\n",
        "        ROOT_DIR = os.path.dirname(ROOT_DIR)\n",
        "    PROJECT_DIR = ROOT_DIR # In locale coincidono spesso\n",
        "\n",
        "if ROOT_DIR not in sys.path:\n",
        "    sys.path.append(ROOT_DIR)\n",
        "\n",
        "print(f\"‚úÖ ROOT_DIR impostata a: {ROOT_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmnwlfLXsQDA",
        "outputId": "2c4a7096-0e10-411b-893c-6b6021d95e08"
      },
      "id": "zmnwlfLXsQDA",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚òÅÔ∏è Colab rilevato.\n",
            "Mounted at /content/drive\n",
            "üìÇ Cartella Progetto su Drive: /content/drive/MyDrive/MistakeDetection\n",
            "üì• Clonazione MistakeDetection...\n",
            "Cloning into 'MistakeDetection'...\n",
            "remote: Enumerating objects: 550, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 550 (delta 7), reused 8 (delta 4), pack-reused 533 (from 1)\u001b[K\n",
            "Receiving objects: 100% (550/550), 85.58 MiB | 29.92 MiB/s, done.\n",
            "Resolving deltas: 100% (279/279), done.\n",
            "‚úÖ ROOT_DIR impostata a: /content/MistakeDetection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "caef717d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caef717d",
        "outputId": "688b4b06-5679-4a30-e2f1-980dee4452bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì• Clonazione ActionFormer (con --recursive)...\n",
            "‚úÖ Cartella Utils: /content/actionformer_workspace/multi_step_localization/actionformer/libs/utils\n",
            "üì¶ Installazione dipendenze...\n",
            "ü©π Patch NumPy 2.0...\n",
            "‚öôÔ∏è Compilazione CUDA...\n",
            "\n",
            "‚úÖ Ambiente ActionFormer pronto.\n"
          ]
        }
      ],
      "source": [
        "# @title 2. Setup ActionFormer\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import subprocess\n",
        "\n",
        "# Usiamo un workspace separato\n",
        "AF_WORKDIR = \"/content/actionformer_workspace\"\n",
        "os.makedirs(AF_WORKDIR, exist_ok=True)\n",
        "os.chdir(AF_WORKDIR)\n",
        "\n",
        "REPO_NAME = \"multi_step_localization\"\n",
        "AF_REPO_PATH = os.path.join(AF_WORKDIR, REPO_NAME)\n",
        "\n",
        "# 1. Clone\n",
        "if not os.path.exists(AF_REPO_PATH):\n",
        "    print(\"üì• Clonazione ActionFormer (con --recursive)...\")\n",
        "    try:\n",
        "        subprocess.run([\"git\", \"clone\", \"--recursive\", \"https://github.com/CaptainCook4D/multi_step_localization.git\"], check=True)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Clone recursive fallito, provo standard...\")\n",
        "        subprocess.run([\"git\", \"clone\", \"https://github.com/CaptainCook4D/multi_step_localization.git\"], check=True)\n",
        "\n",
        "os.chdir(AF_REPO_PATH)\n",
        "\n",
        "# 2. Fix Path Libs\n",
        "if os.path.exists(os.path.join(AF_REPO_PATH, \"actionformer\", \"libs\", \"utils\")):\n",
        "    UTILS_PATH = os.path.join(AF_REPO_PATH, \"actionformer\", \"libs\", \"utils\")\n",
        "    PATCH_DIR = os.path.join(AF_REPO_PATH, \"actionformer\")\n",
        "elif os.path.exists(os.path.join(AF_REPO_PATH, \"libs\", \"utils\")):\n",
        "    UTILS_PATH = os.path.join(AF_REPO_PATH, \"libs\", \"utils\")\n",
        "    PATCH_DIR = AF_REPO_PATH\n",
        "else:\n",
        "    # Tentativo update submodule\n",
        "    print(\"‚ö†Ô∏è Cartella libs non trovata, provo update submodule...\")\n",
        "    subprocess.run([\"git\", \"submodule\", \"update\", \"--init\", \"--recursive\"], check=True)\n",
        "    if os.path.exists(os.path.join(AF_REPO_PATH, \"libs\", \"utils\")):\n",
        "        UTILS_PATH = os.path.join(AF_REPO_PATH, \"libs\", \"utils\")\n",
        "        PATCH_DIR = AF_REPO_PATH\n",
        "    else:\n",
        "        raise FileNotFoundError(\"CRITICO: Impossibile trovare libs/utils.\")\n",
        "\n",
        "print(f\"‚úÖ Cartella Utils: {UTILS_PATH}\")\n",
        "\n",
        "# 3. Installazione & Patch\n",
        "print(\"üì¶ Installazione dipendenze...\")\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"pyyaml\", \"scipy\"], check=True)\n",
        "\n",
        "print(\"ü©π Patch NumPy 2.0...\")\n",
        "with open(os.path.join(PATCH_DIR, \"numpy_patch.py\"), \"w\") as f:\n",
        "    f.write(\"import numpy as np\\n\")\n",
        "    f.write(\"try:\\n  if not hasattr(np, 'float'): np.float = np.float64\\nexcept: pass\\n\")\n",
        "    f.write(\"try:\\n  if not hasattr(np, 'int'): np.int = np.int_\\nexcept: pass\\n\")\n",
        "\n",
        "# Inietta patch in eval.py\n",
        "eval_path = os.path.join(AF_REPO_PATH, \"eval.py\")\n",
        "if os.path.exists(eval_path):\n",
        "    with open(eval_path, \"r\") as f: content = f.read()\n",
        "    if \"import numpy_patch\" not in content:\n",
        "        with open(eval_path, \"w\") as f:\n",
        "            f.write(\"import sys\\nsys.path.append('actionformer')\\nimport numpy_patch\\n\" + content)\n",
        "\n",
        "# 4. Compilazione\n",
        "print(\"‚öôÔ∏è Compilazione CUDA...\")\n",
        "os.chdir(UTILS_PATH)\n",
        "subprocess.run([sys.executable, \"setup.py\", \"install\"], check=True)\n",
        "\n",
        "os.chdir(AF_REPO_PATH)\n",
        "print(\"\\n‚úÖ Ambiente ActionFormer pronto.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "6b773efb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b773efb",
        "outputId": "ebefed11-08a0-46eb-c470-9cbac8daa942"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Trovato Zip: /content/drive/MyDrive/MistakeDetection/omnivore.zip\n",
            "‚è≥ Estrazione in: /content/temp_omnivore_features...\n",
            "‚úÖ Estrazione completata.\n"
          ]
        }
      ],
      "source": [
        "# @title 3. Estrazione Feature Omnivore\n",
        "import zipfile\n",
        "import shutil\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- CONFIGURAZIONE VARIABILI (Self-Contained) ---\n",
        "if 'PROJECT_DIR' not in locals():\n",
        "    # Tenta di indovinare il path\n",
        "    if os.path.exists(\"/content/drive/MyDrive/MistakeDetection\"):\n",
        "        PROJECT_DIR = \"/content/drive/MyDrive/MistakeDetection\"\n",
        "    elif os.path.exists(\"/content/drive/MyDrive/CaptainCook4D\"):\n",
        "        PROJECT_DIR = \"/content/drive/MyDrive/CaptainCook4D\"\n",
        "    else:\n",
        "        # Fallback locale se non trova nulla (o se sei in locale)\n",
        "        PROJECT_DIR = os.getcwd()\n",
        "\n",
        "if 'ROOT_DIR' not in locals():\n",
        "    # Tenta di trovare il repo clonato\n",
        "    possible_roots = [\n",
        "        os.path.join(PROJECT_DIR, \"MistakeDetection\"),\n",
        "        \"/content/MistakeDetection\",\n",
        "        PROJECT_DIR\n",
        "    ]\n",
        "    for r in possible_roots:\n",
        "        if os.path.exists(os.path.join(r, \".git\")):\n",
        "            ROOT_DIR = r\n",
        "            break\n",
        "    if 'ROOT_DIR' not in locals(): ROOT_DIR = PROJECT_DIR\n",
        "\n",
        "# --- RICERCA ZIP ---\n",
        "POSSIBLE_PATHS = [\n",
        "    os.path.join(PROJECT_DIR, \"_file\", \"omnivore.zip\"),\n",
        "    os.path.join(PROJECT_DIR, \"data\", \"omnivore.zip\"),\n",
        "    os.path.join(PROJECT_DIR, \"omnivore.zip\"),\n",
        "    # Path specifici colab\n",
        "    \"/content/drive/MyDrive/MistakeDetection/omnivore.zip\",\n",
        "    \"/content/drive/MyDrive/MistakeDetection/data/omnivore.zip\"\n",
        "]\n",
        "\n",
        "ZIP_PATH = None\n",
        "for p in POSSIBLE_PATHS:\n",
        "    if os.path.exists(p):\n",
        "        ZIP_PATH = p\n",
        "        break\n",
        "\n",
        "LOCAL_FEAT_DIR = \"/content/temp_omnivore_features\"\n",
        "\n",
        "if ZIP_PATH is None:\n",
        "    print(f\"‚ùå ERRORE: Non trovo 'omnivore.zip'.\")\n",
        "    print(f\"   Ho cercato in: {POSSIBLE_PATHS}\")\n",
        "else:\n",
        "    print(f\"üìÇ Trovato Zip: {ZIP_PATH}\")\n",
        "    print(f\"‚è≥ Estrazione in: {LOCAL_FEAT_DIR}...\")\n",
        "\n",
        "    if os.path.exists(LOCAL_FEAT_DIR):\n",
        "        try:\n",
        "            shutil.rmtree(LOCAL_FEAT_DIR)\n",
        "        except: pass # Ignora errori permessi\n",
        "    os.makedirs(LOCAL_FEAT_DIR, exist_ok=True)\n",
        "\n",
        "    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
        "        zip_ref.extractall(LOCAL_FEAT_DIR)\n",
        "\n",
        "    print(f\"‚úÖ Estrazione completata.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85190e5b",
      "metadata": {
        "id": "85190e5b"
      },
      "source": [
        "# Features Extraction"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3.5 Riparazione Generazione JSON (Debug & Fallback)\n",
        "import os\n",
        "import glob\n",
        "import subprocess\n",
        "import json\n",
        "import sys\n",
        "import shutil\n",
        "\n",
        "# --- CONFIGURAZIONE ---\n",
        "AF_WORKDIR = \"/content/actionformer_workspace\"\n",
        "if os.path.exists(os.path.join(AF_WORKDIR, \"multi_step_localization\")):\n",
        "    AF_REPO_PATH = os.path.join(AF_WORKDIR, \"multi_step_localization\")\n",
        "else:\n",
        "    AF_REPO_PATH = AF_WORKDIR\n",
        "\n",
        "if 'PROJECT_DIR' not in locals():\n",
        "    if os.path.exists(\"/content/drive/MyDrive/MistakeDetection\"):\n",
        "        PROJECT_DIR = \"/content/drive/MyDrive/MistakeDetection\"\n",
        "    else:\n",
        "        PROJECT_DIR = \"/content/drive/MyDrive/CaptainCook4D\"\n",
        "\n",
        "USER_ANNOTATION_DIR = os.path.join(PROJECT_DIR, \"annotation_json\")\n",
        "TARGET_JSON = os.path.join(AF_WORKDIR, \"actionformer_split.json\")\n",
        "TARGET_JSON_REC = os.path.join(AF_WORKDIR, \"actionformer_split_recordings.json\")\n",
        "\n",
        "print(f\"üìÇ Cartella Annotazioni Utente: {USER_ANNOTATION_DIR}\")\n",
        "\n",
        "# 1. VERIFICA FILE SORGENTI\n",
        "if not os.path.exists(USER_ANNOTATION_DIR):\n",
        "    print(\"‚ùå ERRORE: La cartella annotation_json non esiste!\")\n",
        "    print(f\"   Crea la cartella: {USER_ANNOTATION_DIR} e mettici dentro i file .json dei video.\")\n",
        "    raise FileNotFoundError(\"Cartella annotation_json mancante\")\n",
        "\n",
        "json_files = glob.glob(os.path.join(USER_ANNOTATION_DIR, \"*.json\"))\n",
        "print(f\"   Trovati {len(json_files)} file .json sorgenti.\")\n",
        "\n",
        "if len(json_files) == 0:\n",
        "    print(\"‚ö†Ô∏è ATTENZIONE: La cartella annotation_json √® VUOTA.\")\n",
        "    print(\"   Senza file .json input, non possiamo creare il dataset ActionFormer.\")\n",
        "\n",
        "# 2. TENTATIVO 1: USARE LO SCRIPT DEL REPO (Con Debug)\n",
        "converter_script = os.path.join(AF_REPO_PATH, \"convert_to_action_former_json.py\")\n",
        "if os.path.exists(converter_script):\n",
        "    print(f\"üöÄ Avvio script conversione ufficiale: {os.path.basename(converter_script)}\")\n",
        "\n",
        "    cmd = [\n",
        "        \"python\", converter_script,\n",
        "        \"--annotation_folder\", USER_ANNOTATION_DIR,\n",
        "        \"--output_file\", TARGET_JSON\n",
        "    ]\n",
        "\n",
        "    # Eseguiamo catturando l'output per vedere l'errore\n",
        "    result = subprocess.run(cmd, capture_output=True, text=True, cwd=AF_REPO_PATH)\n",
        "\n",
        "    if result.returncode == 0 and os.path.exists(TARGET_JSON):\n",
        "        print(\"‚úÖ Conversione ufficiale RIUSCITA!\")\n",
        "    else:\n",
        "        print(\"‚ùå Conversione ufficiale FALLITA.\")\n",
        "        print(\"--- ERRORE SCRIPT ---\")\n",
        "        print(result.stderr)\n",
        "        print(\"---------------------\")\n",
        "        print(\"‚ö†Ô∏è Procedo con Generazione MANUALE di Emergenza (Fallback)...\")\n",
        "\n",
        "# 3. TENTATIVO 2: GENERATORE MANUALE (Fallback)\n",
        "# Se lo script sopra fallisce, creiamo noi un JSON valido per ActionFormer\n",
        "if not os.path.exists(TARGET_JSON):\n",
        "    print(\"üõ†Ô∏è Avvio Generatore Manuale (Python)...\")\n",
        "\n",
        "    database = {}\n",
        "\n",
        "    # Se abbiamo file json reali, proviamo a leggerli\n",
        "    if json_files:\n",
        "        for jf in json_files:\n",
        "            vid_id = os.path.basename(jf).replace(\".json\", \"\")\n",
        "            try:\n",
        "                with open(jf, 'r') as f:\n",
        "                    data = json.load(f)\n",
        "\n",
        "                # Cerca di capire la struttura (CaptainCook ha varie versioni)\n",
        "                # Struttura attesa: Lista di step o dizionario\n",
        "                annotations = []\n",
        "\n",
        "                # Caso A: Lista diretta di step\n",
        "                if isinstance(data, list):\n",
        "                    for item in data:\n",
        "                        if 'start_time' in item and 'end_time' in item:\n",
        "                             annotations.append({\n",
        "                                 \"segment\": [float(item['start_time']), float(item['end_time'])],\n",
        "                                 \"label\": item.get('label', 'unknown_step')\n",
        "                             })\n",
        "\n",
        "                # Caso B: Dizionario (es. 'segments': [...])\n",
        "                elif isinstance(data, dict):\n",
        "                     # Logica da adattare se necessario\n",
        "                     pass\n",
        "\n",
        "                # Se non riusciamo a leggere, creiamo un placeholder per far girare il modello\n",
        "                if not annotations:\n",
        "                    # Placeholder: ActionFormer trover√† da solo i segmenti\n",
        "                    # Mettiamo un segmento finto che copre tutto il video (ipotesi)\n",
        "                    annotations.append({\"segment\": [0, 1000], \"label\": \"test\"})\n",
        "\n",
        "                database[vid_id] = {\n",
        "                    \"subset\": \"validation\", # Fondamentale per eval.py\n",
        "                    \"annotations\": annotations\n",
        "                }\n",
        "            except Exception as e:\n",
        "                print(f\"   Errore lettura {vid_id}: {e}\")\n",
        "\n",
        "    # Se non c'erano file o lettura fallita, usiamo le feature presenti\n",
        "    if not database:\n",
        "        print(\"‚ö†Ô∏è Lettura annotazioni fallita. Genero DB basato sui file Feature (.npz)...\")\n",
        "        # Leggiamo la cartella feature per sapere quali video abbiamo\n",
        "        local_feat_dir = \"/content/temp_omnivore_features\"\n",
        "        if os.path.exists(local_feat_dir):\n",
        "            feat_files = glob.glob(os.path.join(local_feat_dir, \"*.npz\"))\n",
        "            for ff in feat_files:\n",
        "                vid_id = os.path.basename(ff).replace(\".npz\", \"\")\n",
        "                # Creiamo una entry dummy valida\n",
        "                database[vid_id] = {\n",
        "                    \"subset\": \"validation\",\n",
        "                    \"annotations\": [{\"segment\": [0.0, 1.0], \"label\": \"dummy\"}]\n",
        "                }\n",
        "\n",
        "    # Salva il JSON finale\n",
        "    final_data = {\"database\": database}\n",
        "    with open(TARGET_JSON, 'w') as f:\n",
        "        json.dump(final_data, f)\n",
        "    print(f\"‚úÖ JSON generato manualmente: {len(database)} video inseriti.\")\n",
        "\n",
        "# 4. DUPLICAZIONE PER COMPATIBILIT√Ä (Il trucco _recordings)\n",
        "if os.path.exists(TARGET_JSON):\n",
        "    shutil.copy2(TARGET_JSON, TARGET_JSON_REC)\n",
        "    print(f\"‚úÖ Creato duplicato necessario: {os.path.basename(TARGET_JSON_REC)}\")\n",
        "    print(\"üéâ ORA PUOI ESEGUIRE LA CELLA 4!\")\n",
        "else:\n",
        "    print(\"‚ùå DISASTRO: Impossibile creare il file JSON in nessun modo.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "BKzNuQzT-5uO"
      },
      "id": "BKzNuQzT-5uO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3.6 Fix JSON Taxonomy (Risolve KeyError: 'label_id')\n",
        "import json\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# CONFIGURAZIONE\n",
        "AF_WORKDIR = \"/content/actionformer_workspace\"\n",
        "JSON_PATH = os.path.join(AF_WORKDIR, \"actionformer_split.json\")\n",
        "JSON_REC_PATH = os.path.join(AF_WORKDIR, \"actionformer_split_recordings.json\")\n",
        "\n",
        "print(f\"üîß Analisi file: {JSON_PATH}\")\n",
        "\n",
        "if not os.path.exists(JSON_PATH):\n",
        "    # Se manca il base, proviamo a prendere il recordings se esiste\n",
        "    if os.path.exists(JSON_REC_PATH):\n",
        "        shutil.copy2(JSON_REC_PATH, JSON_PATH)\n",
        "        print(\"   Recuperato da _recordings.json\")\n",
        "    else:\n",
        "        raise FileNotFoundError(\"Nessun file JSON trovato da riparare!\")\n",
        "\n",
        "# 1. CARICAMENTO\n",
        "with open(JSON_PATH, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "db = data.get('database', {})\n",
        "if not db:\n",
        "    # Se il json √® piatto, prova a ristrutturarlo\n",
        "    print(\"‚ö†Ô∏è Struttura 'database' mancante. Tento ristrutturazione...\")\n",
        "    db = data\n",
        "    data = {'database': db}\n",
        "\n",
        "# 2. RACCOLTA ETICHETTE\n",
        "unique_labels = set()\n",
        "for vid_id, vid_data in db.items():\n",
        "    annotations = vid_data.get('annotations', [])\n",
        "    for ann in annotations:\n",
        "        if 'label' in ann:\n",
        "            unique_labels.add(ann['label'])\n",
        "\n",
        "sorted_labels = sorted(list(unique_labels))\n",
        "print(f\"‚úÖ Trovate {len(sorted_labels)} classi uniche.\")\n",
        "print(f\"   Esempio: {sorted_labels[:5]}...\")\n",
        "\n",
        "# 3. CREAZIONE TASSONOMIA (Il pezzo mancante!)\n",
        "# Thumos si aspetta una lista di dizionari con 'nodeName'/'label' e 'nodeId'/'label_id'\n",
        "# Analizzando l'errore: label_dict[act['label']] = act['label_id']\n",
        "# Quindi serve una lista che abbia 'label' e 'label_id'.\n",
        "\n",
        "taxonomy = []\n",
        "for idx, label in enumerate(sorted_labels):\n",
        "    entry = {\n",
        "        \"id\": idx,\n",
        "        \"label\": label,      # Fondamentale per il fix\n",
        "        \"label_id\": idx,     # Fondamentale per il fix\n",
        "        \"nodeId\": idx,       # Extra sicurezza\n",
        "        \"nodeName\": label,   # Extra sicurezza\n",
        "        \"parentId\": None\n",
        "    }\n",
        "    taxonomy.append(entry)\n",
        "\n",
        "# Inseriamo la tassonomia nel JSON\n",
        "data['taxonomy'] = taxonomy\n",
        "print(f\"‚úÖ Sezione 'taxonomy' aggiunta con {len(taxonomy)} voci.\")\n",
        "\n",
        "# 4. SALVATAGGIO\n",
        "with open(JSON_PATH, 'w') as f:\n",
        "    json.dump(data, f, indent=4)\n",
        "\n",
        "# Aggiorniamo anche la copia _recordings per sicurezza\n",
        "shutil.copy2(JSON_PATH, JSON_REC_PATH)\n",
        "\n",
        "print(f\"üíæ File salvati e riparati:\\n   -> {os.path.basename(JSON_PATH)}\\n   -> {os.path.basename(JSON_REC_PATH)}\")\n",
        "print(\"üöÄ ORA ESEGUI LA CELLA 4 (Dovrebbe funzionare!)\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1lu3dBYs_I7t",
        "outputId": "0a169192-47e1-4b70-c4a4-5dfd3fdf178e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "1lu3dBYs_I7t",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Analisi file: /content/actionformer_workspace/actionformer_split.json\n",
            "‚úÖ Trovate 1 classi uniche.\n",
            "   Esempio: ['test']...\n",
            "‚úÖ Sezione 'taxonomy' aggiunta con 1 voci.\n",
            "üíæ File salvati e riparati:\n",
            "   -> actionformer_split.json\n",
            "   -> actionformer_split_recordings.json\n",
            "üöÄ ORA ESEGUI LA CELLA 4 (Dovrebbe funzionare!)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "a0a6122b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "id": "a0a6122b",
        "outputId": "77d13e37-a932-44ab-f87c-9728ce9bc8d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Inizializzazione ActionFormer (VERBOSE MODE)...\n",
            "üîç [System] Ricerca file feature...\n",
            "‚úÖ Trovati 384 file in: /content/temp_omnivore_features/omnivore\n",
            "üñ•Ô∏è Hardware: CPU\n",
            "üîç [System] Generazione JSON per 384 video...\n",
            "‚úÖ [System] JSON pronto.\n",
            "üõ†Ô∏è [System] Patching librerie ActionFormer...\n",
            "üèÅ START PROCESS...\n",
            "[DEBUG] 1. Setup Iniziale...\n",
            "[DEBUG]    -> Device: cpu\n",
            "[DEBUG] 2. Dataset & Loader...\n",
            "[DEBUG]    -> Video trovati: 384\n",
            "[DEBUG] 3. Modello...\n",
            "[DEBUG] 4. Caricamento Pesi: ego4d_omnivore.pth.tar (Attendere...)\n",
            "[DEBUG]    -> Pesi caricati correttamente.\n",
            "[DEBUG] 5. Avvio Inferenza...\n",
            "\n",
            "0%|          | 0/384 [00:00<?, ?it/s]\n",
            "0%|          | 1/384 [00:02<15:33,  2.44s/it]\n",
            "1%|          | 2/384 [00:04<14:28,  2.27s/it]\n",
            "1%|          | 3/384 [00:07<15:13,  2.40s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3030664226.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0mprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTDOUT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbufsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniversal_newlines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m''\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# @title 4. Inferenza ActionFormer (Auto-Detect, Fix Totali & Verbose Log)\n",
        "import os\n",
        "import glob\n",
        "import subprocess\n",
        "import yaml\n",
        "import sys\n",
        "import shutil\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Forza output immediato senza buffering\n",
        "os.environ['PYTHONUNBUFFERED'] = '1'\n",
        "\n",
        "print(\"üöÄ Inizializzazione ActionFormer (VERBOSE MODE)...\", flush=True)\n",
        "\n",
        "# --- 1. CONFIGURAZIONE AMBIENTE ---\n",
        "AF_WORKDIR = \"/content/actionformer_workspace\"\n",
        "AF_REPO_PATH = os.path.join(AF_WORKDIR, \"multi_step_localization\")\n",
        "if not os.path.exists(AF_REPO_PATH): AF_REPO_PATH = AF_WORKDIR\n",
        "\n",
        "if AF_REPO_PATH not in sys.path: sys.path.append(AF_REPO_PATH)\n",
        "\n",
        "if 'PROJECT_DIR' not in locals():\n",
        "    if os.path.exists(\"/content/drive/MyDrive/MistakeDetection\"):\n",
        "        PROJECT_DIR = \"/content/drive/MyDrive/MistakeDetection\"\n",
        "    else:\n",
        "        PROJECT_DIR = \"/content/drive/MyDrive/CaptainCook4D\"\n",
        "\n",
        "# --- 2. CACCIA AL TESORO (TROVA I FILE .NPZ) ---\n",
        "print(\"üîç [System] Ricerca file feature...\", flush=True)\n",
        "search_paths = [\n",
        "    \"/content/temp_omnivore_features/omnivore\", # Path specificato da te\n",
        "    \"/content/temp_omnivore_features\",\n",
        "    os.path.join(PROJECT_DIR, \"features\", \"omnivore_video\"),\n",
        "    os.path.join(PROJECT_DIR, \"features\"),\n",
        "    os.path.join(PROJECT_DIR, \"omnivore_features\"),\n",
        "    \"/content/omnivore_features\",\n",
        "    \"/content/features\"\n",
        "]\n",
        "\n",
        "FOUND_FEAT_DIR = None\n",
        "num_files = 0\n",
        "\n",
        "for path in search_paths:\n",
        "    if os.path.exists(path):\n",
        "        files = glob.glob(os.path.join(path, \"*.npz\"))\n",
        "        if len(files) > 0: # Anche pochi file vanno bene\n",
        "            FOUND_FEAT_DIR = path\n",
        "            num_files = len(files)\n",
        "            print(f\"‚úÖ Trovati {num_files} file in: {path}\", flush=True)\n",
        "            break\n",
        "\n",
        "if not FOUND_FEAT_DIR:\n",
        "    print(\"‚ùå ERRORE: Non trovo i file .npz!\", flush=True)\n",
        "    # Creiamo dummy per test se non trova nulla\n",
        "    FOUND_FEAT_DIR = \"/content/temp_dummy\"\n",
        "    os.makedirs(FOUND_FEAT_DIR, exist_ok=True)\n",
        "    print(\"‚ö†Ô∏è Uso cartella dummy per evitare crash.\", flush=True)\n",
        "\n",
        "LOCAL_FEAT_DIR = FOUND_FEAT_DIR\n",
        "MY_CONFIG_PATH = os.path.join(AF_REPO_PATH, \"configs\", \"forced_config.yaml\")\n",
        "DATASETS_LIB_PATH = os.path.join(AF_REPO_PATH, \"actionformer\", \"libs\", \"datasets\")\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"üñ•Ô∏è Hardware: {DEVICE.upper()}\", flush=True)\n",
        "\n",
        "\n",
        "# --- 3. GENERAZIONE JSON SU FILE REALI ---\n",
        "print(f\"üîç [System] Generazione JSON per {num_files} video...\", flush=True)\n",
        "json_base = os.path.join(AF_WORKDIR, \"actionformer_split.json\")\n",
        "\n",
        "full_db = {}\n",
        "feature_files = glob.glob(os.path.join(LOCAL_FEAT_DIR, \"*.npz\"))\n",
        "\n",
        "for f in feature_files:\n",
        "    vid_name = os.path.basename(f).replace(\".npz\", \"\")\n",
        "    full_db[vid_name] = {\n",
        "        \"subset\": \"validation\",\n",
        "        \"annotations\": [{\"label\": \"test\", \"segment\": [0, 1]}]\n",
        "    }\n",
        "\n",
        "# Se vuoto aggiungi un dummy\n",
        "if not full_db:\n",
        "    full_db[\"dummy\"] = {\"subset\": \"validation\", \"annotations\": [{\"label\": \"test\", \"segment\": [0, 1]}]}\n",
        "\n",
        "with open(json_base, 'w') as f:\n",
        "    json.dump({\n",
        "        \"database\": full_db,\n",
        "        \"taxonomy\": [{\"id\":0,\"label\":\"test\",\"label_id\":0,\"nodeName\":\"test\"}],\n",
        "        \"version\": \"1.0\"\n",
        "    }, f)\n",
        "shutil.copy2(json_base, json_base.replace(\".json\",\"\")+\"_recordings.json\")\n",
        "print(f\"‚úÖ [System] JSON pronto.\", flush=True)\n",
        "\n",
        "\n",
        "# --- 4. RISCRITTURA LIBRERIE (Fix NPZ, Dim, Duration) ---\n",
        "print(\"üõ†Ô∏è [System] Patching librerie ActionFormer...\", flush=True)\n",
        "pycache_dir = os.path.join(DATASETS_LIB_PATH, \"__pycache__\")\n",
        "if os.path.exists(pycache_dir): shutil.rmtree(pycache_dir)\n",
        "\n",
        "# A. __INIT__.PY\n",
        "with open(os.path.join(DATASETS_LIB_PATH, \"__init__.py\"), 'w') as f:\n",
        "    f.write(\"from .datasets import make_dataset, make_data_loader\\n\")\n",
        "\n",
        "# B. DATASETS.PY\n",
        "datasets_code = r\"\"\"\n",
        "import torch\n",
        "import os\n",
        "\n",
        "_DATASET_REGISTRY = {}\n",
        "def register_dataset(name):\n",
        "    def decorator(cls):\n",
        "        _DATASET_REGISTRY[name] = cls\n",
        "        return cls\n",
        "    return decorator\n",
        "\n",
        "def make_dataset(name, is_training, split, **kwargs):\n",
        "    if name == 'thumos':\n",
        "        from .thumos14 import THUMOS14Dataset\n",
        "        return THUMOS14Dataset(is_training, split, **kwargs)\n",
        "    if name in _DATASET_REGISTRY:\n",
        "        return _DATASET_REGISTRY[name](is_training, split, **kwargs)\n",
        "    raise KeyError(f\"Dataset sconosciuto: {name}\")\n",
        "\n",
        "def make_data_loader(dataset, is_training, generator, batch_size, num_workers):\n",
        "    persistent = True if num_workers > 0 else False\n",
        "    loader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=batch_size, num_workers=num_workers,\n",
        "        shuffle=(True if is_training else False), collate_fn=None,\n",
        "        pin_memory=False,\n",
        "        drop_last=(True if is_training else False),\n",
        "        persistent_workers=persistent\n",
        "    )\n",
        "    return loader\n",
        "\"\"\"\n",
        "with open(os.path.join(DATASETS_LIB_PATH, \"datasets.py\"), 'w') as f: f.write(datasets_code)\n",
        "\n",
        "# C. THUMOS14.PY (ROBUST NPZ LOADER)\n",
        "thumos_code = r\"\"\"\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from .datasets import register_dataset\n",
        "\n",
        "@register_dataset(\"thumos\")\n",
        "class THUMOS14Dataset(Dataset):\n",
        "    def __init__(self, is_training, split, feat_folder, json_file, feat_stride, num_frames,\n",
        "                 default_fps, downsample_rate, max_seq_len, trunc_thresh, crop_ratio,\n",
        "                 input_dim, num_classes, file_prefix, file_ext, force_upsampling):\n",
        "        self.split = split\n",
        "        self.feat_folder = feat_folder\n",
        "        self.json_file = json_file\n",
        "        self.feat_stride = feat_stride\n",
        "        self.num_frames = num_frames\n",
        "        self.default_fps = default_fps\n",
        "        self.downsample_rate = downsample_rate\n",
        "        self.input_dim = input_dim\n",
        "        self.num_classes = num_classes\n",
        "        self.file_prefix = file_prefix\n",
        "        self.file_ext = file_ext\n",
        "        dict_db, label_dict = self._load_json_db(self.json_file)\n",
        "        self.data_list = [val for key, val in dict_db.items()]\n",
        "        self.label_dict = label_dict\n",
        "\n",
        "    def get_attributes(self): return self.data_list, self.label_dict, self.num_classes\n",
        "    def _load_json_db(self, json_file):\n",
        "        with open(json_file, 'r') as fid: json_data = json.load(fid)\n",
        "        label_dict = {'test': 0}\n",
        "        if 'taxonomy' in json_data:\n",
        "            for act in json_data['taxonomy']:\n",
        "                label_dict[act.get('label', 'unknown')] = act.get('id', 0)\n",
        "        dict_db = json_data['database']\n",
        "        for vid in dict_db:\n",
        "            if 'id' not in dict_db[vid]: dict_db[vid]['id'] = vid\n",
        "        return dict_db, label_dict\n",
        "    def __len__(self): return len(self.data_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data_list[idx]\n",
        "        feat_file = os.path.join(self.feat_folder, self.file_prefix + item['id'] + self.file_ext)\n",
        "\n",
        "        # --- ROBUST LOADING (FIXED) ---\n",
        "        try:\n",
        "            if not os.path.exists(feat_file):\n",
        "                feats = np.zeros((self.input_dim, 100), dtype=np.float32)\n",
        "            else:\n",
        "                loaded = np.load(feat_file)\n",
        "                if isinstance(loaded, np.lib.npyio.NpzFile):\n",
        "                    keys = loaded.files\n",
        "                    if 'feats' in keys: feats = loaded['feats']\n",
        "                    elif 'arr_0' in keys: feats = loaded['arr_0']\n",
        "                    else: feats = loaded[keys[0]]\n",
        "                else:\n",
        "                    feats = loaded\n",
        "\n",
        "            feats = feats.astype(np.float32)\n",
        "\n",
        "            # Fix Dimensions: Ensure (Channels, Time) -> (1024, N)\n",
        "            if feats.ndim == 2:\n",
        "                if feats.shape[1] == self.input_dim: # Se √® (N, 1024) -> Trasponi\n",
        "                    feats = feats.transpose()\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading {feat_file}: {e}\")\n",
        "            feats = np.zeros((self.input_dim, 100), dtype=np.float32)\n",
        "\n",
        "        if self.downsample_rate > 1: feats = feats[:, ::self.downsample_rate]\n",
        "        feat_stride = self.feat_stride * self.downsample_rate\n",
        "\n",
        "        num_feat_frames = feats.shape[1]\n",
        "        duration = (num_feat_frames * feat_stride) / self.default_fps\n",
        "\n",
        "        feats = torch.from_numpy(np.ascontiguousarray(feats))\n",
        "\n",
        "        return {\n",
        "            'video_id': item['id'], 'feats': feats,\n",
        "            'segments': torch.zeros((0, 2), dtype=torch.float32),\n",
        "            'labels': torch.zeros((0), dtype=torch.int64),\n",
        "            'fps': self.default_fps, 'feat_stride': feat_stride,\n",
        "            'feat_num_frames': num_feat_frames, 'duration': duration\n",
        "        }\n",
        "\"\"\"\n",
        "with open(os.path.join(DATASETS_LIB_PATH, \"thumos14.py\"), 'w') as f: f.write(thumos_code)\n",
        "\n",
        "\n",
        "# --- 5. CONFIGURAZIONE ---\n",
        "cands = glob.glob(os.path.join(PROJECT_DIR, \"**\", \"*omnivore*.pth*\"), recursive=True)\n",
        "if not cands: raise FileNotFoundError(\"‚ùå Modello non trovato!\")\n",
        "MODEL_CKPT = sorted(cands)[-1]\n",
        "\n",
        "config_data = {\n",
        "    'dataset_name': 'thumos',\n",
        "    'model_name': 'LocPointTransformer',\n",
        "    'output_folder': './ckpt/',\n",
        "    'devices': [DEVICE],\n",
        "    'dataset': {\n",
        "        'json_file': json_base, 'feat_folder': LOCAL_FEAT_DIR, 'file_prefix': '', 'file_ext': '.npz',\n",
        "        'input_dim': 1024, 'feat_stride': 16, 'num_classes': 24, 'default_fps': 30, 'num_frames': 32,\n",
        "        'downsample_rate': 1, 'max_seq_len': 2304, 'trunc_thresh': 0.5, 'crop_ratio': None, 'force_upsampling': False,\n",
        "    },\n",
        "    'eval': {'batch_size': 1, 'nms_score_thres': 0.1},\n",
        "    'loader': {'batch_size': 1, 'num_workers': 0},\n",
        "    'model': {\n",
        "        'backbone_type': 'convTransformer', 'fpn_type': 'identity', 'backbone_arch': [2, 2, 5],\n",
        "        'scale_factor': 2, 'input_dim': 1024, 'max_seq_len': 2304, 'n_head': 4,\n",
        "        'embd_kernel_size': 3, 'embd_with_ln': True, 'fpn_with_ln': True, 'fpn_start_level': 0,\n",
        "        'head_num_layers': 3, 'head_kernel_size': 3, 'head_with_ln': True, 'use_rel_pe': False,\n",
        "        'num_classes': 24,\n",
        "        'regression_range': [[0, 4], [4, 8], [8, 16], [16, 32], [32, 64], [64, 10000]],\n",
        "        'embd_dim': 512, 'fpn_dim': 512, 'head_dim': 512,\n",
        "        'use_abs_pe': False, 'max_buffer_len_factor': 6.0, 'n_mha_win_size': 19,\n",
        "        'train_cfg': {\n",
        "            'center_sample': 'radius', 'center_sample_radius': 1.5, 'loss_weight': 1.0,\n",
        "            'cls_prior_prob': 0.01, 'init_loss_norm': 2000, 'clip_grad_l2norm': -1,\n",
        "            'label_smoothing': 0.0, 'dropout': 0.1, 'droppath': 0.1, 'head_empty_cls': []\n",
        "        },\n",
        "        'test_cfg': {\n",
        "            'pre_nms_thresh': 0.001, 'pre_nms_topk': 5000, 'iou_threshold': 0.1, 'min_score': 0.01,\n",
        "            'max_seg_num': 1000, 'nms_method': 'soft', 'nms_sigma': 0.5, 'voting_thresh': 0.75,\n",
        "            'multiclass_nms': True, 'duration_thresh': 0.05\n",
        "        }\n",
        "    },\n",
        "    'train': {'head_dim': 512}\n",
        "}\n",
        "with open(MY_CONFIG_PATH, 'w') as f: yaml.dump(config_data, f)\n",
        "\n",
        "\n",
        "# --- 6. EVAL SCRIPT (Con LOG Dettagliati) ---\n",
        "eval_standalone = r\"\"\"\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import argparse\n",
        "import numpy as np\n",
        "import yaml\n",
        "import pickle\n",
        "import json\n",
        "import sys\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "def log(msg): print(f\"[DEBUG] {msg}\", flush=True)\n",
        "def load_config(path):\n",
        "    with open(path, 'r') as f: return yaml.safe_load(f)\n",
        "\n",
        "from actionformer.libs.modeling import make_meta_arch\n",
        "from actionformer.libs.datasets import make_dataset, make_data_loader\n",
        "\n",
        "def main():\n",
        "    log(\"1. Setup Iniziale...\")\n",
        "    config_path = r'\"\"\" + MY_CONFIG_PATH + r\"\"\"'\n",
        "    cfg = load_config(config_path)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    log(f\"   -> Device: {device}\")\n",
        "\n",
        "    # Params Clean\n",
        "    toxic = ['backbone', 'division_type', 'videos_type']\n",
        "    for k in toxic:\n",
        "        if k in cfg['dataset']: del cfg['dataset'][k]\n",
        "\n",
        "    log(\"2. Dataset & Loader...\")\n",
        "    val_dataset = make_dataset(cfg['dataset_name'], False, ['validation'], **cfg['dataset'])\n",
        "    val_loader = make_data_loader(val_dataset, False, None, **cfg['loader'])\n",
        "    log(f\"   -> Video trovati: {len(val_dataset)}\")\n",
        "\n",
        "    log(\"3. Modello...\")\n",
        "    model = make_meta_arch(cfg['model_name'], **cfg['model'])\n",
        "    model = model.to(device)\n",
        "\n",
        "    ckpt_path = r'\"\"\" + MODEL_CKPT + r\"\"\"'\n",
        "    log(f\"4. Caricamento Pesi: {os.path.basename(ckpt_path)} (Attendere...)\")\n",
        "    try:\n",
        "        checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "        state_dict = checkpoint.get('state_dict_ema', checkpoint.get('model', checkpoint.get('state_dict', checkpoint)))\n",
        "        model.load_state_dict(state_dict, strict=False)\n",
        "        log(\"   -> Pesi caricati correttamente.\")\n",
        "    except Exception as e:\n",
        "        log(f\"   ‚ùå ERRORE Pesi: {e}\")\n",
        "        return\n",
        "\n",
        "    model.eval()\n",
        "    results = {'video_ids': [], 'segment_intervals': [], 'scores': [], 'labels': []}\n",
        "\n",
        "    log(\"5. Avvio Inferenza...\")\n",
        "    with torch.no_grad():\n",
        "        for i, batch in tqdm(enumerate(val_loader), total=len(val_loader), file=sys.stdout):\n",
        "            try:\n",
        "                # Prepare Input\n",
        "                model_inputs = []\n",
        "                for k in range(len(batch['video_id'])):\n",
        "                    input_item = {\n",
        "                        'feats': batch['feats'][k].to(device),\n",
        "                        'feat_num_frames': batch['feat_num_frames'][k].to(device),\n",
        "                        'video_id': batch['video_id'][k],\n",
        "                        'fps': batch['fps'][k].item(),\n",
        "                        'feat_stride': batch['feat_stride'][k].item(),\n",
        "                        'duration': batch['duration'][k].item()\n",
        "                    }\n",
        "                    model_inputs.append(input_item)\n",
        "\n",
        "                # Forward Pass\n",
        "                output = model(model_inputs)\n",
        "\n",
        "                # Collect\n",
        "                for k in range(len(output)):\n",
        "                    results['video_ids'].append(batch['video_id'][k])\n",
        "                    results['segment_intervals'].append(output[k]['segments'].cpu().numpy())\n",
        "                    results['scores'].append(output[k]['scores'].cpu().numpy())\n",
        "                    results['labels'].append(output[k]['labels'].cpu().numpy())\n",
        "            except Exception as e:\n",
        "                log(f\"‚ö†Ô∏è Errore nel video {batch.get('video_id', '?')}: {e}\")\n",
        "                continue\n",
        "\n",
        "    out_dir = cfg['output_folder']\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    log(\"6. Salvataggio Risultati JSON...\")\n",
        "    json_out = {'results': {}}\n",
        "    for i, vid in enumerate(results['video_ids']):\n",
        "        segs = results['segment_intervals'][i]\n",
        "        scrs = results['scores'][i]\n",
        "        lbls = results['labels'][i]\n",
        "        res_list = []\n",
        "        for j in range(len(segs)):\n",
        "            res_list.append({\n",
        "                'label': int(lbls[j]), 'score': float(scrs[j]),\n",
        "                'segment': [float(segs[j][0]), float(segs[j][1])]\n",
        "            })\n",
        "        json_out['results'][vid] = res_list\n",
        "\n",
        "    final_path = os.path.join(out_dir, \"results.json\")\n",
        "    with open(final_path, 'w') as f: json.dump(json_out, f)\n",
        "    log(f\"‚úÖ COMPLETATO! Risultati salvati in: {final_path}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\"\"\"\n",
        "with open(os.path.join(AF_REPO_PATH, \"eval_standalone.py\"), 'w') as f: f.write(eval_standalone)\n",
        "\n",
        "# --- RUN ---\n",
        "print(\"üèÅ START PROCESS...\", flush=True)\n",
        "os.chdir(AF_REPO_PATH)\n",
        "cmd = [\"python\", \"eval_standalone.py\"]\n",
        "\n",
        "# Popen per vedere output live\n",
        "process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1, universal_newlines=True)\n",
        "while True:\n",
        "    output = process.stdout.readline()\n",
        "    if output == '' and process.poll() is not None: break\n",
        "    if output: print(output.strip(), flush=True)\n",
        "\n",
        "if process.poll() == 0:\n",
        "    print(\"\\nüéâ SUCCESSO! Ora vai alla Cella 5.\")\n",
        "else:\n",
        "    print(f\"\\n‚ùå ERRORE (Code {process.poll()}).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68440217",
      "metadata": {
        "id": "68440217"
      },
      "source": [
        "# Pooling, Zipping, Upload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af9f40c2",
      "metadata": {
        "id": "af9f40c2"
      },
      "outputs": [],
      "source": [
        "# @title 5. Pooling e Export Finale\n",
        "import numpy as np\n",
        "import json\n",
        "import glob\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "\n",
        "if 'PROJECT_DIR' not in locals(): PROJECT_DIR = \"/content/drive/MyDrive/MistakeDetection\"\n",
        "\n",
        "RESULTS_JSON = os.path.join(AF_WORKDIR, \"output_results\", \"results.json\")\n",
        "TEMP_EMB_DIR = os.path.join(AF_WORKDIR, \"step_embeddings_temp\")\n",
        "if os.path.exists(TEMP_EMB_DIR): shutil.rmtree(TEMP_EMB_DIR)\n",
        "os.makedirs(TEMP_EMB_DIR, exist_ok=True)\n",
        "\n",
        "FINAL_DATA_DIR = os.path.join(PROJECT_DIR, \"data\")\n",
        "if not os.path.exists(FINAL_DATA_DIR): os.makedirs(FINAL_DATA_DIR, exist_ok=True)\n",
        "ZIP_OUTPUT_NAME = \"step_embeddings\"\n",
        "\n",
        "def load_npz(path):\n",
        "    try:\n",
        "        with np.load(path) as data:\n",
        "            if 'features' in data: return data['features']\n",
        "            if 'arr_0' in data: return data['arr_0']\n",
        "            return data[list(data.keys())[0]]\n",
        "    except: return None\n",
        "\n",
        "if os.path.exists(RESULTS_JSON):\n",
        "    print(\"üöÄ Inizio elaborazione embedding...\")\n",
        "\n",
        "    with open(RESULTS_JSON, 'r') as f:\n",
        "        data = json.load(f)\n",
        "        preds = data.get('results', data)\n",
        "\n",
        "    # Mappa file locali delle feature Omnivore\n",
        "    file_map = {os.path.basename(f).split('.')[0]: f for f in glob.glob(os.path.join(LOCAL_FEAT_DIR, \"*.npz\"))}\n",
        "\n",
        "    count = 0\n",
        "    fps, stride = 30.0, 16.0\n",
        "    feat_rate = fps / stride\n",
        "\n",
        "    print(f\"   Analisi di {len(preds)} video...\")\n",
        "\n",
        "    for video_id, segments in tqdm(preds.items()):\n",
        "        feat_path = file_map.get(video_id)\n",
        "        if not feat_path:\n",
        "            # Try fuzzy match\n",
        "            candidates = [f for k, f in file_map.items() if video_id in k]\n",
        "            if candidates: feat_path = candidates[0]\n",
        "\n",
        "        if not feat_path: continue\n",
        "\n",
        "        full_feats = load_npz(feat_path)\n",
        "        if full_feats is None: continue\n",
        "\n",
        "        step_embeddings = []\n",
        "        step_metadata = []\n",
        "\n",
        "        for seg in segments:\n",
        "            t_start, t_end = seg['segment']\n",
        "            # Filtriamo segmenti troppo brevi o con score bassissimo\n",
        "            if seg['score'] < 0.05: continue # Abbassato leggermente per essere inclusivi\n",
        "\n",
        "            s_idx, e_idx = int(t_start * feat_rate), int(t_end * feat_rate)\n",
        "            s_idx, e_idx = max(0, s_idx), min(len(full_feats), e_idx)\n",
        "            if e_idx <= s_idx: e_idx = s_idx + 1\n",
        "\n",
        "            if s_idx < len(full_feats):\n",
        "                # MEDIA delle feature nel segmento (Pooling)\n",
        "                pooled = np.mean(full_feats[s_idx:e_idx], axis=0)\n",
        "                step_embeddings.append(pooled)\n",
        "                step_metadata.append(seg)\n",
        "\n",
        "        if step_embeddings:\n",
        "            np.savez_compressed(\n",
        "                os.path.join(TEMP_EMB_DIR, f\"{video_id}_steps.npz\"),\n",
        "                embeddings=np.array(step_embeddings),\n",
        "                metadata=step_metadata\n",
        "            )\n",
        "            count += 1\n",
        "\n",
        "    print(f\"\\n‚úÖ Generati {count} file .npz in {TEMP_EMB_DIR}\")\n",
        "\n",
        "    # Zipping e Copy su Drive\n",
        "    print(f\"üì¶ Creazione archivio ZIP...\")\n",
        "    zip_path = shutil.make_archive(\n",
        "        base_name=os.path.join(AF_WORKDIR, ZIP_OUTPUT_NAME),\n",
        "        format='zip',\n",
        "        root_dir=TEMP_EMB_DIR\n",
        "    )\n",
        "\n",
        "    print(f\"‚òÅÔ∏è Upload su Drive: {FINAL_DATA_DIR}...\")\n",
        "    try:\n",
        "        shutil.copy2(zip_path, FINAL_DATA_DIR)\n",
        "        print(f\"‚úÖ SUCCESSO! File salvato in: {os.path.join(FINAL_DATA_DIR, ZIP_OUTPUT_NAME + '.zip')}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Errore durante l'upload: {e}\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå Nessun risultato inferenza trovato. Verifica che la Cella 4 abbia finito senza errori.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}