{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8434760c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "import sys\n",
        "import zipfile\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cee1a5b4",
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2293349394.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# --- 2. CONFIGURAZIONE GITHUB (PRIVATO) ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# --- 1. SETUP AMBIENTE E DRIVE ---\n",
        "\n",
        "from google.colab import drive\n",
        "from google.colab import userdata\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# --- 2. CONFIGURAZIONE GITHUB (PRIVATO) ---\n",
        "# Sostituisci QUESTE variabili con i tuoi dati reali\n",
        "GITHUB_USER = 'MarcoPernoVDP'\n",
        "REPO_NAME = 'MistakeDetection'\n",
        "BRANCH = 'main'\n",
        "\n",
        "# Recupera il Token dai Segreti di Colab (\"GITHUB_TOKEN\")\n",
        "try:\n",
        "    GIT_TOKEN = userdata.get('GITHUB_TOKEN')\n",
        "except ImportError:\n",
        "    # Se non usi i Segreti, incolla il token qui (SCONSIGLIATO ma funziona)\n",
        "    GIT_TOKEN = \"INCOLLA_QUI_IL_TUO_PAT_SE_NON_USI_SEGRETI\"\n",
        "except Exception:\n",
        "    print(\"âš ï¸ ERRORE: Non ho trovato il segreto 'GITHUB_TOKEN'.\")\n",
        "    print(\"Vai sulla chiave inglese a sinistra -> Aggiungi nuovo segreto -> Nome: GITHUB_TOKEN\")\n",
        "    raise\n",
        "\n",
        "REPO_URL = f'https://{GIT_TOKEN}@github.com/{GITHUB_USER}/{REPO_NAME}.git'\n",
        "ROOT_DIR = f'/content/{REPO_NAME}'\n",
        "\n",
        "# --- 3. CLONE O AGGIORNAMENTO CODICE ---\n",
        "if not os.path.exists(ROOT_DIR):\n",
        "    print(f\"ðŸ”„ Clonazione del repository {REPO_NAME}...\")\n",
        "    !git clone {REPO_URL}\n",
        "else:\n",
        "    print(f\"ðŸ”„ Aggiornamento repository (git pull)...\")\n",
        "    %cd {ROOT_DIR}\n",
        "    !git pull origin {BRANCH}\n",
        "    %cd /content\n",
        "\n",
        "# --- 4. CONFIGURAZIONE PATH PYTHON ---\n",
        "if ROOT_DIR not in sys.path:\n",
        "    sys.path.append(ROOT_DIR)\n",
        "    print(f\"âœ… Aggiunto {ROOT_DIR} al path di sistema.\")\n",
        "\n",
        "# --- 5. INSTALLAZIONE DIPENDENZE ---\n",
        "\n",
        "#\n",
        "\n",
        "print(\"\\nSETUP COMPLETATO\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "G6wFF41DtQ4b",
      "metadata": {
        "id": "G6wFF41DtQ4b"
      },
      "source": [
        "# Setup environement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "kGMl6Pu2rSYk",
      "metadata": {
        "id": "kGMl6Pu2rSYk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device in uso: cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device in uso:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UtEsNOoRtWa2",
      "metadata": {
        "id": "UtEsNOoRtWa2"
      },
      "source": [
        "# Setup Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d243ace1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['.git', '.gitignore', 'checkpoints', 'configs', 'data', 'dataset', 'models', 'notebooks', 'utils', '_file']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "project_root = os.path.abspath(\"..\")\n",
        "print(os.listdir(project_root))  # verifica che ci sia 'utils'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caea429b",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Root del progetto nella VM di Colab\n",
        "project_root = \"/content/AML_MistakeDetection\"\n",
        "os.makedirs(project_root, exist_ok=True)\n",
        "\n",
        "if project_root not in sys.path:\n",
        "    sys.path.append(project_root)\n",
        "\n",
        "# Importa la funzione\n",
        "from utils.setup_dataset import setup_dataset\n",
        "\n",
        "# Percorso dei dati su Drive\n",
        "data_source_path = \"/content/drive/MyDrive/AML_MistakeDetection_DATA\"\n",
        "\n",
        "# Chiama la funzione\n",
        "setup_dataset(data_source_path, project_root)\n",
        "\n",
        "print(f\"Dati estratti e organizzati in: {os.path.join(project_root, 'data')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EfekzBNAtZ_S",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfekzBNAtZ_S",
        "outputId": "df7f41c8-69d3-421e-e22a-acc737d48448"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cartella 'annotation_json' copiata in 'c:\\Users\\dylan\\Desktop\\UniversitÃ \\5Â° ANNO\\AML\\MistakeDetection\\data/'.\n",
            "File ZIP 'c:\\Users\\dylan\\Desktop\\UniversitÃ \\5Â° ANNO\\AML\\MistakeDetection\\_file\\omnivore_test.zip' estratto in 'c:\\Users\\dylan\\Desktop\\UniversitÃ \\5Â° ANNO\\AML\\MistakeDetection\\data/'.\n",
            "Dati estratti e organizzati in: c:\\Users\\dylan\\Desktop\\UniversitÃ \\5Â° ANNO\\AML\\MistakeDetection\\data\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# Root del progetto (notebook in Notebooks/)\n",
        "project_root = os.path.abspath(\"..\")\n",
        "if project_root not in sys.path:\n",
        "    sys.path.append(project_root)\n",
        "\n",
        "# Importa la funzione\n",
        "from utils.setup_dataset import setup_dataset\n",
        "\n",
        "# Path della cartella _file\n",
        "data_source_path = os.path.join(project_root, \"_file\")\n",
        "\n",
        "\n",
        "# Chiama la funzione\n",
        "setup_dataset(data_source_path, project_root)\n",
        "\n",
        "print(f\"Dati estratti e organizzati in: {os.path.join(project_root, 'data')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "G7sq4nHfy-AZ",
      "metadata": {
        "id": "G7sq4nHfy-AZ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open(\"/content/drive/MyDrive/AML_MistakeDetection_DATA/annotation_json/complete_step_annotations.json\") as f:\n",
        "    annotations = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8N7MZHSuh4N",
      "metadata": {
        "id": "e8N7MZHSuh4N"
      },
      "outputs": [],
      "source": [
        "class VideoFeatureDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.from_numpy(X).float()\n",
        "        self.y = torch.from_numpy(y).long()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TAUlHVSAykFa",
      "metadata": {
        "id": "TAUlHVSAykFa"
      },
      "outputs": [],
      "source": [
        "def get_labels_for_npz(npz_file, annotations):\n",
        "    # es: \"10_3_360.mp4_1s_1s.npz\"\n",
        "    base = os.path.basename(npz_file)\n",
        "    activity, attempt = base.split(\"_\")[:2]  # \"10\", \"3\"\n",
        "    recording_id = f\"{activity}_{attempt}\"\n",
        "\n",
        "    # carica feature\n",
        "    data = np.load(npz_file)\n",
        "    arr = data[list(data.keys())[0]]  # shape (N, 1024)\n",
        "    N = arr.shape[0]\n",
        "\n",
        "    labels = np.zeros(N, dtype=np.int64)  # default: normal = no-error = 0\n",
        "\n",
        "    # trova annotation di questo recording\n",
        "    info = annotations[recording_id]\n",
        "    steps = info[\"steps\"]\n",
        "\n",
        "    # assegnazione label per ogni secondo\n",
        "    for step in steps:\n",
        "        has_error = int(step[\"has_errors\"])  # Trueâ†’1, Falseâ†’0\n",
        "        start = step[\"start_time\"]\n",
        "        end   = step[\"end_time\"]\n",
        "\n",
        "        if start == -1 or end == -1 or has_error == 0:\n",
        "            continue\n",
        "\n",
        "        for sec in range(int(start), int(end) + 1, 1):\n",
        "            sec_start = sec\n",
        "            sec_end   = sec + 1\n",
        "\n",
        "            # check overlap\n",
        "            if sec_start >= start and sec_end <= end: # i secondi ai bordi avranno sempre il valore di default (norml = no-error = 0)\n",
        "                labels[sec] = has_error\n",
        "\n",
        "    return arr, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w6VeoKhMuHuZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6VeoKhMuHuZ",
        "outputId": "0e6bff9c-7933-4fa4-d77a-e2d3972eecc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(340320, 1024) (340320,)\n"
          ]
        }
      ],
      "source": [
        "all_X = []\n",
        "all_y = []\n",
        "\n",
        "extract_dir = \"/content/omnivore_extracted/omnivore\"\n",
        "\n",
        "for f in sorted(os.listdir(extract_dir)):\n",
        "    if f.endswith(\".npz\"):\n",
        "        X, y = get_labels_for_npz(os.path.join(extract_dir, f), annotations)\n",
        "        all_X.append(X)\n",
        "        all_y.append(y)\n",
        "\n",
        "X = np.concatenate(all_X, axis=0)\n",
        "y = np.concatenate(all_y, axis=0)\n",
        "\n",
        "print(X.shape, y.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DZDtlZE8up0H",
      "metadata": {
        "id": "DZDtlZE8up0H"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# \"Step (S)\" data splits (come usato nel paper CaptainCook4D)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "# implementare anche \"Recording (R)\" data splits (come fatto nel paper) per vederne differenze"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WprZqP5g-zGI",
      "metadata": {
        "id": "WprZqP5g-zGI"
      },
      "outputs": [],
      "source": [
        "train_dataset = VideoFeatureDataset(X_train, y_train)\n",
        "test_dataset  = VideoFeatureDataset(X_test,  y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FQ-eX8oeuwVg",
      "metadata": {
        "id": "FQ-eX8oeuwVg"
      },
      "outputs": [],
      "source": [
        "batch_size = 512\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SigNHU8ctaWJ",
      "metadata": {
        "id": "SigNHU8ctaWJ"
      },
      "source": [
        "# MLP (Version 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "O74QGf1Qo2sK",
      "metadata": {
        "id": "O74QGf1Qo2sK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MLPCapitainCook(nn.Module):\n",
        "    def __init__(self, in_features: int, p: float = 0.5) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features, 256)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p)       # Dropout layer con probabilitÃ  p\n",
        "        self.fc2 = nn.Linear(256, 1)       # Output logit (senza sigmoid)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)                # Applica Dropout solo in TRAIN\n",
        "        x = self.fc2(x)                    # Output logit\n",
        "        return x                           # no Sigmoid qui\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UWoFzaKgp-I_",
      "metadata": {
        "id": "UWoFzaKgp-I_"
      },
      "outputs": [],
      "source": [
        "model = MLPCapitainCook(1024).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6FdOuKJopr3m",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FdOuKJopr3m",
        "outputId": "f888829f-fbfc-47d3-b01f-88e429b1457f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.5\n"
          ]
        }
      ],
      "source": [
        "lr = 0.0001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
        "# count classi\n",
        "num_error = (y == 1).sum()   # classe â€œ1â€ = \"error\"\n",
        "num_normal = (y == 0).sum()   # classe â€œ0â€ = \"normal\"\n",
        "\n",
        "# pos_weight = quanto pesa la classe â€œpositivaâ€ = classe \"1\"\n",
        "#pos_weight_value = num_normal/num_error\n",
        "pos_weight_value = 1.5\n",
        "print(pos_weight_value)\n",
        "pos_weight = torch.tensor([pos_weight_value], device=device)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "epochs = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4u_a6jPgq7OI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4u_a6jPgq7OI",
        "outputId": "b0b8b41c-0c65-4729-92b1-8f51632af365"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50 - Train Loss: 0.6710 - Val Loss: 0.6475 - Acc: 0.7680 - F1: 0.2537\n",
            "Epoch 2/50 - Train Loss: 0.6327 - Val Loss: 0.6185 - Acc: 0.7798 - F1: 0.3424\n",
            "Epoch 3/50 - Train Loss: 0.6049 - Val Loss: 0.5917 - Acc: 0.7903 - F1: 0.4095\n",
            "Epoch 4/50 - Train Loss: 0.5822 - Val Loss: 0.5715 - Acc: 0.7972 - F1: 0.4712\n",
            "Epoch 5/50 - Train Loss: 0.5638 - Val Loss: 0.5534 - Acc: 0.8049 - F1: 0.5094\n",
            "Epoch 6/50 - Train Loss: 0.5471 - Val Loss: 0.5384 - Acc: 0.8108 - F1: 0.5248\n",
            "Epoch 7/50 - Train Loss: 0.5331 - Val Loss: 0.5260 - Acc: 0.8173 - F1: 0.5480\n",
            "Epoch 8/50 - Train Loss: 0.5204 - Val Loss: 0.5156 - Acc: 0.8179 - F1: 0.5578\n",
            "Epoch 9/50 - Train Loss: 0.5097 - Val Loss: 0.5056 - Acc: 0.8226 - F1: 0.5754\n",
            "Epoch 10/50 - Train Loss: 0.4993 - Val Loss: 0.4971 - Acc: 0.8278 - F1: 0.5817\n",
            "Epoch 11/50 - Train Loss: 0.4901 - Val Loss: 0.4878 - Acc: 0.8301 - F1: 0.5992\n",
            "Epoch 12/50 - Train Loss: 0.4821 - Val Loss: 0.4823 - Acc: 0.8329 - F1: 0.6030\n",
            "Epoch 13/50 - Train Loss: 0.4750 - Val Loss: 0.4744 - Acc: 0.8342 - F1: 0.6142\n",
            "Epoch 14/50 - Train Loss: 0.4674 - Val Loss: 0.4697 - Acc: 0.8350 - F1: 0.6147\n",
            "Epoch 15/50 - Train Loss: 0.4600 - Val Loss: 0.4637 - Acc: 0.8386 - F1: 0.6189\n",
            "Epoch 16/50 - Train Loss: 0.4536 - Val Loss: 0.4585 - Acc: 0.8412 - F1: 0.6237\n",
            "Epoch 17/50 - Train Loss: 0.4479 - Val Loss: 0.4535 - Acc: 0.8412 - F1: 0.6372\n",
            "Epoch 18/50 - Train Loss: 0.4408 - Val Loss: 0.4477 - Acc: 0.8445 - F1: 0.6416\n",
            "Epoch 19/50 - Train Loss: 0.4358 - Val Loss: 0.4443 - Acc: 0.8460 - F1: 0.6380\n",
            "Epoch 20/50 - Train Loss: 0.4312 - Val Loss: 0.4399 - Acc: 0.8471 - F1: 0.6496\n",
            "Epoch 21/50 - Train Loss: 0.4267 - Val Loss: 0.4362 - Acc: 0.8500 - F1: 0.6544\n",
            "Epoch 22/50 - Train Loss: 0.4203 - Val Loss: 0.4347 - Acc: 0.8507 - F1: 0.6463\n",
            "Epoch 23/50 - Train Loss: 0.4167 - Val Loss: 0.4311 - Acc: 0.8521 - F1: 0.6507\n",
            "Epoch 24/50 - Train Loss: 0.4114 - Val Loss: 0.4273 - Acc: 0.8533 - F1: 0.6537\n",
            "Epoch 25/50 - Train Loss: 0.4076 - Val Loss: 0.4224 - Acc: 0.8531 - F1: 0.6648\n",
            "Epoch 26/50 - Train Loss: 0.4037 - Val Loss: 0.4208 - Acc: 0.8559 - F1: 0.6619\n",
            "Epoch 27/50 - Train Loss: 0.3991 - Val Loss: 0.4178 - Acc: 0.8558 - F1: 0.6671\n",
            "Epoch 28/50 - Train Loss: 0.3955 - Val Loss: 0.4145 - Acc: 0.8571 - F1: 0.6726\n",
            "Epoch 29/50 - Train Loss: 0.3921 - Val Loss: 0.4112 - Acc: 0.8576 - F1: 0.6765\n",
            "Epoch 30/50 - Train Loss: 0.3876 - Val Loss: 0.4113 - Acc: 0.8581 - F1: 0.6697\n",
            "Epoch 31/50 - Train Loss: 0.3837 - Val Loss: 0.4055 - Acc: 0.8600 - F1: 0.6827\n",
            "Epoch 32/50 - Train Loss: 0.3803 - Val Loss: 0.4025 - Acc: 0.8600 - F1: 0.6903\n",
            "Epoch 33/50 - Train Loss: 0.3763 - Val Loss: 0.4017 - Acc: 0.8615 - F1: 0.6886\n",
            "Epoch 34/50 - Train Loss: 0.3748 - Val Loss: 0.3983 - Acc: 0.8620 - F1: 0.6917\n",
            "Epoch 35/50 - Train Loss: 0.3713 - Val Loss: 0.3971 - Acc: 0.8618 - F1: 0.6902\n",
            "Epoch 36/50 - Train Loss: 0.3687 - Val Loss: 0.3948 - Acc: 0.8624 - F1: 0.6908\n",
            "Epoch 37/50 - Train Loss: 0.3649 - Val Loss: 0.3927 - Acc: 0.8632 - F1: 0.6979\n",
            "Epoch 38/50 - Train Loss: 0.3619 - Val Loss: 0.3921 - Acc: 0.8638 - F1: 0.6942\n",
            "Epoch 39/50 - Train Loss: 0.3591 - Val Loss: 0.3915 - Acc: 0.8646 - F1: 0.6910\n",
            "Epoch 40/50 - Train Loss: 0.3578 - Val Loss: 0.3882 - Acc: 0.8648 - F1: 0.6988\n",
            "Epoch 41/50 - Train Loss: 0.3545 - Val Loss: 0.3877 - Acc: 0.8663 - F1: 0.6988\n",
            "Epoch 42/50 - Train Loss: 0.3509 - Val Loss: 0.3864 - Acc: 0.8659 - F1: 0.6977\n",
            "Epoch 43/50 - Train Loss: 0.3493 - Val Loss: 0.3850 - Acc: 0.8674 - F1: 0.7001\n",
            "Epoch 44/50 - Train Loss: 0.3459 - Val Loss: 0.3831 - Acc: 0.8669 - F1: 0.7018\n",
            "Epoch 45/50 - Train Loss: 0.3418 - Val Loss: 0.3820 - Acc: 0.8677 - F1: 0.7031\n",
            "Epoch 46/50 - Train Loss: 0.3403 - Val Loss: 0.3825 - Acc: 0.8680 - F1: 0.6995\n",
            "Epoch 47/50 - Train Loss: 0.3389 - Val Loss: 0.3784 - Acc: 0.8698 - F1: 0.7097\n",
            "Epoch 48/50 - Train Loss: 0.3373 - Val Loss: 0.3777 - Acc: 0.8679 - F1: 0.7102\n",
            "Epoch 49/50 - Train Loss: 0.3335 - Val Loss: 0.3764 - Acc: 0.8692 - F1: 0.7094\n",
            "Epoch 50/50 - Train Loss: 0.3309 - Val Loss: 0.3773 - Acc: 0.8698 - F1: 0.7081\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    # -------------------------\n",
        "    #        TRAIN\n",
        "    # -------------------------\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device).float()\n",
        "\n",
        "        outputs = model(inputs)            # [B, 1]\n",
        "        outputs = outputs.squeeze(1)       # [B]\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "    # -------------------------\n",
        "    #        EVAL\n",
        "    # -------------------------\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device).float()\n",
        "\n",
        "            outputs = model(inputs).squeeze(1)  # logits\n",
        "\n",
        "            # same loss as train\n",
        "            val_loss = criterion(outputs, labels)\n",
        "            total_val_loss += val_loss.item()\n",
        "\n",
        "            # convert logits â†’ probabilities â†’ binary predictions\n",
        "            probs = torch.sigmoid(outputs)\n",
        "            preds = (probs >= 0.5).long()\n",
        "\n",
        "            all_preds.append(preds.cpu())\n",
        "            all_targets.append(labels.cpu())\n",
        "\n",
        "    # concat\n",
        "    all_preds = torch.cat(all_preds).numpy()\n",
        "    all_targets = torch.cat(all_targets).numpy()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(test_loader)\n",
        "    acc = accuracy_score(all_targets, all_preds)\n",
        "    f1  = f1_score(all_targets, all_preds, zero_division=0)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} \"\n",
        "          f\"- Train Loss: {avg_train_loss:.4f} \"\n",
        "          f\"- Val Loss: {avg_val_loss:.4f} \"\n",
        "          f\"- Acc: {acc:.4f} \"\n",
        "          f\"- F1: {f1:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Kernel Python LLM",
      "language": "python",
      "name": "kernel_llm"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
