{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a841a958",
      "metadata": {
        "id": "a841a958"
      },
      "source": [
        "# Environement Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "cee1a5b4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cee1a5b4",
        "outputId": "efe477f7-a461-449c-dc2f-4b1a9fb3a3f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”„ Aggiornamento repository (git pull)...\n",
            "/content/MistakeDetection\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 4 (delta 2), reused 4 (delta 2), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects: 100% (4/4), 1.06 KiB | 136.00 KiB/s, done.\n",
            "From https://github.com/MarcoPernoVDP/MistakeDetection\n",
            " * branch            main       -> FETCH_HEAD\n",
            "   727c1ca..17996eb  main       -> origin/main\n",
            "Updating 727c1ca..17996eb\n",
            "Fast-forward\n",
            " notebooks/test.ipynb | 203 \u001b[32m+++++++++++++++++++\u001b[m\u001b[31m--------------------------------\u001b[m\n",
            " 1 file changed, 75 insertions(+), 128 deletions(-)\n",
            "/content\n",
            "\n",
            "SETUP COMPLETATO\n"
          ]
        }
      ],
      "source": [
        "# ATTENZIONE: eseguire solo su Colab!\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# --- 1. SETUP AMBIENTE E DRIVE ---\n",
        "\n",
        "from google.colab import drive\n",
        "from google.colab import userdata\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# --- 2. CONFIGURAZIONE GITHUB (PRIVATO) ---\n",
        "# Sostituisci QUESTE variabili con i tuoi dati reali\n",
        "GITHUB_USER = 'MarcoPernoVDP'\n",
        "REPO_NAME = 'MistakeDetection'\n",
        "BRANCH = 'main'\n",
        "\n",
        "# Recupera il Token dai Segreti di Colab (\"GITHUB_TOKEN\")\n",
        "try:\n",
        "    GIT_TOKEN = userdata.get('GITHUB_TOKEN')\n",
        "except ImportError:\n",
        "    # Se non usi i Segreti, incolla il token qui (SCONSIGLIATO ma funziona)\n",
        "    GIT_TOKEN = \"INCOLLA_QUI_IL_TUO_PAT_SE_NON_USI_SEGRETI\"\n",
        "except Exception:\n",
        "    print(\"âš ï¸ ERRORE: Non ho trovato il segreto 'GITHUB_TOKEN'.\")\n",
        "    print(\"Vai sulla chiave inglese a sinistra -> Aggiungi nuovo segreto -> Nome: GITHUB_TOKEN\")\n",
        "    raise\n",
        "\n",
        "REPO_URL = f'https://{GIT_TOKEN}@github.com/{GITHUB_USER}/{REPO_NAME}.git'\n",
        "ROOT_DIR = f'/content/{REPO_NAME}'\n",
        "\n",
        "# --- 3. CLONE O AGGIORNAMENTO CODICE ---\n",
        "if not os.path.exists(ROOT_DIR):\n",
        "    print(f\"ðŸ”„ Clonazione del repository {REPO_NAME}...\")\n",
        "    !git clone {REPO_URL}\n",
        "else:\n",
        "    print(f\"ðŸ”„ Aggiornamento repository (git pull)...\")\n",
        "    %cd {ROOT_DIR}\n",
        "    !git pull origin {BRANCH}\n",
        "    %cd /content\n",
        "\n",
        "# --- 4. CONFIGURAZIONE PATH PYTHON ---\n",
        "if ROOT_DIR not in sys.path:\n",
        "    sys.path.append(ROOT_DIR)\n",
        "    print(f\"âœ… Aggiunto {ROOT_DIR} al path di sistema.\")\n",
        "\n",
        "# --- 5. INSTALLAZIONE DIPENDENZE ---\n",
        "\n",
        "#\n",
        "\n",
        "print(\"\\nSETUP COMPLETATO\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UtEsNOoRtWa2",
      "metadata": {
        "id": "UtEsNOoRtWa2"
      },
      "source": [
        "# Dataset Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "73e181dd",
      "metadata": {
        "id": "73e181dd",
        "outputId": "991bb26e-b0cb-4b7f-8e15-b9859dbeae04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Esecuzione su Google Colab\n",
            "Cartella 'annotation_json' copiata in '/content/MistakeDetection/data/'.\n",
            "File ZIP '/content/drive/MyDrive/AML_MistakeDetection_DATA/omnivore.zip' estratto in '/content/MistakeDetection/data/'.\n",
            "File ZIP '/content/drive/MyDrive/AML_MistakeDetection_DATA/omnivore_test.zip' estratto in '/content/MistakeDetection/data/'.\n",
            "Dati estratti e organizzati in: /content/MistakeDetection/data\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# Rileva se siamo su Google Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    ON_COLAB = True\n",
        "except ImportError:\n",
        "    ON_COLAB = False\n",
        "\n",
        "if ON_COLAB:\n",
        "    print(\"Esecuzione su Google Colab\")\n",
        "    project_root = \"/content/MistakeDetection\"\n",
        "    if project_root not in sys.path:\n",
        "        sys.path.append(project_root)\n",
        "    from utils.setup_dataset import setup_dataset\n",
        "    data_source_path = os.path.join(\"/content\", \"drive\", \"MyDrive\", \"AML_MistakeDetection_DATA\")\n",
        "else:\n",
        "    print(\"Esecuzione su ambiente locale (VSCode)\")\n",
        "    project_root = os.path.abspath(\"..\")\n",
        "    if project_root not in sys.path:\n",
        "        sys.path.append(project_root)\n",
        "    from utils.setup_dataset import setup_dataset\n",
        "    data_source_path = os.path.join(project_root, \"_file\")\n",
        "\n",
        "# Chiama la funzione\n",
        "setup_dataset(data_source_path, project_root)\n",
        "\n",
        "print(f\"Dati estratti e organizzati in: {os.path.join(project_root, 'data')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "G7sq4nHfy-AZ",
      "metadata": {
        "id": "G7sq4nHfy-AZ",
        "outputId": "cd0ee154-44b4-4b5d-a823-9eed6360715a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device in uso: cpu\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Root del progetto\n",
        "project_root = os.path.abspath(\"..\") # serve per far si che Python veda il percorso della cartella padre \"MistakeDetection\" per poter importare i moduli delle altre cartelle\n",
        "if project_root not in sys.path:\n",
        "    sys.path.append(project_root)\n",
        "\n",
        "from dataset.loader import CaptainCook4D_Dataset\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device in uso:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "75a3a690",
      "metadata": {
        "id": "75a3a690",
        "outputId": "3fef530a-8d16-46d1-e421-95f26047631e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full Dataset samples (100%): 9798\n",
            "\n",
            "Train samples (70%): 6858\n",
            "Validation samples (10%): 980\n",
            "Test samples (20%): 1960\n"
          ]
        }
      ],
      "source": [
        "if ON_COLAB:\n",
        "    # Definisci la root del progetto sul Drive\n",
        "    project_root = '/content/MistakeDetection'\n",
        "    # Percorsi dati\n",
        "    features_dir = os.path.join(project_root, \"data\", \"omnivore_test\")\n",
        "    annotations_dir = os.path.join(project_root, \"data\", \"annotation_json\")\n",
        "else:\n",
        "    features_dir = os.path.join(\"..\", \"data\", \"omnivore_test\")          # cartella npz\n",
        "    annotations_dir = os.path.join(\"..\", \"data\", \"annotation_json\")  # cartella JSON\n",
        "\n",
        "# --- Inizializza il dataset completo ---\n",
        "full_dataset = CaptainCook4D_Dataset(features_dir, annotations_dir)\n",
        "\n",
        "# --- Indici di train, val e test ---\n",
        "indices = list(range(len(full_dataset)))\n",
        "\n",
        "# 20% test\n",
        "trainval_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42, shuffle=True)\n",
        "# 10% validation sul rimanente\n",
        "val_relative_size = 0.1 / 0.8\n",
        "train_idx, val_idx = train_test_split(trainval_idx, test_size=val_relative_size, random_state=42, shuffle=True)\n",
        "\n",
        "# --- Creazione dei Subset ---\n",
        "train_dataset = Subset(full_dataset, train_idx)\n",
        "val_dataset   = Subset(full_dataset, val_idx)\n",
        "test_dataset  = Subset(full_dataset, test_idx)\n",
        "\n",
        "# --- Creazione DataLoader ---\n",
        "batch_size = 512\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# --- Controllo dimensioni ---\n",
        "print(f\"Full Dataset samples (100%): {len(full_dataset)}\\n\")\n",
        "print(f\"Train samples (70%): {len(train_dataset)}\")\n",
        "print(f\"Validation samples (10%): {len(val_dataset)}\")\n",
        "print(f\"Test samples (20%): {len(test_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "f77288d0",
      "metadata": {
        "id": "f77288d0",
        "outputId": "80e5acdf-ea15-4a71-d991-82c46f992911",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FULL DATASET shape: torch.Size([9798, 1024])\n",
            "Numero di campioni: 9798\n",
            "Numero di feature: 1024\n",
            "\n",
            "TRAIN DATASET:\n",
            "label_0 = 4189\n",
            "label_1 = 2669\n",
            "label_1/label_0 = 0.6371\n",
            "label_0/label_1 = 1.5695\n",
            "\n",
            "VALIDATION DATASET:\n",
            "label_0 = 597\n",
            "label_1 = 383\n",
            "label_1/label_0 = 0.6415\n",
            "label_0/label_1 = 1.5587\n",
            "\n",
            "TEST DATASET:\n",
            "label_0 = 1184\n",
            "label_1 = 776\n",
            "label_1/label_0 = 0.6554\n",
            "label_0/label_1 = 1.5258\n"
          ]
        }
      ],
      "source": [
        "print(f\"FULL DATASET shape: {full_dataset.shape()}\")  # Output leggibile\n",
        "print(f\"Numero di campioni: {full_dataset.shape()[0]}\")\n",
        "print(f\"Numero di feature: {full_dataset.shape()[1]}\")\n",
        "\n",
        "\n",
        "def count_labels(dataset, name=\"Dataset\"):\n",
        "    cnt_0, cnt_1 = 0, 0\n",
        "    for i in range(len(dataset)):\n",
        "        label = dataset[i][1].item()\n",
        "        if label == 0:\n",
        "            cnt_0 += 1\n",
        "        else:\n",
        "            cnt_1 += 1\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"label_0 = {cnt_0}\")\n",
        "    print(f\"label_1 = {cnt_1}\")\n",
        "    print(f\"label_1/label_0 = {cnt_1/cnt_0:.4f}\")\n",
        "    print(f\"label_0/label_1 = {cnt_0/cnt_1:.4f}\")\n",
        "    return cnt_0, cnt_1\n",
        "\n",
        "# Conta le label per ciascun subset\n",
        "train_cnt_0, train_cnt_1 = count_labels(train_dataset, \"TRAIN DATASET\")\n",
        "val_cnt_0, val_cnt_1     = count_labels(val_dataset, \"VALIDATION DATASET\")\n",
        "test_cnt_0, test_cnt_1   = count_labels(test_dataset, \"TEST DATASET\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "6a5eed01",
      "metadata": {
        "id": "6a5eed01",
        "outputId": "7c2b2b89-ea25-4d92-ee08-4be468c9437b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File: /content/MistakeDetection/data/omnivore_test/1_7_360p.mp4_1s_1s.npz\n",
            "Chiavi presenti nel file: ['arr_0']\n",
            "\n",
            "Array 'arr_0' - shape: (604, 1024), dtype: float32\n",
            "[[ 0.6910985   0.09298898 -0.6608225  ... -0.75679165  1.2401273\n",
            "  -0.5683658 ]\n",
            " [ 0.40254688 -0.4466254  -0.8645446  ... -1.2709565   0.7917245\n",
            "  -0.5052321 ]\n",
            " [ 0.643613   -0.48683766 -0.88651866 ... -1.0358062   0.658605\n",
            "  -0.27201462]\n",
            " [ 0.8338395  -0.51338077 -0.8236387  ... -0.8753807   0.51246065\n",
            "  -0.5449421 ]\n",
            " [ 0.98503673 -0.4786031  -0.6167189  ... -1.0904019   0.94557023\n",
            "  -0.4631019 ]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def inspect_npz(npz_path, n_rows=5):\n",
        "    \"\"\"\n",
        "    Mostra il contenuto di un file .npz.\n",
        "\n",
        "    Args:\n",
        "        npz_path (str): percorso del file .npz\n",
        "        n_rows (int): numero di righe da stampare per ogni array\n",
        "    \"\"\"\n",
        "    if not os.path.exists(npz_path):\n",
        "        print(f\"[ERROR] File non trovato: {npz_path}\")\n",
        "        return\n",
        "\n",
        "    data = np.load(npz_path)\n",
        "    print(f\"File: {npz_path}\")\n",
        "    print(\"Chiavi presenti nel file:\", list(data.keys()))\n",
        "\n",
        "    for key in data.keys():\n",
        "        arr = data[key]\n",
        "        print(f\"\\nArray '{key}' - shape: {arr.shape}, dtype: {arr.dtype}\")\n",
        "        print(arr[:n_rows])  # stampa le prime n_rows righe\n",
        "\n",
        "\n",
        "# Esempio di utilizzo\n",
        "if ON_COLAB:\n",
        "    npz_file = \"/content/MistakeDetection/data/omnivore_test/1_7_360p.mp4_1s_1s.npz\"\n",
        "else:\n",
        "    npz_file = \"../data/omnivore_test/1_7_360p.mp4_1s_1s.npz\"\n",
        "\n",
        "# Esegui l'ispezione\n",
        "inspect_npz(npz_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SigNHU8ctaWJ",
      "metadata": {
        "id": "SigNHU8ctaWJ"
      },
      "source": [
        "# MLP (Version 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "O74QGf1Qo2sK",
      "metadata": {
        "id": "O74QGf1Qo2sK"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from models.MLP_version1 import MLP_version1\n",
        "\n",
        "model = MLP_version1(1024).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "6FdOuKJopr3m",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FdOuKJopr3m",
        "outputId": "b0ae8d3b-821b-4e6e-8403-5a198ba29e19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Peso classe positiva: 1.5\n"
          ]
        }
      ],
      "source": [
        "lr = 0.0001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
        "\n",
        "# Quanto pesa la classe â€œpositivaâ€ = classe \"1\" = classe \"error\":\n",
        "# - CASO 1: rapporto effettivo del dataset\n",
        "#train_pos_weight = train_cnt_0 / train_cnt_1\n",
        "\n",
        "# - CASO 2: rapporto usato nel paper\n",
        "train_pos_weight = 1.5\n",
        "\n",
        "print(f\"Peso classe positiva: {train_pos_weight}\")\n",
        "train_pos_weight = torch.tensor([train_pos_weight], device=device)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=train_pos_weight)\n",
        "\n",
        "epochs = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "4u_a6jPgq7OI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4u_a6jPgq7OI",
        "outputId": "1eff1238-adf3-4e05-ba13-0476c14542e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50 - Train Loss: 0.8084 - Val Loss: 0.7823 - Acc: 0.6923 - F1: 0.6303\n",
            "Epoch 2/50 - Train Loss: 0.7627 - Val Loss: 0.7444 - Acc: 0.7107 - F1: 0.6709\n",
            "Epoch 3/50 - Train Loss: 0.7255 - Val Loss: 0.7131 - Acc: 0.7214 - F1: 0.6758\n",
            "Epoch 4/50 - Train Loss: 0.6938 - Val Loss: 0.6868 - Acc: 0.7270 - F1: 0.6891\n",
            "Epoch 5/50 - Train Loss: 0.6696 - Val Loss: 0.6644 - Acc: 0.7383 - F1: 0.6945\n",
            "Epoch 6/50 - Train Loss: 0.6472 - Val Loss: 0.6464 - Acc: 0.7464 - F1: 0.7085\n",
            "Epoch 7/50 - Train Loss: 0.6293 - Val Loss: 0.6326 - Acc: 0.7490 - F1: 0.7099\n",
            "Epoch 8/50 - Train Loss: 0.6104 - Val Loss: 0.6189 - Acc: 0.7546 - F1: 0.7111\n",
            "Epoch 9/50 - Train Loss: 0.5965 - Val Loss: 0.6075 - Acc: 0.7561 - F1: 0.7134\n",
            "Epoch 10/50 - Train Loss: 0.5874 - Val Loss: 0.5972 - Acc: 0.7612 - F1: 0.7211\n",
            "Epoch 11/50 - Train Loss: 0.5759 - Val Loss: 0.5881 - Acc: 0.7663 - F1: 0.7280\n",
            "Epoch 12/50 - Train Loss: 0.5613 - Val Loss: 0.5796 - Acc: 0.7724 - F1: 0.7326\n",
            "Epoch 13/50 - Train Loss: 0.5506 - Val Loss: 0.5711 - Acc: 0.7755 - F1: 0.7365\n",
            "Epoch 14/50 - Train Loss: 0.5393 - Val Loss: 0.5628 - Acc: 0.7796 - F1: 0.7422\n",
            "Epoch 15/50 - Train Loss: 0.5326 - Val Loss: 0.5562 - Acc: 0.7791 - F1: 0.7399\n",
            "Epoch 16/50 - Train Loss: 0.5212 - Val Loss: 0.5485 - Acc: 0.7847 - F1: 0.7494\n",
            "Epoch 17/50 - Train Loss: 0.5136 - Val Loss: 0.5427 - Acc: 0.7857 - F1: 0.7467\n",
            "Epoch 18/50 - Train Loss: 0.5067 - Val Loss: 0.5353 - Acc: 0.7939 - F1: 0.7581\n",
            "Epoch 19/50 - Train Loss: 0.4983 - Val Loss: 0.5299 - Acc: 0.7939 - F1: 0.7589\n",
            "Epoch 20/50 - Train Loss: 0.4849 - Val Loss: 0.5238 - Acc: 0.7985 - F1: 0.7625\n",
            "Epoch 21/50 - Train Loss: 0.4822 - Val Loss: 0.5186 - Acc: 0.7985 - F1: 0.7628\n",
            "Epoch 22/50 - Train Loss: 0.4715 - Val Loss: 0.5127 - Acc: 0.7964 - F1: 0.7629\n",
            "Epoch 23/50 - Train Loss: 0.4677 - Val Loss: 0.5075 - Acc: 0.8031 - F1: 0.7652\n",
            "Epoch 24/50 - Train Loss: 0.4610 - Val Loss: 0.5022 - Acc: 0.8046 - F1: 0.7713\n",
            "Epoch 25/50 - Train Loss: 0.4518 - Val Loss: 0.4976 - Acc: 0.8026 - F1: 0.7673\n",
            "Epoch 26/50 - Train Loss: 0.4428 - Val Loss: 0.4939 - Acc: 0.8128 - F1: 0.7755\n",
            "Epoch 27/50 - Train Loss: 0.4426 - Val Loss: 0.4882 - Acc: 0.8097 - F1: 0.7778\n",
            "Epoch 28/50 - Train Loss: 0.4316 - Val Loss: 0.4844 - Acc: 0.8138 - F1: 0.7789\n",
            "Epoch 29/50 - Train Loss: 0.4230 - Val Loss: 0.4796 - Acc: 0.8133 - F1: 0.7790\n",
            "Epoch 30/50 - Train Loss: 0.4220 - Val Loss: 0.4752 - Acc: 0.8163 - F1: 0.7826\n",
            "Epoch 31/50 - Train Loss: 0.4122 - Val Loss: 0.4718 - Acc: 0.8194 - F1: 0.7857\n",
            "Epoch 32/50 - Train Loss: 0.4111 - Val Loss: 0.4674 - Acc: 0.8199 - F1: 0.7857\n",
            "Epoch 33/50 - Train Loss: 0.4009 - Val Loss: 0.4630 - Acc: 0.8184 - F1: 0.7855\n",
            "Epoch 34/50 - Train Loss: 0.3968 - Val Loss: 0.4606 - Acc: 0.8230 - F1: 0.7883\n",
            "Epoch 35/50 - Train Loss: 0.3934 - Val Loss: 0.4575 - Acc: 0.8240 - F1: 0.7892\n",
            "Epoch 36/50 - Train Loss: 0.3857 - Val Loss: 0.4541 - Acc: 0.8245 - F1: 0.7892\n",
            "Epoch 37/50 - Train Loss: 0.3832 - Val Loss: 0.4495 - Acc: 0.8291 - F1: 0.7981\n",
            "Epoch 38/50 - Train Loss: 0.3773 - Val Loss: 0.4476 - Acc: 0.8316 - F1: 0.7975\n",
            "Epoch 39/50 - Train Loss: 0.3705 - Val Loss: 0.4455 - Acc: 0.8332 - F1: 0.7988\n",
            "Epoch 40/50 - Train Loss: 0.3656 - Val Loss: 0.4413 - Acc: 0.8362 - F1: 0.8020\n",
            "Epoch 41/50 - Train Loss: 0.3627 - Val Loss: 0.4385 - Acc: 0.8357 - F1: 0.8029\n",
            "Epoch 42/50 - Train Loss: 0.3521 - Val Loss: 0.4353 - Acc: 0.8372 - F1: 0.8032\n",
            "Epoch 43/50 - Train Loss: 0.3553 - Val Loss: 0.4311 - Acc: 0.8423 - F1: 0.8098\n",
            "Epoch 44/50 - Train Loss: 0.3488 - Val Loss: 0.4291 - Acc: 0.8393 - F1: 0.8080\n",
            "Epoch 45/50 - Train Loss: 0.3443 - Val Loss: 0.4276 - Acc: 0.8459 - F1: 0.8129\n",
            "Epoch 46/50 - Train Loss: 0.3391 - Val Loss: 0.4242 - Acc: 0.8454 - F1: 0.8151\n",
            "Epoch 47/50 - Train Loss: 0.3389 - Val Loss: 0.4221 - Acc: 0.8454 - F1: 0.8145\n",
            "Epoch 48/50 - Train Loss: 0.3315 - Val Loss: 0.4195 - Acc: 0.8464 - F1: 0.8155\n",
            "Epoch 49/50 - Train Loss: 0.3254 - Val Loss: 0.4182 - Acc: 0.8464 - F1: 0.8127\n",
            "Epoch 50/50 - Train Loss: 0.3244 - Val Loss: 0.4169 - Acc: 0.8459 - F1: 0.8113\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    # -------------------------\n",
        "    #        TRAIN\n",
        "    # -------------------------\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device).float()\n",
        "\n",
        "        outputs = model(inputs)            # [B, 1]\n",
        "        outputs = outputs.squeeze(1)       # [B]\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "    # -------------------------\n",
        "    #        EVAL\n",
        "    # -------------------------\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device).float()\n",
        "\n",
        "            outputs = model(inputs).squeeze(1)  # logits\n",
        "\n",
        "            # same loss as train\n",
        "            val_loss = criterion(outputs, labels)\n",
        "            total_val_loss += val_loss.item()\n",
        "\n",
        "            # convert logits â†’ probabilities â†’ binary predictions\n",
        "            probs = torch.sigmoid(outputs)\n",
        "            preds = (probs >= 0.5).long()\n",
        "\n",
        "            all_preds.append(preds.cpu())\n",
        "            all_targets.append(labels.cpu())\n",
        "\n",
        "    # concat\n",
        "    all_preds = torch.cat(all_preds).numpy()\n",
        "    all_targets = torch.cat(all_targets).numpy()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(test_loader)\n",
        "    acc = accuracy_score(all_targets, all_preds)\n",
        "    f1  = f1_score(all_targets, all_preds, zero_division=0)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} \"\n",
        "          f\"- Train Loss: {avg_train_loss:.4f} \"\n",
        "          f\"- Val Loss: {avg_val_loss:.4f} \"\n",
        "          f\"- Acc: {acc:.4f} \"\n",
        "          f\"- F1: {f1:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Kernel Python LLM",
      "language": "python",
      "name": "kernel_llm"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}