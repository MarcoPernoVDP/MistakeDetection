{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a841a958",
      "metadata": {
        "id": "a841a958"
      },
      "source": [
        "# Environement Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cee1a5b4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cee1a5b4",
        "outputId": "041a6794-58bd-42e6-a961-3c1f18d26193"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ambiente locale rilevato.\n"
          ]
        }
      ],
      "source": [
        "import sys, os\n",
        "try:\n",
        "    from google.colab import drive, userdata\n",
        "    IS_COLAB = True\n",
        "except ImportError:\n",
        "    IS_COLAB = False\n",
        "\n",
        "REPO_NAME = 'MistakeDetection'\n",
        "\n",
        "if IS_COLAB:\n",
        "    print(\"☁️ Colab rilevato.\")\n",
        "    if not os.path.exists('/content/drive'): drive.mount('/content/drive')\n",
        "\n",
        "    GITHUB_USER = 'MarcoPernoVDP'\n",
        "    try:\n",
        "        TOKEN = userdata.get('GITHUB_TOKEN')\n",
        "        REPO_URL = f'https://{TOKEN}@github.com/{GITHUB_USER}/{REPO_NAME}.git'\n",
        "    except:\n",
        "        REPO_URL = f'https://github.com/{GITHUB_USER}/{REPO_NAME}.git'\n",
        "\n",
        "    ROOT_DIR = f'/content/{REPO_NAME}'\n",
        "    if not os.path.exists(ROOT_DIR):\n",
        "        !git clone {REPO_URL}\n",
        "    else:\n",
        "        %cd {ROOT_DIR}\n",
        "        !git pull\n",
        "        %cd /content\n",
        "\n",
        "    \n",
        "else:\n",
        "    print(\"Ambiente locale rilevato.\")\n",
        "    ROOT_DIR = os.getcwd()\n",
        "    while not os.path.exists(os.path.join(ROOT_DIR, '.gitignore')) and ROOT_DIR != os.path.dirname(ROOT_DIR):\n",
        "        ROOT_DIR = os.path.dirname(ROOT_DIR)\n",
        "\n",
        "if ROOT_DIR not in sys.path:\n",
        "    sys.path.append(ROOT_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UtEsNOoRtWa2",
      "metadata": {
        "id": "UtEsNOoRtWa2"
      },
      "source": [
        "# Dataset Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "73e181dd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73e181dd",
        "outputId": "1937dac1-fa6e-4918-cf85-4ffbb4dfa50f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setup Progetto in: c:\\Users\\marco\\Desktop\\Marco\\Programmazione\\C\\EsPoli\\Advanced Machine Learning\\MistakeDetection\n",
            "Setup Dati da: c:\\Users\\marco\\Desktop\\Marco\\Programmazione\\C\\EsPoli\\Advanced Machine Learning\\MistakeDetection\\_file\n",
            "Inizio setup dati...\n",
            "   Sorgente: c:\\Users\\marco\\Desktop\\Marco\\Programmazione\\C\\EsPoli\\Advanced Machine Learning\\MistakeDetection\\_file\n",
            "   Destinazione: c:\\Users\\marco\\Desktop\\Marco\\Programmazione\\C\\EsPoli\\Advanced Machine Learning\\MistakeDetection\\data\n",
            "Copia cartella: annotation_json...\n",
            "Copia cartella: omnivore...\n",
            "✅ Setup completato! Dati pronti in: c:\\Users\\marco\\Desktop\\Marco\\Programmazione\\C\\EsPoli\\Advanced Machine Learning\\MistakeDetection\\data\n",
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "from utils import setup_project\n",
        "\n",
        "# Esegue: Setup Dati (unzip/copy), Login WandB, Setup Device\n",
        "device = setup_project.initialize(ROOT_DIR)\n",
        "\n",
        "# Ora puoi passare agli import del modello\n",
        "from dataset.capitain_cook_4d_dataset import CaptainCook4D_Dataset\n",
        "from models.BaselineV1_MLP import BaselineV1_MLP"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c480212",
      "metadata": {
        "id": "1c480212"
      },
      "source": [
        "# Dataset Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75a3a690",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75a3a690",
        "outputId": "6f0aaa0b-e95b-484b-acb0-a6dfecc065e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading from: c:\\Users\\marco\\Desktop\\Marco\\Programmazione\\C\\EsPoli\\Advanced Machine Learning\\MistakeDetection\\data\\omnivore...\n",
            "\n",
            "=================================================================\n",
            "DATASET INFO\n",
            "   Shape: torch.Size([9798, 1024]) -> 9798 Campioni, 1024 Features\n",
            "=================================================================\n",
            "FULL DATASET       | Tot: 9798   | OK: 5970  (60.9%) | ERR: 3828  (39.1%) | Ratio: 1:1.6\n",
            "-----------------------------------------------------------------\n",
            "TRAIN SET          | Tot: 6860   | OK: 4173  (60.8%) | ERR: 2687  (39.2%) | Ratio: 1:1.6\n",
            "VALIDATION SET     | Tot: 979    | OK: 603   (61.6%) | ERR: 376   (38.4%) | Ratio: 1:1.6\n",
            "TEST SET           | Tot: 1959   | OK: 1194  (60.9%) | ERR: 765   (39.1%) | Ratio: 1:1.6\n",
            "=================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dataset.capitain_cook_4d_dataset import CaptainCook4D_Dataset, DatasetSource\n",
        "from dataset.utils import get_loaders\n",
        "\n",
        "try:\n",
        "    full_dataset = CaptainCook4D_Dataset(dataset_source=DatasetSource.OMNIVORE, root_dir=ROOT_DIR)\n",
        "    train_loader, val_loader, test_loader = get_loaders(\n",
        "        full_dataset,\n",
        "        batch_size=512,\n",
        "        seed=42\n",
        "    )\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Errore: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "6a5eed01",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a5eed01",
        "outputId": "c4297822-2218-47f6-c8de-ea743748dce0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File: c:\\Users\\marco\\Desktop\\Marco\\Programmazione\\C\\EsPoli\\Advanced Machine Learning\\MistakeDetection\\data\\omnivore_test\\1_7_360p.mp4_1s_1s.npz\n",
            "Chiavi presenti nel file: ['arr_0']\n",
            "\n",
            "Array 'arr_0' - shape: (604, 1024), dtype: float32\n",
            "[[ 0.6910985   0.09298898 -0.6608225  ... -0.75679165  1.2401273\n",
            "  -0.5683658 ]\n",
            " [ 0.40254688 -0.4466254  -0.8645446  ... -1.2709565   0.7917245\n",
            "  -0.5052321 ]\n",
            " [ 0.643613   -0.48683766 -0.88651866 ... -1.0358062   0.658605\n",
            "  -0.27201462]\n",
            " [ 0.8338395  -0.51338077 -0.8236387  ... -0.8753807   0.51246065\n",
            "  -0.5449421 ]\n",
            " [ 0.98503673 -0.4786031  -0.6167189  ... -1.0904019   0.94557023\n",
            "  -0.4631019 ]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def inspect_npz(npz_path, n_rows=5):\n",
        "    \"\"\"\n",
        "    Mostra il contenuto di un file .npz.\n",
        "\n",
        "    Args:\n",
        "        npz_path (str): percorso del file .npz\n",
        "        n_rows (int): numero di righe da stampare per ogni array\n",
        "    \"\"\"\n",
        "    if not os.path.exists(npz_path):\n",
        "        print(f\"[ERROR] File non trovato: {npz_path}\")\n",
        "        return\n",
        "\n",
        "    data = np.load(npz_path)\n",
        "    print(f\"File: {npz_path}\")\n",
        "    print(\"Chiavi presenti nel file:\", list(data.keys()))\n",
        "\n",
        "    for key in data.keys():\n",
        "        arr = data[key]\n",
        "        print(f\"\\nArray '{key}' - shape: {arr.shape}, dtype: {arr.dtype}\")\n",
        "        print(arr[:n_rows])  # stampa le prime n_rows righe\n",
        "\n",
        "\n",
        "# Esempio di utilizzo\n",
        "npz_file = os.path.join(ROOT_DIR, \"data\", \"omnivore_test\", \"1_7_360p.mp4_1s_1s.npz\")\n",
        "\n",
        "# Esegui l'ispezione (se il file esiste)\n",
        "if os.path.exists(npz_file):\n",
        "    inspect_npz(npz_file)\n",
        "else:\n",
        "    print(f\"⚠️ File di esempio non trovato in: {npz_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SigNHU8ctaWJ",
      "metadata": {
        "id": "SigNHU8ctaWJ"
      },
      "source": [
        "# MLP (Version 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "O74QGf1Qo2sK",
      "metadata": {
        "id": "O74QGf1Qo2sK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "model = BaselineV1_MLP(1024).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6FdOuKJopr3m",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FdOuKJopr3m",
        "outputId": "aea62f20-5747-4ab8-fce4-a6e97a86806f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peso classe positiva: 1.5\n"
          ]
        }
      ],
      "source": [
        "lr = 0.0001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
        "\n",
        "# Quanto pesa la classe “positiva” = classe \"1\" = classe \"error\":\n",
        "# - CASO 1: rapporto effettivo del dataset\n",
        "#train_pos_weight = train_cnt_0 / train_cnt_1\n",
        "\n",
        "# - CASO 2: rapporto usato nel paper\n",
        "train_pos_weight = 1.5\n",
        "\n",
        "print(f\"Peso classe positiva: {train_pos_weight}\")\n",
        "train_pos_weight = torch.tensor([train_pos_weight], device=device)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=train_pos_weight)\n",
        "\n",
        "epochs = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4u_a6jPgq7OI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "4u_a6jPgq7OI",
        "outputId": "8054f45a-ecef-42de-9892-c0ea5c89acb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50 - Train Loss: 0.8171 - Val Loss: 0.7903 - Acc: 0.6799 - F1: 0.5952\n",
            "Epoch 2/50 - Train Loss: 0.7708 - Val Loss: 0.7505 - Acc: 0.7101 - F1: 0.6405\n",
            "Epoch 3/50 - Train Loss: 0.7312 - Val Loss: 0.7184 - Acc: 0.7162 - F1: 0.6568\n",
            "Epoch 4/50 - Train Loss: 0.6997 - Val Loss: 0.6920 - Acc: 0.7223 - F1: 0.6683\n",
            "Epoch 5/50 - Train Loss: 0.6761 - Val Loss: 0.6699 - Acc: 0.7305 - F1: 0.6757\n",
            "Epoch 6/50 - Train Loss: 0.6524 - Val Loss: 0.6522 - Acc: 0.7427 - F1: 0.6889\n",
            "Epoch 7/50 - Train Loss: 0.6345 - Val Loss: 0.6377 - Acc: 0.7458 - F1: 0.6933\n",
            "Epoch 8/50 - Train Loss: 0.6243 - Val Loss: 0.6253 - Acc: 0.7524 - F1: 0.7030\n",
            "Epoch 9/50 - Train Loss: 0.6018 - Val Loss: 0.6138 - Acc: 0.7616 - F1: 0.7158\n",
            "Epoch 10/50 - Train Loss: 0.5861 - Val Loss: 0.6035 - Acc: 0.7642 - F1: 0.7200\n",
            "Epoch 11/50 - Train Loss: 0.5766 - Val Loss: 0.5943 - Acc: 0.7703 - F1: 0.7212\n",
            "Epoch 12/50 - Train Loss: 0.5648 - Val Loss: 0.5858 - Acc: 0.7718 - F1: 0.7299\n",
            "Epoch 13/50 - Train Loss: 0.5527 - Val Loss: 0.5776 - Acc: 0.7774 - F1: 0.7338\n",
            "Epoch 14/50 - Train Loss: 0.5426 - Val Loss: 0.5699 - Acc: 0.7779 - F1: 0.7293\n",
            "Epoch 15/50 - Train Loss: 0.5339 - Val Loss: 0.5627 - Acc: 0.7836 - F1: 0.7405\n",
            "Epoch 16/50 - Train Loss: 0.5223 - Val Loss: 0.5567 - Acc: 0.7851 - F1: 0.7387\n",
            "Epoch 17/50 - Train Loss: 0.5183 - Val Loss: 0.5501 - Acc: 0.7871 - F1: 0.7382\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m model.train()\n\u001b[32m      9\u001b[39m total_loss = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marco\\Desktop\\Marco\\Programmazione\\C\\EsPoli\\Advanced Machine Learning\\MistakeDetection\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marco\\Desktop\\Marco\\Programmazione\\C\\EsPoli\\Advanced Machine Learning\\MistakeDetection\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:790\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    788\u001b[39m data = \u001b[38;5;28mself\u001b[39m._dataset_fetcher.fetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     data = \u001b[43m_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpin_memory\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpin_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pin_memory_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marco\\Desktop\\Marco\\Programmazione\\C\\EsPoli\\Advanced Machine Learning\\MistakeDetection\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py:55\u001b[39m, in \u001b[36mpin_memory\u001b[39m\u001b[34m(data, device)\u001b[39m\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done_event.is_set():\n\u001b[32m     50\u001b[39m         \u001b[38;5;66;03m# Make sure that we don't preserve any object from one iteration\u001b[39;00m\n\u001b[32m     51\u001b[39m         \u001b[38;5;66;03m# to the next\u001b[39;00m\n\u001b[32m     52\u001b[39m         do_one_step()\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpin_memory\u001b[39m(data, device=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, torch.Tensor):\n\u001b[32m     57\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m data.pin_memory(device)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    # -------------------------\n",
        "    #        TRAIN\n",
        "    # -------------------------\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device).float()\n",
        "\n",
        "        outputs = model(inputs)            # [B, 1]\n",
        "        outputs = outputs.squeeze(1)       # [B]\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "    # -------------------------\n",
        "    #        EVAL\n",
        "    # -------------------------\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device).float()\n",
        "\n",
        "            outputs = model(inputs).squeeze(1)  # logits\n",
        "\n",
        "            # same loss as train\n",
        "            val_loss = criterion(outputs, labels)\n",
        "            total_val_loss += val_loss.item()\n",
        "\n",
        "            # convert logits → probabilities → binary predictions\n",
        "            probs = torch.sigmoid(outputs)\n",
        "            preds = (probs >= 0.5).long()\n",
        "\n",
        "            all_preds.append(preds.cpu())\n",
        "            all_targets.append(labels.cpu())\n",
        "\n",
        "    # concat\n",
        "    all_preds = torch.cat(all_preds).numpy()\n",
        "    all_targets = torch.cat(all_targets).numpy()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(test_loader)\n",
        "    acc = accuracy_score(all_targets, all_preds)\n",
        "    f1  = f1_score(all_targets, all_preds, zero_division=0)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} \"\n",
        "          f\"- Train Loss: {avg_train_loss:.4f} \"\n",
        "          f\"- Val Loss: {avg_val_loss:.4f} \"\n",
        "          f\"- Acc: {acc:.4f} \"\n",
        "          f\"- F1: {f1:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv (3.12.6)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
