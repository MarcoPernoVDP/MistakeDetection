{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a841a958",
      "metadata": {},
      "source": [
        "# Environement Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cee1a5b4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cee1a5b4",
        "outputId": "4105683e-56dc-4570-acf8-2d84b10a1fe2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "ðŸ”„ Clonazione del repository MistakeDetection...\n",
            "Cloning into 'MistakeDetection'...\n",
            "remote: Enumerating objects: 61, done.\u001b[K\n",
            "remote: Counting objects: 100% (61/61), done.\u001b[K\n",
            "remote: Compressing objects: 100% (41/41), done.\u001b[K\n",
            "remote: Total 61 (delta 16), reused 37 (delta 6), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (61/61), 21.32 KiB | 5.33 MiB/s, done.\n",
            "Resolving deltas: 100% (16/16), done.\n",
            "âœ… Aggiunto /content/MistakeDetection al path di sistema.\n",
            "\n",
            "SETUP COMPLETATO\n"
          ]
        }
      ],
      "source": [
        "# ATTENZIONE: eseguire solo su Colab!\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# --- 1. SETUP AMBIENTE E DRIVE ---\n",
        "\n",
        "from google.colab import drive\n",
        "from google.colab import userdata\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# --- 2. CONFIGURAZIONE GITHUB (PRIVATO) ---\n",
        "# Sostituisci QUESTE variabili con i tuoi dati reali\n",
        "GITHUB_USER = 'MarcoPernoVDP'\n",
        "REPO_NAME = 'MistakeDetection'\n",
        "BRANCH = 'main'\n",
        "\n",
        "# Recupera il Token dai Segreti di Colab (\"GITHUB_TOKEN\")\n",
        "try:\n",
        "    GIT_TOKEN = userdata.get('GITHUB_TOKEN')\n",
        "except ImportError:\n",
        "    # Se non usi i Segreti, incolla il token qui (SCONSIGLIATO ma funziona)\n",
        "    GIT_TOKEN = \"INCOLLA_QUI_IL_TUO_PAT_SE_NON_USI_SEGRETI\"\n",
        "except Exception:\n",
        "    print(\"âš ï¸ ERRORE: Non ho trovato il segreto 'GITHUB_TOKEN'.\")\n",
        "    print(\"Vai sulla chiave inglese a sinistra -> Aggiungi nuovo segreto -> Nome: GITHUB_TOKEN\")\n",
        "    raise\n",
        "\n",
        "REPO_URL = f'https://{GIT_TOKEN}@github.com/{GITHUB_USER}/{REPO_NAME}.git'\n",
        "ROOT_DIR = f'/content/{REPO_NAME}'\n",
        "\n",
        "# --- 3. CLONE O AGGIORNAMENTO CODICE ---\n",
        "if not os.path.exists(ROOT_DIR):\n",
        "    print(f\"ðŸ”„ Clonazione del repository {REPO_NAME}...\")\n",
        "    !git clone {REPO_URL}\n",
        "else:\n",
        "    print(f\"ðŸ”„ Aggiornamento repository (git pull)...\")\n",
        "    %cd {ROOT_DIR}\n",
        "    !git pull origin {BRANCH}\n",
        "    %cd /content\n",
        "\n",
        "# --- 4. CONFIGURAZIONE PATH PYTHON ---\n",
        "if ROOT_DIR not in sys.path:\n",
        "    sys.path.append(ROOT_DIR)\n",
        "    print(f\"âœ… Aggiunto {ROOT_DIR} al path di sistema.\")\n",
        "\n",
        "# --- 5. INSTALLAZIONE DIPENDENZE ---\n",
        "\n",
        "#\n",
        "\n",
        "print(\"\\nSETUP COMPLETATO\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "kGMl6Pu2rSYk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGMl6Pu2rSYk",
        "outputId": "a3e952d2-2c16-48ab-ea10-fdb661a42c62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device in uso: cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device in uso:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UtEsNOoRtWa2",
      "metadata": {
        "id": "UtEsNOoRtWa2"
      },
      "source": [
        "# Dataset Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "EfekzBNAtZ_S",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfekzBNAtZ_S",
        "outputId": "4c2e14be-f564-442c-8f9d-8415c0ff630d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cartella 'annotation_json' copiata in '/content/MistakeDetection/data/'.\n",
            "File ZIP '/content/drive/MyDrive/AML_MistakeDetection_DATA/omnivore.zip' estratto in '/content/MistakeDetection/data/'.\n",
            "File ZIP '/content/drive/MyDrive/AML_MistakeDetection_DATA/omnivore_test.zip' estratto in '/content/MistakeDetection/data/'.\n",
            "Dati estratti e organizzati in: /content/MistakeDetection/data\n"
          ]
        }
      ],
      "source": [
        "# ATTENZIONE: usare questa cella solo su Colab!\n",
        "\n",
        "# COSA FA LA CELLA:\n",
        "# crea la cartella \"/content/MistakeDetection/data\" e salva al suo interno i dati presenti\n",
        "# in Drive al percorso \"/content/drive/MyDrive/AML_MistakeDetection_DATA\" seguenti:\n",
        "#   - la cartella annotation_json (con tutti i file json al suo interno);\n",
        "#   - i dataset zip (come omnivore.zip) estratti.\n",
        "\n",
        "# Questa cella prepara dunque i dataset e gli annotation per poter usare i dati su Colab\n",
        "\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Root del progetto (notebook in Notebooks/)\n",
        "# Use the ROOT_DIR established during repository cloning\n",
        "project_root = \"/content/MistakeDetection\" # Corrected project root\n",
        "if project_root not in sys.path:\n",
        "    sys.path.append(project_root)\n",
        "\n",
        "# Importa la funzione\n",
        "from utils.setup_dataset import setup_dataset\n",
        "\n",
        "# Path della cartella _file\n",
        "#data_source_path = os.path.join(project_root, \"_file\")\n",
        "data_source_path = os.path.join(\"/content\", \"drive\", \"MyDrive\", \"AML_MistakeDetection_DATA\")\n",
        "\n",
        "\n",
        "# Chiama la funzione\n",
        "setup_dataset(data_source_path, project_root)\n",
        "\n",
        "print(f\"Dati estratti e organizzati in: {os.path.join(project_root, 'data')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caea429b",
      "metadata": {
        "id": "caea429b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cartella 'annotation_json' copiata in 'c:\\Users\\dylan\\Desktop\\UniversitÃ \\5Â° ANNO\\AML\\MistakeDetection\\data/'.\n",
            "File ZIP 'c:\\Users\\dylan\\Desktop\\UniversitÃ \\5Â° ANNO\\AML\\MistakeDetection\\_file\\omnivore_test.zip' estratto in 'c:\\Users\\dylan\\Desktop\\UniversitÃ \\5Â° ANNO\\AML\\MistakeDetection\\data/'.\n",
            "Dati estratti e organizzati in: c:\\Users\\dylan\\Desktop\\UniversitÃ \\5Â° ANNO\\AML\\MistakeDetection\\data\n"
          ]
        }
      ],
      "source": [
        "# ATTENZIONE: usare questa cella solo su VScode!\n",
        "\n",
        "# NB: per testare il codice su VScode usando questa cella, bisogna prima aver creato nel progetto la cartella \"_file/\"\n",
        "# contenente gli stessi file presenti in Drive (cartella annotation_json e file zip dei dataset, come omnivore.zip o la sua versione\n",
        "# con pochi campioni, chiamata omnivore_test.zip, che ha 5 file .npz al suo interno)\n",
        "\n",
        "# COSA FA LA CELLA:\n",
        "# crea la cartella \"/MistakeDetection/data\" e salva al suo interno i dati presenti\n",
        "# nella cartella \"MistakeDetection/_file\" seguenti:\n",
        "#   - la cartella annotation_json (con tutti i file json al suo interno);\n",
        "#   - i dataset zip (come omnivore.zip) estratti.\n",
        "\n",
        "# Questa cella prepara dunque i dataset e gli annotation per poter usare i dati su VScode in fase di test su pochi dati\n",
        "\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Root del progetto\n",
        "project_root = os.path.abspath(\"..\") # serve per far si che Python veda il percorso della cartella padre \"MistakeDetection\" per poter importare i moduli delle altre cartelle\n",
        "if project_root not in sys.path:\n",
        "    sys.path.append(project_root)\n",
        "\n",
        "# Importa la funzione\n",
        "from utils.setup_dataset import setup_dataset\n",
        "\n",
        "# Path della cartella _file\n",
        "data_source_path = os.path.join(project_root, \"_file\")\n",
        "\n",
        "\n",
        "# Chiama la funzione\n",
        "setup_dataset(data_source_path, project_root)\n",
        "\n",
        "print(f\"Dati estratti e organizzati in: {os.path.join(project_root, 'data')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "G7sq4nHfy-AZ",
      "metadata": {
        "id": "G7sq4nHfy-AZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Root del progetto\n",
        "project_root = os.path.abspath(\"..\") # serve per far si che Python veda il percorso della cartella padre \"MistakeDetection\" per poter importare i moduli delle altre cartelle\n",
        "if project_root not in sys.path:\n",
        "    sys.path.append(project_root)\n",
        "\n",
        "from dataset.loader import CaptainCook4D_Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "75a3a690",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train samples: 2214\n",
            "Validation samples: 317\n",
            "Test samples: 633\n"
          ]
        }
      ],
      "source": [
        "# --- Percorsi dati ---\n",
        "features_dir = os.path.join(\"..\", \"data\", \"omnivore_test\")          # cartella npz\n",
        "annotations_dir = os.path.join(\"..\", \"data\", \"annotation_json\")  # cartella JSON\n",
        "\n",
        "# --- Inizializza il dataset completo ---\n",
        "full_dataset = CaptainCook4D_Dataset(features_dir, annotations_dir)\n",
        "\n",
        "# --- Indici di train, val e test ---\n",
        "indices = list(range(len(full_dataset)))\n",
        "\n",
        "# 20% test\n",
        "trainval_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42, shuffle=True)\n",
        "# 10% validation sul rimanente\n",
        "val_relative_size = 0.1 / 0.8\n",
        "train_idx, val_idx = train_test_split(trainval_idx, test_size=val_relative_size, random_state=42, shuffle=True)\n",
        "\n",
        "# --- Creazione dei Subset ---\n",
        "train_dataset = Subset(full_dataset, train_idx)\n",
        "val_dataset   = Subset(full_dataset, val_idx)\n",
        "test_dataset  = Subset(full_dataset, test_idx)\n",
        "\n",
        "# --- Creazione DataLoader ---\n",
        "batch_size = 512\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# --- Controllo dimensioni ---\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(val_dataset)}\")\n",
        "print(f\"Test samples: {len(test_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8N7MZHSuh4N",
      "metadata": {
        "id": "e8N7MZHSuh4N"
      },
      "outputs": [],
      "source": [
        "class VideoFeatureDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.from_numpy(X).float()\n",
        "        self.y = torch.from_numpy(y).long()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TAUlHVSAykFa",
      "metadata": {
        "id": "TAUlHVSAykFa"
      },
      "outputs": [],
      "source": [
        "def get_labels_for_npz(npz_file, annotations):\n",
        "    # es: \"10_3_360.mp4_1s_1s.npz\"\n",
        "    base = os.path.basename(npz_file)\n",
        "    activity, attempt = base.split(\"_\")[:2]  # \"10\", \"3\"\n",
        "    recording_id = f\"{activity}_{attempt}\"\n",
        "\n",
        "    # carica feature\n",
        "    data = np.load(npz_file)\n",
        "    arr = data[list(data.keys())[0]]  # shape (N, 1024)\n",
        "    N = arr.shape[0]\n",
        "\n",
        "    labels = np.zeros(N, dtype=np.int64)  # default: normal = no-error = 0\n",
        "\n",
        "    # trova annotation di questo recording\n",
        "    info = annotations[recording_id]\n",
        "    steps = info[\"steps\"]\n",
        "\n",
        "    # assegnazione label per ogni secondo\n",
        "    for step in steps:\n",
        "        has_error = int(step[\"has_errors\"])  # Trueâ†’1, Falseâ†’0\n",
        "        start = step[\"start_time\"]\n",
        "        end   = step[\"end_time\"]\n",
        "\n",
        "        if start == -1 or end == -1 or has_error == 0:\n",
        "            continue\n",
        "\n",
        "        for sec in range(int(start), int(end) + 1, 1):\n",
        "            sec_start = sec\n",
        "            sec_end   = sec + 1\n",
        "\n",
        "            # check overlap\n",
        "            if sec_start >= start and sec_end <= end: # i secondi ai bordi avranno sempre il valore di default (norml = no-error = 0)\n",
        "                labels[sec] = has_error\n",
        "\n",
        "    return arr, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w6VeoKhMuHuZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6VeoKhMuHuZ",
        "outputId": "0e6bff9c-7933-4fa4-d77a-e2d3972eecc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(340320, 1024) (340320,)\n"
          ]
        }
      ],
      "source": [
        "all_X = []\n",
        "all_y = []\n",
        "\n",
        "extract_dir = \"/content/omnivore_extracted/omnivore\"\n",
        "\n",
        "for f in sorted(os.listdir(extract_dir)):\n",
        "    if f.endswith(\".npz\"):\n",
        "        X, y = get_labels_for_npz(os.path.join(extract_dir, f), annotations)\n",
        "        all_X.append(X)\n",
        "        all_y.append(y)\n",
        "\n",
        "X = np.concatenate(all_X, axis=0)\n",
        "y = np.concatenate(all_y, axis=0)\n",
        "\n",
        "print(X.shape, y.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DZDtlZE8up0H",
      "metadata": {
        "id": "DZDtlZE8up0H"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# \"Step (S)\" data splits (come usato nel paper CaptainCook4D)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "# implementare anche \"Recording (R)\" data splits (come fatto nel paper) per vederne differenze"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WprZqP5g-zGI",
      "metadata": {
        "id": "WprZqP5g-zGI"
      },
      "outputs": [],
      "source": [
        "train_dataset = VideoFeatureDataset(X_train, y_train)\n",
        "test_dataset  = VideoFeatureDataset(X_test,  y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FQ-eX8oeuwVg",
      "metadata": {
        "id": "FQ-eX8oeuwVg"
      },
      "outputs": [],
      "source": [
        "batch_size = 512\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SigNHU8ctaWJ",
      "metadata": {
        "id": "SigNHU8ctaWJ"
      },
      "source": [
        "# MLP (Version 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "O74QGf1Qo2sK",
      "metadata": {
        "id": "O74QGf1Qo2sK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MLPCapitainCook(nn.Module):\n",
        "    def __init__(self, in_features: int, p: float = 0.5) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features, 256)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p)       # Dropout layer con probabilitÃ  p\n",
        "        self.fc2 = nn.Linear(256, 1)       # Output logit (senza sigmoid)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)                # Applica Dropout solo in TRAIN\n",
        "        x = self.fc2(x)                    # Output logit\n",
        "        return x                           # no Sigmoid qui\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UWoFzaKgp-I_",
      "metadata": {
        "id": "UWoFzaKgp-I_"
      },
      "outputs": [],
      "source": [
        "model = MLPCapitainCook(1024).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6FdOuKJopr3m",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FdOuKJopr3m",
        "outputId": "f888829f-fbfc-47d3-b01f-88e429b1457f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.5\n"
          ]
        }
      ],
      "source": [
        "lr = 0.0001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
        "# count classi\n",
        "num_error = (y == 1).sum()   # classe â€œ1â€ = \"error\"\n",
        "num_normal = (y == 0).sum()   # classe â€œ0â€ = \"normal\"\n",
        "\n",
        "# pos_weight = quanto pesa la classe â€œpositivaâ€ = classe \"1\"\n",
        "#pos_weight_value = num_normal/num_error\n",
        "pos_weight_value = 1.5\n",
        "print(pos_weight_value)\n",
        "pos_weight = torch.tensor([pos_weight_value], device=device)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "epochs = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4u_a6jPgq7OI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4u_a6jPgq7OI",
        "outputId": "b0b8b41c-0c65-4729-92b1-8f51632af365"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50 - Train Loss: 0.6710 - Val Loss: 0.6475 - Acc: 0.7680 - F1: 0.2537\n",
            "Epoch 2/50 - Train Loss: 0.6327 - Val Loss: 0.6185 - Acc: 0.7798 - F1: 0.3424\n",
            "Epoch 3/50 - Train Loss: 0.6049 - Val Loss: 0.5917 - Acc: 0.7903 - F1: 0.4095\n",
            "Epoch 4/50 - Train Loss: 0.5822 - Val Loss: 0.5715 - Acc: 0.7972 - F1: 0.4712\n",
            "Epoch 5/50 - Train Loss: 0.5638 - Val Loss: 0.5534 - Acc: 0.8049 - F1: 0.5094\n",
            "Epoch 6/50 - Train Loss: 0.5471 - Val Loss: 0.5384 - Acc: 0.8108 - F1: 0.5248\n",
            "Epoch 7/50 - Train Loss: 0.5331 - Val Loss: 0.5260 - Acc: 0.8173 - F1: 0.5480\n",
            "Epoch 8/50 - Train Loss: 0.5204 - Val Loss: 0.5156 - Acc: 0.8179 - F1: 0.5578\n",
            "Epoch 9/50 - Train Loss: 0.5097 - Val Loss: 0.5056 - Acc: 0.8226 - F1: 0.5754\n",
            "Epoch 10/50 - Train Loss: 0.4993 - Val Loss: 0.4971 - Acc: 0.8278 - F1: 0.5817\n",
            "Epoch 11/50 - Train Loss: 0.4901 - Val Loss: 0.4878 - Acc: 0.8301 - F1: 0.5992\n",
            "Epoch 12/50 - Train Loss: 0.4821 - Val Loss: 0.4823 - Acc: 0.8329 - F1: 0.6030\n",
            "Epoch 13/50 - Train Loss: 0.4750 - Val Loss: 0.4744 - Acc: 0.8342 - F1: 0.6142\n",
            "Epoch 14/50 - Train Loss: 0.4674 - Val Loss: 0.4697 - Acc: 0.8350 - F1: 0.6147\n",
            "Epoch 15/50 - Train Loss: 0.4600 - Val Loss: 0.4637 - Acc: 0.8386 - F1: 0.6189\n",
            "Epoch 16/50 - Train Loss: 0.4536 - Val Loss: 0.4585 - Acc: 0.8412 - F1: 0.6237\n",
            "Epoch 17/50 - Train Loss: 0.4479 - Val Loss: 0.4535 - Acc: 0.8412 - F1: 0.6372\n",
            "Epoch 18/50 - Train Loss: 0.4408 - Val Loss: 0.4477 - Acc: 0.8445 - F1: 0.6416\n",
            "Epoch 19/50 - Train Loss: 0.4358 - Val Loss: 0.4443 - Acc: 0.8460 - F1: 0.6380\n",
            "Epoch 20/50 - Train Loss: 0.4312 - Val Loss: 0.4399 - Acc: 0.8471 - F1: 0.6496\n",
            "Epoch 21/50 - Train Loss: 0.4267 - Val Loss: 0.4362 - Acc: 0.8500 - F1: 0.6544\n",
            "Epoch 22/50 - Train Loss: 0.4203 - Val Loss: 0.4347 - Acc: 0.8507 - F1: 0.6463\n",
            "Epoch 23/50 - Train Loss: 0.4167 - Val Loss: 0.4311 - Acc: 0.8521 - F1: 0.6507\n",
            "Epoch 24/50 - Train Loss: 0.4114 - Val Loss: 0.4273 - Acc: 0.8533 - F1: 0.6537\n",
            "Epoch 25/50 - Train Loss: 0.4076 - Val Loss: 0.4224 - Acc: 0.8531 - F1: 0.6648\n",
            "Epoch 26/50 - Train Loss: 0.4037 - Val Loss: 0.4208 - Acc: 0.8559 - F1: 0.6619\n",
            "Epoch 27/50 - Train Loss: 0.3991 - Val Loss: 0.4178 - Acc: 0.8558 - F1: 0.6671\n",
            "Epoch 28/50 - Train Loss: 0.3955 - Val Loss: 0.4145 - Acc: 0.8571 - F1: 0.6726\n",
            "Epoch 29/50 - Train Loss: 0.3921 - Val Loss: 0.4112 - Acc: 0.8576 - F1: 0.6765\n",
            "Epoch 30/50 - Train Loss: 0.3876 - Val Loss: 0.4113 - Acc: 0.8581 - F1: 0.6697\n",
            "Epoch 31/50 - Train Loss: 0.3837 - Val Loss: 0.4055 - Acc: 0.8600 - F1: 0.6827\n",
            "Epoch 32/50 - Train Loss: 0.3803 - Val Loss: 0.4025 - Acc: 0.8600 - F1: 0.6903\n",
            "Epoch 33/50 - Train Loss: 0.3763 - Val Loss: 0.4017 - Acc: 0.8615 - F1: 0.6886\n",
            "Epoch 34/50 - Train Loss: 0.3748 - Val Loss: 0.3983 - Acc: 0.8620 - F1: 0.6917\n",
            "Epoch 35/50 - Train Loss: 0.3713 - Val Loss: 0.3971 - Acc: 0.8618 - F1: 0.6902\n",
            "Epoch 36/50 - Train Loss: 0.3687 - Val Loss: 0.3948 - Acc: 0.8624 - F1: 0.6908\n",
            "Epoch 37/50 - Train Loss: 0.3649 - Val Loss: 0.3927 - Acc: 0.8632 - F1: 0.6979\n",
            "Epoch 38/50 - Train Loss: 0.3619 - Val Loss: 0.3921 - Acc: 0.8638 - F1: 0.6942\n",
            "Epoch 39/50 - Train Loss: 0.3591 - Val Loss: 0.3915 - Acc: 0.8646 - F1: 0.6910\n",
            "Epoch 40/50 - Train Loss: 0.3578 - Val Loss: 0.3882 - Acc: 0.8648 - F1: 0.6988\n",
            "Epoch 41/50 - Train Loss: 0.3545 - Val Loss: 0.3877 - Acc: 0.8663 - F1: 0.6988\n",
            "Epoch 42/50 - Train Loss: 0.3509 - Val Loss: 0.3864 - Acc: 0.8659 - F1: 0.6977\n",
            "Epoch 43/50 - Train Loss: 0.3493 - Val Loss: 0.3850 - Acc: 0.8674 - F1: 0.7001\n",
            "Epoch 44/50 - Train Loss: 0.3459 - Val Loss: 0.3831 - Acc: 0.8669 - F1: 0.7018\n",
            "Epoch 45/50 - Train Loss: 0.3418 - Val Loss: 0.3820 - Acc: 0.8677 - F1: 0.7031\n",
            "Epoch 46/50 - Train Loss: 0.3403 - Val Loss: 0.3825 - Acc: 0.8680 - F1: 0.6995\n",
            "Epoch 47/50 - Train Loss: 0.3389 - Val Loss: 0.3784 - Acc: 0.8698 - F1: 0.7097\n",
            "Epoch 48/50 - Train Loss: 0.3373 - Val Loss: 0.3777 - Acc: 0.8679 - F1: 0.7102\n",
            "Epoch 49/50 - Train Loss: 0.3335 - Val Loss: 0.3764 - Acc: 0.8692 - F1: 0.7094\n",
            "Epoch 50/50 - Train Loss: 0.3309 - Val Loss: 0.3773 - Acc: 0.8698 - F1: 0.7081\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    # -------------------------\n",
        "    #        TRAIN\n",
        "    # -------------------------\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device).float()\n",
        "\n",
        "        outputs = model(inputs)            # [B, 1]\n",
        "        outputs = outputs.squeeze(1)       # [B]\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "    # -------------------------\n",
        "    #        EVAL\n",
        "    # -------------------------\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device).float()\n",
        "\n",
        "            outputs = model(inputs).squeeze(1)  # logits\n",
        "\n",
        "            # same loss as train\n",
        "            val_loss = criterion(outputs, labels)\n",
        "            total_val_loss += val_loss.item()\n",
        "\n",
        "            # convert logits â†’ probabilities â†’ binary predictions\n",
        "            probs = torch.sigmoid(outputs)\n",
        "            preds = (probs >= 0.5).long()\n",
        "\n",
        "            all_preds.append(preds.cpu())\n",
        "            all_targets.append(labels.cpu())\n",
        "\n",
        "    # concat\n",
        "    all_preds = torch.cat(all_preds).numpy()\n",
        "    all_targets = torch.cat(all_targets).numpy()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(test_loader)\n",
        "    acc = accuracy_score(all_targets, all_preds)\n",
        "    f1  = f1_score(all_targets, all_preds, zero_division=0)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} \"\n",
        "          f\"- Train Loss: {avg_train_loss:.4f} \"\n",
        "          f\"- Val Loss: {avg_val_loss:.4f} \"\n",
        "          f\"- Acc: {acc:.4f} \"\n",
        "          f\"- F1: {f1:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Kernel Python LLM",
      "language": "python",
      "name": "kernel_llm"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
