{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a841a958",
      "metadata": {},
      "source": [
        "# Environement Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cee1a5b4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cee1a5b4",
        "outputId": "4105683e-56dc-4570-acf8-2d84b10a1fe2"
      },
      "outputs": [],
      "source": [
        "# ATTENZIONE: eseguire solo su Colab!\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# --- 1. SETUP AMBIENTE E DRIVE ---\n",
        "\n",
        "from google.colab import drive\n",
        "from google.colab import userdata\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# --- 2. CONFIGURAZIONE GITHUB (PRIVATO) ---\n",
        "# Sostituisci QUESTE variabili con i tuoi dati reali\n",
        "GITHUB_USER = 'MarcoPernoVDP'\n",
        "REPO_NAME = 'MistakeDetection'\n",
        "BRANCH = 'main'\n",
        "\n",
        "# Recupera il Token dai Segreti di Colab (\"GITHUB_TOKEN\")\n",
        "try:\n",
        "    GIT_TOKEN = userdata.get('GITHUB_TOKEN')\n",
        "except ImportError:\n",
        "    # Se non usi i Segreti, incolla il token qui (SCONSIGLIATO ma funziona)\n",
        "    GIT_TOKEN = \"INCOLLA_QUI_IL_TUO_PAT_SE_NON_USI_SEGRETI\"\n",
        "except Exception:\n",
        "    print(\"âš ï¸ ERRORE: Non ho trovato il segreto 'GITHUB_TOKEN'.\")\n",
        "    print(\"Vai sulla chiave inglese a sinistra -> Aggiungi nuovo segreto -> Nome: GITHUB_TOKEN\")\n",
        "    raise\n",
        "\n",
        "REPO_URL = f'https://{GIT_TOKEN}@github.com/{GITHUB_USER}/{REPO_NAME}.git'\n",
        "ROOT_DIR = f'/content/{REPO_NAME}'\n",
        "\n",
        "# --- 3. CLONE O AGGIORNAMENTO CODICE ---\n",
        "if not os.path.exists(ROOT_DIR):\n",
        "    print(f\"ðŸ”„ Clonazione del repository {REPO_NAME}...\")\n",
        "    !git clone {REPO_URL}\n",
        "else:\n",
        "    print(f\"ðŸ”„ Aggiornamento repository (git pull)...\")\n",
        "    %cd {ROOT_DIR}\n",
        "    !git pull origin {BRANCH}\n",
        "    %cd /content\n",
        "\n",
        "# --- 4. CONFIGURAZIONE PATH PYTHON ---\n",
        "if ROOT_DIR not in sys.path:\n",
        "    sys.path.append(ROOT_DIR)\n",
        "    print(f\"âœ… Aggiunto {ROOT_DIR} al path di sistema.\")\n",
        "\n",
        "# --- 5. INSTALLAZIONE DIPENDENZE ---\n",
        "\n",
        "#\n",
        "\n",
        "print(\"\\nSETUP COMPLETATO\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "kGMl6Pu2rSYk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGMl6Pu2rSYk",
        "outputId": "a3e952d2-2c16-48ab-ea10-fdb661a42c62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device in uso: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device in uso:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UtEsNOoRtWa2",
      "metadata": {
        "id": "UtEsNOoRtWa2"
      },
      "source": [
        "# Dataset Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EfekzBNAtZ_S",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfekzBNAtZ_S",
        "outputId": "4c2e14be-f564-442c-8f9d-8415c0ff630d"
      },
      "outputs": [],
      "source": [
        "# ATTENZIONE: usare questa cella solo su Colab!\n",
        "\n",
        "# COSA FA LA CELLA:\n",
        "# crea la cartella \"/content/MistakeDetection/data\" e salva al suo interno i dati presenti\n",
        "# in Drive al percorso \"/content/drive/MyDrive/AML_MistakeDetection_DATA\" seguenti:\n",
        "#   - la cartella annotation_json (con tutti i file json al suo interno);\n",
        "#   - i dataset zip (come omnivore.zip) estratti.\n",
        "\n",
        "# Questa cella prepara dunque i dataset e gli annotation per poter usare i dati su Colab\n",
        "\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Root del progetto (notebook in Notebooks/)\n",
        "# Use the ROOT_DIR established during repository cloning\n",
        "project_root = \"/content/MistakeDetection\" # Corrected project root\n",
        "if project_root not in sys.path:\n",
        "    sys.path.append(project_root)\n",
        "\n",
        "# Importa la funzione\n",
        "from utils.setup_dataset import setup_dataset\n",
        "\n",
        "# Path della cartella _file\n",
        "#data_source_path = os.path.join(project_root, \"_file\")\n",
        "data_source_path = os.path.join(\"/content\", \"drive\", \"MyDrive\", \"AML_MistakeDetection_DATA\")\n",
        "\n",
        "\n",
        "# Chiama la funzione\n",
        "setup_dataset(data_source_path, project_root)\n",
        "\n",
        "print(f\"Dati estratti e organizzati in: {os.path.join(project_root, 'data')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "caea429b",
      "metadata": {
        "id": "caea429b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cartella 'annotation_json' copiata in 'c:\\Users\\dylan\\Desktop\\UniversitÃ \\5Â° ANNO\\AML\\MistakeDetection\\data/'.\n",
            "File ZIP 'c:\\Users\\dylan\\Desktop\\UniversitÃ \\5Â° ANNO\\AML\\MistakeDetection\\_file\\omnivore_test.zip' estratto in 'c:\\Users\\dylan\\Desktop\\UniversitÃ \\5Â° ANNO\\AML\\MistakeDetection\\data/'.\n",
            "Dati estratti e organizzati in: c:\\Users\\dylan\\Desktop\\UniversitÃ \\5Â° ANNO\\AML\\MistakeDetection\\data\n"
          ]
        }
      ],
      "source": [
        "# ATTENZIONE: usare questa cella solo su VScode!\n",
        "\n",
        "# NB: per testare il codice su VScode usando questa cella, bisogna prima aver creato nel progetto la cartella \"_file/\"\n",
        "# contenente gli stessi file presenti in Drive (cartella annotation_json e file zip dei dataset, come omnivore.zip o la sua versione\n",
        "# con pochi campioni, chiamata omnivore_test.zip, che ha 5 file .npz al suo interno)\n",
        "\n",
        "# COSA FA LA CELLA:\n",
        "# crea la cartella \"/MistakeDetection/data\" e salva al suo interno i dati presenti\n",
        "# nella cartella \"MistakeDetection/_file\" seguenti:\n",
        "#   - la cartella annotation_json (con tutti i file json al suo interno);\n",
        "#   - i dataset zip (come omnivore.zip) estratti.\n",
        "\n",
        "# Questa cella prepara dunque i dataset e gli annotation per poter usare i dati su VScode in fase di test su pochi dati\n",
        "\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Root del progetto\n",
        "project_root = os.path.abspath(\"..\") # serve per far si che Python veda il percorso della cartella padre \"MistakeDetection\" per poter importare i moduli delle altre cartelle\n",
        "if project_root not in sys.path:\n",
        "    sys.path.append(project_root)\n",
        "\n",
        "# Importa la funzione\n",
        "from utils.setup_dataset import setup_dataset\n",
        "\n",
        "# Path della cartella _file\n",
        "data_source_path = os.path.join(project_root, \"_file\")\n",
        "\n",
        "\n",
        "# Chiama la funzione\n",
        "setup_dataset(data_source_path, project_root)\n",
        "\n",
        "print(f\"Dati estratti e organizzati in: {os.path.join(project_root, 'data')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "G7sq4nHfy-AZ",
      "metadata": {
        "id": "G7sq4nHfy-AZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Root del progetto\n",
        "project_root = os.path.abspath(\"..\") # serve per far si che Python veda il percorso della cartella padre \"MistakeDetection\" per poter importare i moduli delle altre cartelle\n",
        "if project_root not in sys.path:\n",
        "    sys.path.append(project_root)\n",
        "\n",
        "from dataset.loader import CaptainCook4D_Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "75a3a690",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Full Dataset samples (100%): 9798\n",
            "\n",
            "Train samples (70%): 6858\n",
            "Validation samples (10%): 980\n",
            "Test samples (20%): 1960\n"
          ]
        }
      ],
      "source": [
        "# --- Percorsi dati ---\n",
        "features_dir = os.path.join(\"..\", \"data\", \"omnivore_test\")          # cartella npz\n",
        "annotations_dir = os.path.join(\"..\", \"data\", \"annotation_json\")  # cartella JSON\n",
        "\n",
        "# --- Inizializza il dataset completo ---\n",
        "full_dataset = CaptainCook4D_Dataset(features_dir, annotations_dir)\n",
        "\n",
        "# --- Indici di train, val e test ---\n",
        "indices = list(range(len(full_dataset)))\n",
        "\n",
        "# 20% test\n",
        "trainval_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42, shuffle=True)\n",
        "# 10% validation sul rimanente\n",
        "val_relative_size = 0.1 / 0.8\n",
        "train_idx, val_idx = train_test_split(trainval_idx, test_size=val_relative_size, random_state=42, shuffle=True)\n",
        "\n",
        "# --- Creazione dei Subset ---\n",
        "train_dataset = Subset(full_dataset, train_idx)\n",
        "val_dataset   = Subset(full_dataset, val_idx)\n",
        "test_dataset  = Subset(full_dataset, test_idx)\n",
        "\n",
        "# --- Creazione DataLoader ---\n",
        "batch_size = 512\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# --- Controllo dimensioni ---\n",
        "print(f\"Full Dataset samples (100%): {len(full_dataset)}\\n\")\n",
        "print(f\"Train samples (70%): {len(train_dataset)}\")\n",
        "print(f\"Validation samples (10%): {len(val_dataset)}\")\n",
        "print(f\"Test samples (20%): {len(test_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "f77288d0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FULL DATASET shape: torch.Size([9798, 1024])\n",
            "Numero di campioni: 9798\n",
            "Numero di feature: 1024\n",
            "\n",
            "TRAIN DATASET:\n",
            "label_0 = 4189\n",
            "label_1 = 2669\n",
            "label_1/label_0 = 0.6371\n",
            "label_0/label_1 = 1.5695\n",
            "\n",
            "VALIDATION DATASET:\n",
            "label_0 = 597\n",
            "label_1 = 383\n",
            "label_1/label_0 = 0.6415\n",
            "label_0/label_1 = 1.5587\n",
            "\n",
            "TEST DATASET:\n",
            "label_0 = 1184\n",
            "label_1 = 776\n",
            "label_1/label_0 = 0.6554\n",
            "label_0/label_1 = 1.5258\n"
          ]
        }
      ],
      "source": [
        "print(f\"FULL DATASET shape: {full_dataset.shape()}\")  # Output leggibile\n",
        "print(f\"Numero di campioni: {full_dataset.shape()[0]}\")\n",
        "print(f\"Numero di feature: {full_dataset.shape()[1]}\")\n",
        "\n",
        "\n",
        "def count_labels(dataset, name=\"Dataset\"):\n",
        "    cnt_0, cnt_1 = 0, 0\n",
        "    for i in range(len(dataset)):\n",
        "        label = dataset[i][1].item()\n",
        "        if label == 0:\n",
        "            cnt_0 += 1\n",
        "        else:\n",
        "            cnt_1 += 1\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"label_0 = {cnt_0}\")\n",
        "    print(f\"label_1 = {cnt_1}\")\n",
        "    print(f\"label_1/label_0 = {cnt_1/cnt_0:.4f}\")\n",
        "    print(f\"label_0/label_1 = {cnt_0/cnt_1:.4f}\")\n",
        "    return cnt_0, cnt_1\n",
        "\n",
        "# Conta le label per ciascun subset\n",
        "train_cnt_0, train_cnt_1 = count_labels(train_dataset, \"TRAIN DATASET\")\n",
        "val_cnt_0, val_cnt_1     = count_labels(val_dataset, \"VALIDATION DATASET\")\n",
        "test_cnt_0, test_cnt_1   = count_labels(test_dataset, \"TEST DATASET\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "6a5eed01",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File: ../data/omnivore_test/1_7_360p.mp4_1s_1s.npz\n",
            "Chiavi presenti nel file: ['arr_0']\n",
            "\n",
            "Array 'arr_0' - shape: (604, 1024), dtype: float32\n",
            "[[ 0.6910985   0.09298898 -0.6608225  ... -0.75679165  1.2401273\n",
            "  -0.5683658 ]\n",
            " [ 0.40254688 -0.4466254  -0.8645446  ... -1.2709565   0.7917245\n",
            "  -0.5052321 ]\n",
            " [ 0.643613   -0.48683766 -0.88651866 ... -1.0358062   0.658605\n",
            "  -0.27201462]\n",
            " [ 0.8338395  -0.51338077 -0.8236387  ... -0.8753807   0.51246065\n",
            "  -0.5449421 ]\n",
            " [ 0.98503673 -0.4786031  -0.6167189  ... -1.0904019   0.94557023\n",
            "  -0.4631019 ]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def inspect_npz(npz_path, n_rows=5):\n",
        "    \"\"\"\n",
        "    Mostra il contenuto di un file .npz.\n",
        "\n",
        "    Args:\n",
        "        npz_path (str): percorso del file .npz\n",
        "        n_rows (int): numero di righe da stampare per ogni array\n",
        "    \"\"\"\n",
        "    if not os.path.exists(npz_path):\n",
        "        print(f\"[ERROR] File non trovato: {npz_path}\")\n",
        "        return\n",
        "\n",
        "    data = np.load(npz_path)\n",
        "    print(f\"File: {npz_path}\")\n",
        "    print(\"Chiavi presenti nel file:\", list(data.keys()))\n",
        "\n",
        "    for key in data.keys():\n",
        "        arr = data[key]\n",
        "        print(f\"\\nArray '{key}' - shape: {arr.shape}, dtype: {arr.dtype}\")\n",
        "        print(arr[:n_rows])  # stampa le prime n_rows righe\n",
        "\n",
        "# Esempio di utilizzo\n",
        "inspect_npz(\"../data/omnivore_test/1_7_360p.mp4_1s_1s.npz\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SigNHU8ctaWJ",
      "metadata": {
        "id": "SigNHU8ctaWJ"
      },
      "source": [
        "# MLP (Version 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "O74QGf1Qo2sK",
      "metadata": {
        "id": "O74QGf1Qo2sK"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from models.MLP_version1 import MLP_version1\n",
        "\n",
        "model = MLP_version1(1024).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6FdOuKJopr3m",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FdOuKJopr3m",
        "outputId": "f888829f-fbfc-47d3-b01f-88e429b1457f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peso classe positiva: 1.5\n"
          ]
        }
      ],
      "source": [
        "lr = 0.0001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
        "\n",
        "# Quanto pesa la classe â€œpositivaâ€ = classe \"1\" = classe \"error\":\n",
        "# - CASO 1: rapporto effettivo del dataset\n",
        "#train_pos_weight = train_cnt_0 / train_cnt_1\n",
        "\n",
        "# - CASO 2: rapporto usato nel paper\n",
        "train_pos_weight = 1.5\n",
        "\n",
        "print(f\"Peso classe positiva: {train_pos_weight}\")\n",
        "train_pos_weight = torch.tensor([train_pos_weight], device=device)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=train_pos_weight)\n",
        "\n",
        "epochs = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4u_a6jPgq7OI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4u_a6jPgq7OI",
        "outputId": "b0b8b41c-0c65-4729-92b1-8f51632af365"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    # -------------------------\n",
        "    #        TRAIN\n",
        "    # -------------------------\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device).float()\n",
        "\n",
        "        outputs = model(inputs)            # [B, 1]\n",
        "        outputs = outputs.squeeze(1)       # [B]\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "    # -------------------------\n",
        "    #        EVAL\n",
        "    # -------------------------\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device).float()\n",
        "\n",
        "            outputs = model(inputs).squeeze(1)  # logits\n",
        "\n",
        "            # same loss as train\n",
        "            val_loss = criterion(outputs, labels)\n",
        "            total_val_loss += val_loss.item()\n",
        "\n",
        "            # convert logits â†’ probabilities â†’ binary predictions\n",
        "            probs = torch.sigmoid(outputs)\n",
        "            preds = (probs >= 0.5).long()\n",
        "\n",
        "            all_preds.append(preds.cpu())\n",
        "            all_targets.append(labels.cpu())\n",
        "\n",
        "    # concat\n",
        "    all_preds = torch.cat(all_preds).numpy()\n",
        "    all_targets = torch.cat(all_targets).numpy()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(test_loader)\n",
        "    acc = accuracy_score(all_targets, all_preds)\n",
        "    f1  = f1_score(all_targets, all_preds, zero_division=0)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} \"\n",
        "          f\"- Train Loss: {avg_train_loss:.4f} \"\n",
        "          f\"- Val Loss: {avg_val_loss:.4f} \"\n",
        "          f\"- Acc: {acc:.4f} \"\n",
        "          f\"- F1: {f1:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Kernel Python LLM",
      "language": "python",
      "name": "kernel_llm"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
