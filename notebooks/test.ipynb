{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a841a958",
      "metadata": {
        "id": "a841a958"
      },
      "source": [
        "# Environement Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cee1a5b4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cee1a5b4",
        "outputId": "8a58b80c-cb65-49eb-c61e-ec38f11a339a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â˜ï¸ Colab rilevato.\n",
            "Mounted at /content/drive\n",
            "Cloning into 'MistakeDetection'...\n",
            "remote: Enumerating objects: 308, done.\u001b[K\n",
            "remote: Counting objects: 100% (93/93), done.\u001b[K\n",
            "remote: Compressing objects: 100% (65/65), done.\u001b[K\n",
            "remote: Total 308 (delta 41), reused 72 (delta 22), pack-reused 215 (from 1)\u001b[K\n",
            "Receiving objects: 100% (308/308), 49.98 MiB | 29.06 MiB/s, done.\n",
            "Resolving deltas: 100% (122/122), done.\n"
          ]
        }
      ],
      "source": [
        "import sys, os\n",
        "try:\n",
        "    from google.colab import drive, userdata\n",
        "    IS_COLAB = True\n",
        "except ImportError:\n",
        "    IS_COLAB = False\n",
        "\n",
        "REPO_NAME = 'MistakeDetection'\n",
        "\n",
        "if IS_COLAB:\n",
        "    print(\"â˜ï¸ Colab rilevato.\")\n",
        "    if not os.path.exists('/content/drive'): drive.mount('/content/drive')\n",
        "\n",
        "    GITHUB_USER = 'MarcoPernoVDP'\n",
        "    try:\n",
        "        TOKEN = userdata.get('GITHUB_TOKEN')\n",
        "        REPO_URL = f'https://{TOKEN}@github.com/{GITHUB_USER}/{REPO_NAME}.git'\n",
        "    except:\n",
        "        REPO_URL = f'https://github.com/{GITHUB_USER}/{REPO_NAME}.git'\n",
        "\n",
        "    ROOT_DIR = f'/content/{REPO_NAME}'\n",
        "    if not os.path.exists(ROOT_DIR):\n",
        "        !git clone {REPO_URL}\n",
        "    else:\n",
        "        %cd {ROOT_DIR}\n",
        "        !git pull\n",
        "        %cd /content\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"Ambiente locale rilevato.\")\n",
        "    ROOT_DIR = os.getcwd()\n",
        "    while not os.path.exists(os.path.join(ROOT_DIR, '.gitignore')) and ROOT_DIR != os.path.dirname(ROOT_DIR):\n",
        "        ROOT_DIR = os.path.dirname(ROOT_DIR)\n",
        "\n",
        "if ROOT_DIR not in sys.path:\n",
        "    sys.path.append(ROOT_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UtEsNOoRtWa2",
      "metadata": {
        "id": "UtEsNOoRtWa2"
      },
      "source": [
        "# Dataset Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "73e181dd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73e181dd",
        "outputId": "d4cb32b4-8cf6-4a21-f43a-7775028498fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup Progetto in: /content/MistakeDetection\n",
            "source_path: /content/drive/MyDrive/MistakeDetection\n",
            "Setup Dati da: /content/drive/MyDrive/MistakeDetection\n",
            "Inizio setup dati...\n",
            "   Sorgente: /content/drive/MyDrive/MistakeDetection\n",
            "   Destinazione: /content/MistakeDetection/data\n",
            "Estrazione ZIP: omnivore.zip...\n",
            "Copia cartella: annotation_json...\n",
            "Estrazione ZIP: omnivore_test.zip...\n",
            "âœ… Setup completato! Dati pronti in: /content/MistakeDetection/data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33ms339450\u001b[0m (\u001b[33ms339450-politecnico-di-torino\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WandB Logged in.\n",
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "from utils import setup_project\n",
        "\n",
        "# Esegue: Setup Dati (unzip/copy), Login WandB, Setup Device\n",
        "device = setup_project.initialize(ROOT_DIR)\n",
        "\n",
        "# Ora puoi passare agli import del modello\n",
        "from dataset.capitain_cook_4d_dataset import CaptainCook4D_Dataset\n",
        "from models.BaselineV1_MLP import BaselineV1_MLP\n",
        "\n",
        "# Import wandb\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c480212",
      "metadata": {
        "id": "1c480212"
      },
      "source": [
        "# Dataset Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "75a3a690",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75a3a690",
        "outputId": "33fd56c6-e0be-4083-e4b6-063d490e2fa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading from: /content/MistakeDetection/data/omnivore...\n",
            "\n",
            "=================================================================\n",
            "DATASET INFO\n",
            "   Shape: torch.Size([340320, 1024]) -> 340320 Campioni, 1024 Features\n",
            "=================================================================\n",
            "FULL DATASET       | Tot: 340320 | OK: 257978 (75.8%) | ERR: 82342 (24.2%) | Ratio: 1:3.1\n",
            "-----------------------------------------------------------------\n",
            "TRAIN SET          | Tot: 238224 | OK: 180594 (75.8%) | ERR: 57630 (24.2%) | Ratio: 1:3.1\n",
            "VALIDATION SET     | Tot: 34032  | OK: 25725 (75.6%) | ERR: 8307  (24.4%) | Ratio: 1:3.1\n",
            "TEST SET           | Tot: 68064  | OK: 51659 (75.9%) | ERR: 16405 (24.1%) | Ratio: 1:3.1\n",
            "=================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dataset.capitain_cook_4d_dataset import CaptainCook4D_Dataset, DatasetSource\n",
        "from dataset.utils import get_loaders\n",
        "\n",
        "try:\n",
        "    full_dataset = CaptainCook4D_Dataset(dataset_source=DatasetSource.OMNIVORE, root_dir=ROOT_DIR)\n",
        "    train_loader, val_loader, test_loader = get_loaders(\n",
        "        full_dataset,\n",
        "        batch_size=512,\n",
        "        seed=42\n",
        "    )\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Errore: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6a5eed01",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a5eed01",
        "outputId": "4b0d06be-6ecd-4979-b00c-4e63c94217f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File: /content/MistakeDetection/data/omnivore/1_7_360p.mp4_1s_1s.npz\n",
            "Chiavi presenti nel file: ['arr_0']\n",
            "\n",
            "Array 'arr_0' - shape: (604, 1024), dtype: float32\n",
            "[[ 0.6910985   0.09298898 -0.6608225  ... -0.75679165  1.2401273\n",
            "  -0.5683658 ]\n",
            " [ 0.40254688 -0.4466254  -0.8645446  ... -1.2709565   0.7917245\n",
            "  -0.5052321 ]\n",
            " [ 0.643613   -0.48683766 -0.88651866 ... -1.0358062   0.658605\n",
            "  -0.27201462]]\n"
          ]
        }
      ],
      "source": [
        "from utils.inspect_npz import inspect_npz_from_dataset\n",
        "\n",
        "dataset_folder = \"omnivore\"\n",
        "npz_filename = \"1_7_360p.mp4_1s_1s.npz\"\n",
        "\n",
        "# Ispezione del file .npz\n",
        "inspect_npz_from_dataset(full_dataset.features_dir(), npz_filename, n_rows=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "fd232444",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "fd232444",
        "outputId": "a5214869-46e3-48f5-f017-bcd62ea4bdda"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">baseline-mlp-v1</strong> at: <a href='https://wandb.ai/s339450-politecnico-di-torino/mistake-detection/runs/lh723osf' target=\"_blank\">https://wandb.ai/s339450-politecnico-di-torino/mistake-detection/runs/lh723osf</a><br> View project at: <a href='https://wandb.ai/s339450-politecnico-di-torino/mistake-detection' target=\"_blank\">https://wandb.ai/s339450-politecnico-di-torino/mistake-detection</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20251206_110542-lh723osf/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.23.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251206_110933-xc75lngy</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/s339450-politecnico-di-torino/mistake-detection/runs/xc75lngy' target=\"_blank\">baseline-mlp-v1</a></strong> to <a href='https://wandb.ai/s339450-politecnico-di-torino/mistake-detection' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/s339450-politecnico-di-torino/mistake-detection' target=\"_blank\">https://wandb.ai/s339450-politecnico-di-torino/mistake-detection</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/s339450-politecnico-di-torino/mistake-detection/runs/xc75lngy' target=\"_blank\">https://wandb.ai/s339450-politecnico-di-torino/mistake-detection/runs/xc75lngy</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ W&B Run: baseline-mlp-v1 (ID: xc75lngy)\n"
          ]
        }
      ],
      "source": [
        "# Configurazione esperimento\n",
        "config = {\n",
        "    \"architecture\": \"BaselineV1_MLP\",\n",
        "    \"dataset\": \"CaptainCook4D\",\n",
        "    \"feature_extractor\": \"Omnivore\",\n",
        "    \"input_dim\": 1024,\n",
        "    \"batch_size\": 512,\n",
        "    \"learning_rate\": 0.0001,\n",
        "    \"epochs\": 100,\n",
        "    \"pos_weight\": 1.5,\n",
        "    \"optimizer\": \"Adam\",\n",
        "    \"loss_function\": \"BCEWithLogitsLoss\",\n",
        "    \"seed\": 42\n",
        "}\n",
        "\n",
        "# Inizializzazione W&B\n",
        "run = wandb.init(\n",
        "    project=\"mistake-detection\",\n",
        "    name=\"baseline-mlp-v1\",\n",
        "    config=config,\n",
        "    tags=[\"baseline\", \"mlp\", \"omnivore\"],\n",
        "    notes=\"Baseline MLP with Omnivore features for mistake detection\"\n",
        ")\n",
        "\n",
        "print(f\"ğŸš€ W&B Run: {run.name} (ID: {run.id})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d89edd5e",
      "metadata": {
        "id": "d89edd5e"
      },
      "source": [
        "# W&B Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SigNHU8ctaWJ",
      "metadata": {
        "id": "SigNHU8ctaWJ"
      },
      "source": [
        "# MLP (Version 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "O74QGf1Qo2sK",
      "metadata": {
        "id": "O74QGf1Qo2sK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "model = BaselineV1_MLP(1024).to(device)\n",
        "\n",
        "# Watch del modello per tracciare gradienti e parametri\n",
        "wandb.watch(model, log=\"all\", log_freq=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "6FdOuKJopr3m",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FdOuKJopr3m",
        "outputId": "57233821-2e98-4535-c1d4-591e8947b408"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Peso classe positiva: 1.5\n"
          ]
        }
      ],
      "source": [
        "lr = config[\"learning_rate\"]\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
        "\n",
        "# Quanto pesa la classe \"positiva\" = classe \"1\" = classe \"error\":\n",
        "# - CASO 1: rapporto effettivo del dataset\n",
        "#train_pos_weight = train_cnt_0 / train_cnt_1\n",
        "\n",
        "# - CASO 2: rapporto usato nel paper\n",
        "train_pos_weight = config[\"pos_weight\"]\n",
        "\n",
        "print(f\"Peso classe positiva: {train_pos_weight}\")\n",
        "train_pos_weight = torch.tensor([train_pos_weight], device=device)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=train_pos_weight)\n",
        "\n",
        "epochs = config[\"epochs\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "4u_a6jPgq7OI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4u_a6jPgq7OI",
        "outputId": "13345508-7fdf-4fa8-8e38-eb9ce91f13b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 - Train Loss: 0.6735 - Val Loss: 0.6474 - Val Acc: 0.7684 - Val F1: 0.2383 - Val Precision: 0.5751 - Val Recall: 0.1503\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.2383\n",
            "Epoch 2/100 - Train Loss: 0.6373 - Val Loss: 0.6205 - Val Acc: 0.7793 - Val F1: 0.3694 - Val Precision: 0.5932 - Val Recall: 0.2682\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.3694\n",
            "Epoch 3/100 - Train Loss: 0.6115 - Val Loss: 0.5968 - Val Acc: 0.7888 - Val F1: 0.3955 - Val Precision: 0.6376 - Val Recall: 0.2867\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.3955\n",
            "Epoch 4/100 - Train Loss: 0.5897 - Val Loss: 0.5765 - Val Acc: 0.7960 - Val F1: 0.4406 - Val Precision: 0.6500 - Val Recall: 0.3333\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.4406\n",
            "Epoch 5/100 - Train Loss: 0.5698 - Val Loss: 0.5597 - Val Acc: 0.8029 - Val F1: 0.4815 - Val Precision: 0.6581 - Val Recall: 0.3796\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.4815\n",
            "Epoch 6/100 - Train Loss: 0.5547 - Val Loss: 0.5466 - Val Acc: 0.8084 - Val F1: 0.4911 - Val Precision: 0.6824 - Val Recall: 0.3836\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.4911\n",
            "Epoch 7/100 - Train Loss: 0.5409 - Val Loss: 0.5331 - Val Acc: 0.8129 - Val F1: 0.5261 - Val Precision: 0.6753 - Val Recall: 0.4310\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.5261\n",
            "Epoch 8/100 - Train Loss: 0.5283 - Val Loss: 0.5227 - Val Acc: 0.8166 - Val F1: 0.5351 - Val Precision: 0.6879 - Val Recall: 0.4378\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.5351\n",
            "Epoch 9/100 - Train Loss: 0.5180 - Val Loss: 0.5128 - Val Acc: 0.8195 - Val F1: 0.5633 - Val Precision: 0.6758 - Val Recall: 0.4829\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.5633\n",
            "Epoch 10/100 - Train Loss: 0.5076 - Val Loss: 0.5053 - Val Acc: 0.8218 - Val F1: 0.5519 - Val Precision: 0.7008 - Val Recall: 0.4552\n",
            "Epoch 11/100 - Train Loss: 0.4976 - Val Loss: 0.4961 - Val Acc: 0.8261 - Val F1: 0.5767 - Val Precision: 0.6974 - Val Recall: 0.4917\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.5767\n",
            "Epoch 12/100 - Train Loss: 0.4897 - Val Loss: 0.4890 - Val Acc: 0.8290 - Val F1: 0.5939 - Val Precision: 0.6944 - Val Recall: 0.5187\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.5939\n",
            "Epoch 13/100 - Train Loss: 0.4818 - Val Loss: 0.4844 - Val Acc: 0.8321 - Val F1: 0.5821 - Val Precision: 0.7273 - Val Recall: 0.4852\n",
            "Epoch 14/100 - Train Loss: 0.4740 - Val Loss: 0.4762 - Val Acc: 0.8327 - Val F1: 0.6124 - Val Precision: 0.6934 - Val Recall: 0.5483\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.6124\n",
            "Epoch 15/100 - Train Loss: 0.4677 - Val Loss: 0.4709 - Val Acc: 0.8345 - Val F1: 0.6154 - Val Precision: 0.6996 - Val Recall: 0.5493\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.6154\n",
            "Epoch 16/100 - Train Loss: 0.4611 - Val Loss: 0.4651 - Val Acc: 0.8374 - Val F1: 0.6278 - Val Precision: 0.7000 - Val Recall: 0.5692\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.6278\n",
            "Epoch 17/100 - Train Loss: 0.4549 - Val Loss: 0.4609 - Val Acc: 0.8391 - Val F1: 0.6276 - Val Precision: 0.7096 - Val Recall: 0.5626\n",
            "Epoch 18/100 - Train Loss: 0.4484 - Val Loss: 0.4564 - Val Acc: 0.8415 - Val F1: 0.6241 - Val Precision: 0.7287 - Val Recall: 0.5457\n",
            "Epoch 19/100 - Train Loss: 0.4433 - Val Loss: 0.4521 - Val Acc: 0.8433 - Val F1: 0.6284 - Val Precision: 0.7334 - Val Recall: 0.5496\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.6284\n",
            "Epoch 20/100 - Train Loss: 0.4397 - Val Loss: 0.4483 - Val Acc: 0.8451 - Val F1: 0.6312 - Val Precision: 0.7408 - Val Recall: 0.5499\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.6312\n",
            "Epoch 21/100 - Train Loss: 0.4332 - Val Loss: 0.4464 - Val Acc: 0.8462 - Val F1: 0.6272 - Val Precision: 0.7542 - Val Recall: 0.5367\n",
            "Epoch 22/100 - Train Loss: 0.4289 - Val Loss: 0.4405 - Val Acc: 0.8467 - Val F1: 0.6458 - Val Precision: 0.7288 - Val Recall: 0.5798\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.6458\n",
            "Epoch 23/100 - Train Loss: 0.4231 - Val Loss: 0.4366 - Val Acc: 0.8488 - Val F1: 0.6484 - Val Precision: 0.7379 - Val Recall: 0.5782\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.6484\n",
            "Epoch 24/100 - Train Loss: 0.4180 - Val Loss: 0.4334 - Val Acc: 0.8498 - Val F1: 0.6511 - Val Precision: 0.7396 - Val Recall: 0.5816\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.6511\n",
            "Epoch 25/100 - Train Loss: 0.4138 - Val Loss: 0.4311 - Val Acc: 0.8500 - Val F1: 0.6573 - Val Precision: 0.7314 - Val Recall: 0.5968\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.6573\n",
            "Epoch 26/100 - Train Loss: 0.4092 - Val Loss: 0.4274 - Val Acc: 0.8536 - Val F1: 0.6580 - Val Precision: 0.7531 - Val Recall: 0.5842\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.6580\n",
            "Epoch 27/100 - Train Loss: 0.4057 - Val Loss: 0.4255 - Val Acc: 0.8535 - Val F1: 0.6579 - Val Precision: 0.7524 - Val Recall: 0.5845\n",
            "Epoch 28/100 - Train Loss: 0.4021 - Val Loss: 0.4216 - Val Acc: 0.8545 - Val F1: 0.6615 - Val Precision: 0.7529 - Val Recall: 0.5899\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.6615\n",
            "Epoch 29/100 - Train Loss: 0.3979 - Val Loss: 0.4190 - Val Acc: 0.8545 - Val F1: 0.6707 - Val Precision: 0.7378 - Val Recall: 0.6147\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.6707\n",
            "Epoch 30/100 - Train Loss: 0.3944 - Val Loss: 0.4178 - Val Acc: 0.8560 - Val F1: 0.6658 - Val Precision: 0.7552 - Val Recall: 0.5954\n",
            "Epoch 31/100 - Train Loss: 0.3909 - Val Loss: 0.4148 - Val Acc: 0.8577 - Val F1: 0.6723 - Val Precision: 0.7554 - Val Recall: 0.6056\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.6723\n",
            "Epoch 32/100 - Train Loss: 0.3866 - Val Loss: 0.4166 - Val Acc: 0.8572 - Val F1: 0.6605 - Val Precision: 0.7737 - Val Recall: 0.5762\n",
            "Epoch 33/100 - Train Loss: 0.3827 - Val Loss: 0.4095 - Val Acc: 0.8580 - Val F1: 0.6797 - Val Precision: 0.7446 - Val Recall: 0.6252\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.6797\n",
            "Epoch 34/100 - Train Loss: 0.3798 - Val Loss: 0.4086 - Val Acc: 0.8601 - Val F1: 0.6788 - Val Precision: 0.7602 - Val Recall: 0.6131\n",
            "Epoch 35/100 - Train Loss: 0.3760 - Val Loss: 0.4066 - Val Acc: 0.8600 - Val F1: 0.6787 - Val Precision: 0.7595 - Val Recall: 0.6134\n",
            "Epoch 36/100 - Train Loss: 0.3736 - Val Loss: 0.4059 - Val Acc: 0.8603 - Val F1: 0.6773 - Val Precision: 0.7637 - Val Recall: 0.6085\n",
            "Epoch 37/100 - Train Loss: 0.3699 - Val Loss: 0.4020 - Val Acc: 0.8613 - Val F1: 0.6875 - Val Precision: 0.7523 - Val Recall: 0.6330\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.6875\n",
            "Epoch 38/100 - Train Loss: 0.3688 - Val Loss: 0.4005 - Val Acc: 0.8600 - Val F1: 0.6859 - Val Precision: 0.7470 - Val Recall: 0.6341\n",
            "Epoch 39/100 - Train Loss: 0.3635 - Val Loss: 0.4022 - Val Acc: 0.8610 - Val F1: 0.6790 - Val Precision: 0.7660 - Val Recall: 0.6097\n",
            "Epoch 40/100 - Train Loss: 0.3619 - Val Loss: 0.3975 - Val Acc: 0.8623 - Val F1: 0.6904 - Val Precision: 0.7536 - Val Recall: 0.6369\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.6904\n",
            "Epoch 41/100 - Train Loss: 0.3581 - Val Loss: 0.3961 - Val Acc: 0.8621 - Val F1: 0.6927 - Val Precision: 0.7480 - Val Recall: 0.6450\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.6927\n",
            "Epoch 42/100 - Train Loss: 0.3562 - Val Loss: 0.3954 - Val Acc: 0.8650 - Val F1: 0.6931 - Val Precision: 0.7667 - Val Recall: 0.6324\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.6931\n",
            "Epoch 43/100 - Train Loss: 0.3533 - Val Loss: 0.3926 - Val Acc: 0.8639 - Val F1: 0.6980 - Val Precision: 0.7500 - Val Recall: 0.6528\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.6980\n",
            "Epoch 44/100 - Train Loss: 0.3503 - Val Loss: 0.3938 - Val Acc: 0.8657 - Val F1: 0.6920 - Val Precision: 0.7739 - Val Recall: 0.6258\n",
            "Epoch 45/100 - Train Loss: 0.3479 - Val Loss: 0.3901 - Val Acc: 0.8658 - Val F1: 0.6985 - Val Precision: 0.7615 - Val Recall: 0.6452\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.6985\n",
            "Epoch 46/100 - Train Loss: 0.3457 - Val Loss: 0.3897 - Val Acc: 0.8658 - Val F1: 0.6984 - Val Precision: 0.7621 - Val Recall: 0.6446\n",
            "Epoch 47/100 - Train Loss: 0.3428 - Val Loss: 0.3888 - Val Acc: 0.8670 - Val F1: 0.6990 - Val Precision: 0.7691 - Val Recall: 0.6407\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.6990\n",
            "Epoch 48/100 - Train Loss: 0.3409 - Val Loss: 0.3868 - Val Acc: 0.8672 - Val F1: 0.7010 - Val Precision: 0.7666 - Val Recall: 0.6458\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.7010\n",
            "Epoch 49/100 - Train Loss: 0.3379 - Val Loss: 0.3855 - Val Acc: 0.8672 - Val F1: 0.7036 - Val Precision: 0.7612 - Val Recall: 0.6541\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.7036\n",
            "Epoch 50/100 - Train Loss: 0.3354 - Val Loss: 0.3873 - Val Acc: 0.8691 - Val F1: 0.6996 - Val Precision: 0.7828 - Val Recall: 0.6324\n",
            "Epoch 51/100 - Train Loss: 0.3336 - Val Loss: 0.3852 - Val Acc: 0.8684 - Val F1: 0.7014 - Val Precision: 0.7743 - Val Recall: 0.6410\n",
            "Epoch 52/100 - Train Loss: 0.3304 - Val Loss: 0.3852 - Val Acc: 0.8682 - Val F1: 0.6991 - Val Precision: 0.7774 - Val Recall: 0.6352\n",
            "Epoch 53/100 - Train Loss: 0.3299 - Val Loss: 0.3840 - Val Acc: 0.8688 - Val F1: 0.7013 - Val Precision: 0.7771 - Val Recall: 0.6390\n",
            "Epoch 54/100 - Train Loss: 0.3268 - Val Loss: 0.3835 - Val Acc: 0.8693 - Val F1: 0.7013 - Val Precision: 0.7807 - Val Recall: 0.6365\n",
            "Epoch 55/100 - Train Loss: 0.3254 - Val Loss: 0.3815 - Val Acc: 0.8695 - Val F1: 0.7053 - Val Precision: 0.7742 - Val Recall: 0.6476\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.7053\n",
            "Epoch 56/100 - Train Loss: 0.3245 - Val Loss: 0.3794 - Val Acc: 0.8695 - Val F1: 0.7067 - Val Precision: 0.7708 - Val Recall: 0.6524\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.7067\n",
            "Epoch 57/100 - Train Loss: 0.3196 - Val Loss: 0.3782 - Val Acc: 0.8706 - Val F1: 0.7077 - Val Precision: 0.7768 - Val Recall: 0.6499\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.7077\n",
            "Epoch 58/100 - Train Loss: 0.3188 - Val Loss: 0.3783 - Val Acc: 0.8698 - Val F1: 0.7066 - Val Precision: 0.7729 - Val Recall: 0.6508\n",
            "Epoch 59/100 - Train Loss: 0.3157 - Val Loss: 0.3793 - Val Acc: 0.8718 - Val F1: 0.7057 - Val Precision: 0.7901 - Val Recall: 0.6375\n",
            "Epoch 60/100 - Train Loss: 0.3140 - Val Loss: 0.3756 - Val Acc: 0.8708 - Val F1: 0.7121 - Val Precision: 0.7692 - Val Recall: 0.6630\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.7121\n",
            "Epoch 61/100 - Train Loss: 0.3128 - Val Loss: 0.3775 - Val Acc: 0.8719 - Val F1: 0.7080 - Val Precision: 0.7855 - Val Recall: 0.6444\n",
            "Epoch 62/100 - Train Loss: 0.3111 - Val Loss: 0.3766 - Val Acc: 0.8725 - Val F1: 0.7105 - Val Precision: 0.7846 - Val Recall: 0.6492\n",
            "Epoch 63/100 - Train Loss: 0.3100 - Val Loss: 0.3744 - Val Acc: 0.8728 - Val F1: 0.7140 - Val Precision: 0.7795 - Val Recall: 0.6586\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.7140\n",
            "Epoch 64/100 - Train Loss: 0.3068 - Val Loss: 0.3738 - Val Acc: 0.8723 - Val F1: 0.7150 - Val Precision: 0.7733 - Val Recall: 0.6649\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.7150\n",
            "Epoch 65/100 - Train Loss: 0.3057 - Val Loss: 0.3729 - Val Acc: 0.8724 - Val F1: 0.7155 - Val Precision: 0.7736 - Val Recall: 0.6656\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.7155\n",
            "Epoch 66/100 - Train Loss: 0.3036 - Val Loss: 0.3714 - Val Acc: 0.8724 - Val F1: 0.7155 - Val Precision: 0.7733 - Val Recall: 0.6658\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.7155\n",
            "Epoch 67/100 - Train Loss: 0.3014 - Val Loss: 0.3707 - Val Acc: 0.8735 - Val F1: 0.7193 - Val Precision: 0.7729 - Val Recall: 0.6727\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.7193\n",
            "Epoch 68/100 - Train Loss: 0.3007 - Val Loss: 0.3700 - Val Acc: 0.8727 - Val F1: 0.7193 - Val Precision: 0.7676 - Val Recall: 0.6766\n",
            "Epoch 69/100 - Train Loss: 0.2975 - Val Loss: 0.3720 - Val Acc: 0.8751 - Val F1: 0.7165 - Val Precision: 0.7912 - Val Recall: 0.6546\n",
            "Epoch 70/100 - Train Loss: 0.2974 - Val Loss: 0.3708 - Val Acc: 0.8742 - Val F1: 0.7176 - Val Precision: 0.7817 - Val Recall: 0.6632\n",
            "Epoch 71/100 - Train Loss: 0.2957 - Val Loss: 0.3725 - Val Acc: 0.8746 - Val F1: 0.7147 - Val Precision: 0.7909 - Val Recall: 0.6519\n",
            "Epoch 72/100 - Train Loss: 0.2944 - Val Loss: 0.3686 - Val Acc: 0.8732 - Val F1: 0.7195 - Val Precision: 0.7705 - Val Recall: 0.6749\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.7195\n",
            "Epoch 73/100 - Train Loss: 0.2921 - Val Loss: 0.3697 - Val Acc: 0.8748 - Val F1: 0.7205 - Val Precision: 0.7795 - Val Recall: 0.6699\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.7205\n",
            "Epoch 74/100 - Train Loss: 0.2908 - Val Loss: 0.3728 - Val Acc: 0.8758 - Val F1: 0.7171 - Val Precision: 0.7951 - Val Recall: 0.6530\n",
            "Epoch 75/100 - Train Loss: 0.2882 - Val Loss: 0.3708 - Val Acc: 0.8752 - Val F1: 0.7175 - Val Precision: 0.7898 - Val Recall: 0.6573\n",
            "Epoch 76/100 - Train Loss: 0.2882 - Val Loss: 0.3684 - Val Acc: 0.8755 - Val F1: 0.7205 - Val Precision: 0.7850 - Val Recall: 0.6658\n",
            "Epoch 77/100 - Train Loss: 0.2872 - Val Loss: 0.3666 - Val Acc: 0.8746 - Val F1: 0.7232 - Val Precision: 0.7726 - Val Recall: 0.6797\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.7232\n",
            "Epoch 78/100 - Train Loss: 0.2851 - Val Loss: 0.3680 - Val Acc: 0.8754 - Val F1: 0.7200 - Val Precision: 0.7858 - Val Recall: 0.6643\n",
            "Epoch 79/100 - Train Loss: 0.2841 - Val Loss: 0.3653 - Val Acc: 0.8757 - Val F1: 0.7249 - Val Precision: 0.7766 - Val Recall: 0.6795\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.7249\n",
            "Epoch 80/100 - Train Loss: 0.2823 - Val Loss: 0.3670 - Val Acc: 0.8763 - Val F1: 0.7213 - Val Precision: 0.7894 - Val Recall: 0.6641\n",
            "Epoch 81/100 - Train Loss: 0.2795 - Val Loss: 0.3652 - Val Acc: 0.8755 - Val F1: 0.7242 - Val Precision: 0.7770 - Val Recall: 0.6780\n",
            "Epoch 82/100 - Train Loss: 0.2785 - Val Loss: 0.3668 - Val Acc: 0.8769 - Val F1: 0.7227 - Val Precision: 0.7908 - Val Recall: 0.6654\n",
            "Epoch 83/100 - Train Loss: 0.2770 - Val Loss: 0.3671 - Val Acc: 0.8766 - Val F1: 0.7228 - Val Precision: 0.7882 - Val Recall: 0.6675\n",
            "Epoch 84/100 - Train Loss: 0.2744 - Val Loss: 0.3654 - Val Acc: 0.8771 - Val F1: 0.7245 - Val Precision: 0.7877 - Val Recall: 0.6708\n",
            "Epoch 85/100 - Train Loss: 0.2768 - Val Loss: 0.3646 - Val Acc: 0.8765 - Val F1: 0.7256 - Val Precision: 0.7813 - Val Recall: 0.6773\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.7256\n",
            "Epoch 86/100 - Train Loss: 0.2749 - Val Loss: 0.3645 - Val Acc: 0.8766 - Val F1: 0.7239 - Val Precision: 0.7859 - Val Recall: 0.6710\n",
            "Epoch 87/100 - Train Loss: 0.2729 - Val Loss: 0.3646 - Val Acc: 0.8762 - Val F1: 0.7253 - Val Precision: 0.7800 - Val Recall: 0.6778\n",
            "Epoch 88/100 - Train Loss: 0.2716 - Val Loss: 0.3646 - Val Acc: 0.8776 - Val F1: 0.7261 - Val Precision: 0.7881 - Val Recall: 0.6732\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.7261\n",
            "Epoch 89/100 - Train Loss: 0.2704 - Val Loss: 0.3676 - Val Acc: 0.8786 - Val F1: 0.7250 - Val Precision: 0.7985 - Val Recall: 0.6639\n",
            "Epoch 90/100 - Train Loss: 0.2693 - Val Loss: 0.3636 - Val Acc: 0.8770 - Val F1: 0.7271 - Val Precision: 0.7816 - Val Recall: 0.6797\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.7271\n",
            "Epoch 91/100 - Train Loss: 0.2677 - Val Loss: 0.3640 - Val Acc: 0.8779 - Val F1: 0.7265 - Val Precision: 0.7896 - Val Recall: 0.6727\n",
            "Epoch 92/100 - Train Loss: 0.2657 - Val Loss: 0.3618 - Val Acc: 0.8779 - Val F1: 0.7293 - Val Precision: 0.7829 - Val Recall: 0.6827\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.7293\n",
            "Epoch 93/100 - Train Loss: 0.2645 - Val Loss: 0.3631 - Val Acc: 0.8785 - Val F1: 0.7294 - Val Precision: 0.7872 - Val Recall: 0.6794\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.7294\n",
            "Epoch 94/100 - Train Loss: 0.2640 - Val Loss: 0.3627 - Val Acc: 0.8776 - Val F1: 0.7276 - Val Precision: 0.7850 - Val Recall: 0.6780\n",
            "Epoch 95/100 - Train Loss: 0.2624 - Val Loss: 0.3637 - Val Acc: 0.8784 - Val F1: 0.7285 - Val Precision: 0.7888 - Val Recall: 0.6767\n",
            "Epoch 96/100 - Train Loss: 0.2620 - Val Loss: 0.3622 - Val Acc: 0.8782 - Val F1: 0.7286 - Val Precision: 0.7868 - Val Recall: 0.6785\n",
            "Epoch 97/100 - Train Loss: 0.2612 - Val Loss: 0.3613 - Val Acc: 0.8781 - Val F1: 0.7314 - Val Precision: 0.7801 - Val Recall: 0.6884\n",
            "âœ… Nuovo miglior modello salvato! F1: 0.7314\n",
            "Epoch 98/100 - Train Loss: 0.2594 - Val Loss: 0.3624 - Val Acc: 0.8776 - Val F1: 0.7303 - Val Precision: 0.7785 - Val Recall: 0.6878\n",
            "Epoch 99/100 - Train Loss: 0.2583 - Val Loss: 0.3639 - Val Acc: 0.8780 - Val F1: 0.7283 - Val Precision: 0.7861 - Val Recall: 0.6785\n",
            "Epoch 100/100 - Train Loss: 0.2583 - Val Loss: 0.3632 - Val Acc: 0.8791 - Val F1: 0.7284 - Val Precision: 0.7945 - Val Recall: 0.6725\n",
            "\n",
            "ğŸ‰ Training completato!\n",
            "Miglior F1 Score: 0.7314\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "best_f1 = 0.0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    # -------------------------\n",
        "    #        TRAIN\n",
        "    # -------------------------\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    train_preds_list = []\n",
        "    train_targets_list = []\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device).float()\n",
        "\n",
        "        outputs = model(inputs)            # [B, 1]\n",
        "        outputs = outputs.squeeze(1)       # [B]\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Raccolta predizioni per metriche di train\n",
        "        with torch.no_grad():\n",
        "            probs = torch.sigmoid(outputs)\n",
        "            preds = (probs >= 0.5).long()\n",
        "            train_preds_list.append(preds.cpu())\n",
        "            train_targets_list.append(labels.cpu())\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "    # Metriche di training\n",
        "    train_preds = torch.cat(train_preds_list).numpy()\n",
        "    train_targets = torch.cat(train_targets_list).numpy()\n",
        "    train_acc = accuracy_score(train_targets, train_preds)\n",
        "    train_f1 = f1_score(train_targets, train_preds, zero_division=0)\n",
        "    train_precision = precision_score(train_targets, train_preds, zero_division=0)\n",
        "    train_recall = recall_score(train_targets, train_preds, zero_division=0)\n",
        "\n",
        "    # -------------------------\n",
        "    #        EVAL\n",
        "    # -------------------------\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device).float()\n",
        "\n",
        "            outputs = model(inputs).squeeze(1)  # logits\n",
        "\n",
        "            # same loss as train\n",
        "            val_loss = criterion(outputs, labels)\n",
        "            total_val_loss += val_loss.item()\n",
        "\n",
        "            # convert logits â†’ probabilities â†’ binary predictions\n",
        "            probs = torch.sigmoid(outputs)\n",
        "            preds = (probs >= 0.5).long()\n",
        "\n",
        "            all_preds.append(preds.cpu())\n",
        "            all_targets.append(labels.cpu())\n",
        "            all_probs.append(probs.cpu())\n",
        "\n",
        "    # concat\n",
        "    all_preds = torch.cat(all_preds).numpy()\n",
        "    all_targets = torch.cat(all_targets).numpy()\n",
        "    all_probs = torch.cat(all_probs).numpy()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(test_loader)\n",
        "    val_acc = accuracy_score(all_targets, all_preds)\n",
        "    val_f1 = f1_score(all_targets, all_preds, zero_division=0)\n",
        "    val_precision = precision_score(all_targets, all_preds, zero_division=0)\n",
        "    val_recall = recall_score(all_targets, all_preds, zero_division=0)\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(all_targets, all_preds)\n",
        "\n",
        "    # Log su W&B\n",
        "    wandb.log({\n",
        "        # Training metrics\n",
        "        \"train/loss\": avg_train_loss,\n",
        "        \"train/accuracy\": train_acc,\n",
        "        \"train/f1\": train_f1,\n",
        "        \"train/precision\": train_precision,\n",
        "        \"train/recall\": train_recall,\n",
        "\n",
        "        # Validation metrics\n",
        "        \"val/loss\": avg_val_loss,\n",
        "        \"val/accuracy\": val_acc,\n",
        "        \"val/f1\": val_f1,\n",
        "        \"val/precision\": val_precision,\n",
        "        \"val/recall\": val_recall,\n",
        "\n",
        "        # Confusion Matrix\n",
        "        \"val/confusion_matrix\": wandb.plot.confusion_matrix(\n",
        "            probs=None,\n",
        "            y_true=all_targets,\n",
        "            preds=all_preds,\n",
        "            class_names=[\"No Error\", \"Error\"]\n",
        "        ),\n",
        "\n",
        "        # Learning\n",
        "        \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
        "        \"epoch\": epoch + 1\n",
        "    })\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} \"\n",
        "          f\"- Train Loss: {avg_train_loss:.4f} \"\n",
        "          f\"- Val Loss: {avg_val_loss:.4f} \"\n",
        "          f\"- Val Acc: {val_acc:.4f} \"\n",
        "          f\"- Val F1: {val_f1:.4f} \"\n",
        "          f\"- Val Precision: {val_precision:.4f} \"\n",
        "          f\"- Val Recall: {val_recall:.4f}\")\n",
        "\n",
        "    # Salva il miglior modello\n",
        "    if val_f1 > best_f1:\n",
        "        best_f1 = val_f1\n",
        "        checkpoint_path = os.path.join(ROOT_DIR, \"checkpoints\", f\"best_model_f1_{best_f1:.4f}.pth\")\n",
        "        os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_f1': val_f1,\n",
        "            'val_acc': val_acc,\n",
        "        }, checkpoint_path)\n",
        "\n",
        "        # Salva come artifact su W&B\n",
        "        artifact = wandb.Artifact(\n",
        "            name=f\"model-{run.id}\",\n",
        "            type=\"model\",\n",
        "            description=f\"Best model with F1={best_f1:.4f}\",\n",
        "            metadata={\n",
        "                \"epoch\": epoch + 1,\n",
        "                \"val_f1\": val_f1,\n",
        "                \"val_acc\": val_acc,\n",
        "                \"architecture\": config[\"architecture\"]\n",
        "            }\n",
        "        )\n",
        "        artifact.add_file(checkpoint_path)\n",
        "        wandb.log_artifact(artifact)\n",
        "\n",
        "        print(f\"âœ… Nuovo miglior modello salvato! F1: {best_f1:.4f}\")\n",
        "\n",
        "print(\"\\nğŸ‰ Training completato!\")\n",
        "print(f\"Miglior F1 Score: {best_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "673f70e8",
      "metadata": {
        "id": "673f70e8"
      },
      "source": [
        "# Results & Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "cf287b55",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf287b55",
        "outputId": "555886e6-4b62-4a49-d37d-94c46f44657f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Tabella predizioni e summary caricati su W&B\n"
          ]
        }
      ],
      "source": [
        "# Log della tabella con esempi di predizioni\n",
        "model.eval()\n",
        "predictions_table = wandb.Table(\n",
        "    columns=[\"ID\", \"True Label\", \"Predicted\", \"Probability\", \"Correct\"]\n",
        ")\n",
        "\n",
        "global_id = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device).float()\n",
        "\n",
        "        outputs = model(inputs).squeeze(1)\n",
        "        probs = torch.sigmoid(outputs)\n",
        "        preds = (probs >= 0.5).long()\n",
        "\n",
        "        for i in range(min(50, len(labels))):\n",
        "            predictions_table.add_data(\n",
        "                global_id,\n",
        "                labels[i].item(),\n",
        "                preds[i].item(),\n",
        "                probs[i].item(),\n",
        "                preds[i].item() == labels[i].item()\n",
        "            )\n",
        "            global_id += 1\n",
        "\n",
        "        break  # solo un batch\n",
        "\n",
        "wandb.log({\"predictions/sample_table\": predictions_table})\n",
        "\n",
        "# Summary\n",
        "wandb.run.summary[\"best_f1\"] = best_f1\n",
        "wandb.run.summary[\"final_val_acc\"] = val_acc\n",
        "wandb.run.summary[\"final_val_precision\"] = val_precision\n",
        "wandb.run.summary[\"final_val_recall\"] = val_recall\n",
        "\n",
        "print(\"âœ… Tabella predizioni e summary caricati su W&B\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "68310055",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 753
        },
        "id": "68310055",
        "outputId": "344af75f-3aea-49c2-e9de-77de00022ff5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ</td></tr><tr><td>learning_rate</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>train/accuracy</td><td>â–â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/f1</td><td>â–â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/loss</td><td>â–ˆâ–‡â–†â–†â–†â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–</td></tr><tr><td>train/precision</td><td>â–â–‚â–‚â–‚â–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/recall</td><td>â–â–„â–„â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>val/accuracy</td><td>â–â–ƒâ–ƒâ–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>val/f1</td><td>â–â–‚â–ƒâ–„â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>val/loss</td><td>â–ˆâ–†â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>+2</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_f1</td><td>0.73141</td></tr><tr><td>epoch</td><td>100</td></tr><tr><td>final_val_acc</td><td>0.87913</td></tr><tr><td>final_val_precision</td><td>0.79447</td></tr><tr><td>final_val_recall</td><td>0.67248</td></tr><tr><td>learning_rate</td><td>0.0001</td></tr><tr><td>train/accuracy</td><td>0.91211</td></tr><tr><td>train/f1</td><td>0.81703</td></tr><tr><td>train/loss</td><td>0.25831</td></tr><tr><td>train/precision</td><td>0.82298</td></tr><tr><td>+6</td><td>...</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">baseline-mlp-v1</strong> at: <a href='https://wandb.ai/s339450-politecnico-di-torino/mistake-detection/runs/xc75lngy' target=\"_blank\">https://wandb.ai/s339450-politecnico-di-torino/mistake-detection/runs/xc75lngy</a><br> View project at: <a href='https://wandb.ai/s339450-politecnico-di-torino/mistake-detection' target=\"_blank\">https://wandb.ai/s339450-politecnico-di-torino/mistake-detection</a><br>Synced 5 W&B file(s), 103 media file(s), 310 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20251206_110933-xc75lngy/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ W&B run terminato\n"
          ]
        }
      ],
      "source": [
        "# Chiudi il run di W&B\n",
        "wandb.finish()\n",
        "print(\"ğŸ W&B run terminato\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv (3.12.6)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}