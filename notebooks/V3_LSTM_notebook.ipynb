{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be184e4f",
   "metadata": {},
   "source": [
    "# Environement Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "897b4c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiente locale rilevato.\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "try:\n",
    "    from google.colab import drive, userdata\n",
    "    IS_COLAB = True\n",
    "except ImportError:\n",
    "    IS_COLAB = False\n",
    "\n",
    "REPO_NAME = 'MistakeDetection'\n",
    "\n",
    "if IS_COLAB:\n",
    "    print(\"‚òÅÔ∏è Colab rilevato.\")\n",
    "    if not os.path.exists('/content/drive'): drive.mount('/content/drive')\n",
    "\n",
    "    GITHUB_USER = 'MarcoPernoVDP'\n",
    "    try:\n",
    "        TOKEN = userdata.get('GITHUB_TOKEN')\n",
    "        REPO_URL = f'https://{TOKEN}@github.com/{GITHUB_USER}/{REPO_NAME}.git'\n",
    "    except:\n",
    "        REPO_URL = f'https://github.com/{GITHUB_USER}/{REPO_NAME}.git'\n",
    "\n",
    "    ROOT_DIR = f'/content/{REPO_NAME}'\n",
    "    if not os.path.exists(ROOT_DIR):\n",
    "        !git clone {REPO_URL}\n",
    "    else:\n",
    "        %cd {ROOT_DIR}\n",
    "        !git pull\n",
    "        %cd /content\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"Ambiente locale rilevato.\")\n",
    "    ROOT_DIR = os.getcwd()\n",
    "    while not os.path.exists(os.path.join(ROOT_DIR, '.gitignore')) and ROOT_DIR != os.path.dirname(ROOT_DIR):\n",
    "        ROOT_DIR = os.path.dirname(ROOT_DIR)\n",
    "\n",
    "if ROOT_DIR not in sys.path:\n",
    "    sys.path.append(ROOT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eeacfa3",
   "metadata": {},
   "source": [
    "# Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2e4e004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Progetto in: c:\\Users\\marco\\Desktop\\Marco\\Programmazione\\C\\EsPoli\\Advanced Machine Learning\\MistakeDetection\n",
      "source_path: c:\\Users\\marco\\Desktop\\Marco\\Programmazione\\C\\EsPoli\\Advanced Machine Learning\\MistakeDetection\\_file\n",
      "Setup Dati da: c:\\Users\\marco\\Desktop\\Marco\\Programmazione\\C\\EsPoli\\Advanced Machine Learning\\MistakeDetection\\_file\n",
      "Inizio setup dati...\n",
      "   Sorgente: c:\\Users\\marco\\Desktop\\Marco\\Programmazione\\C\\EsPoli\\Advanced Machine Learning\\MistakeDetection\\_file\n",
      "   Destinazione: c:\\Users\\marco\\Desktop\\Marco\\Programmazione\\C\\EsPoli\\Advanced Machine Learning\\MistakeDetection\\data\n",
      "Copia cartella: annotation_json...\n",
      "Copia cartella: omnivore...\n",
      "‚úÖ Setup completato! Dati pronti in: c:\\Users\\marco\\Desktop\\Marco\\Programmazione\\C\\EsPoli\\Advanced Machine Learning\\MistakeDetection\\data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\marco\\_netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33ms339450\u001b[0m (\u001b[33ms339450-politecnico-di-torino\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandB Logged in.\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "from utils import setup_project\n",
    "# Ora puoi passare agli import del modello\n",
    "from dataset.capitain_cook_4d_mlp_dataset import CaptainCook4DMLP_Dataset, DatasetSource\n",
    "from models.BaselineV3_LSTM import BaselineV3_LSTM\n",
    "from dataset.utils import SplitType\n",
    "\n",
    "# Esegue: Setup Dati (unzip/copy), Login WandB, Setup Device\n",
    "device = setup_project.initialize(ROOT_DIR)\n",
    "\n",
    "# Import wandb\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140d213d",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "292bb23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurazione esperimento\n",
    "DATASET_SOURCE = DatasetSource.OMNIVORE\n",
    "SPLIT_TYPE = SplitType.STEP_ID\n",
    "\n",
    "config = {\n",
    "    \"architecture\": \"BaselineV3_LSTM_\" + DATASET_SOURCE.value + \"_\" + SPLIT_TYPE.value,\n",
    "    \"dataset\": \"CaptainCook4D\",\n",
    "    \"feature_extractor\": DATASET_SOURCE.value,\n",
    "    \"input_dim\": DATASET_SOURCE.input_dims(),\n",
    "    \"batch_size\": 1,  # DEVE essere 1 per sequenze di lunghezza variabile\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"epochs\": 10,\n",
    "    \"pos_weight\": 1.5,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"loss_function\": \"BCEWithLogitsLoss\",\n",
    "    \"seed\": 42,\n",
    "    \"split_type\": SPLIT_TYPE.value\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c389a25d",
   "metadata": {},
   "source": [
    "# Dataset Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecf4d783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from: c:\\Users\\marco\\Desktop\\Marco\\Programmazione\\C\\EsPoli\\Advanced Machine Learning\\MistakeDetection\\data\\omnivore...\n",
      "Dataset creato: 200 step completi da 8828 secondi\n",
      "\n",
      "=====================================================================================\n",
      "DATASET INFO [TRANSFORMER - STEP-BASED]\n",
      "   Total Steps: 200\n",
      "   Features per second: 1024\n",
      "   Step duration: min=3s, max=184s, avg=44.14s\n",
      "=====================================================================================\n",
      "FULL DATASET       | Tot: 200    | OK: 117   (58.5%) | ERR: 83    (41.5%) | Ratio: 1:1.4\n",
      "-------------------------------------------------------------------------------------\n",
      "TRAIN SET          | Tot: 140    | OK: 83    (59.3%) | ERR: 57    (40.7%) | Ratio: 1:1.5\n",
      "VALIDATION SET     | Tot: 20     | OK: 10    (50.0%) | ERR: 10    (50.0%) | Ratio: 1:1.0\n",
      "TEST SET           | Tot: 40     | OK: 24    (60.0%) | ERR: 16    (40.0%) | Ratio: 1:1.5\n",
      "=====================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dataset.capitain_cook_4d_mlp_dataset import DatasetSource\n",
    "from dataset.capitain_cook_4d_transformer_dataset import CaptainCook4DTransformer_Dataset\n",
    "from dataset.utils import get_transformer_loaders\n",
    "\n",
    "try:\n",
    "    full_dataset = CaptainCook4DTransformer_Dataset(\n",
    "        dataset_source=DATASET_SOURCE,\n",
    "        root_dir=ROOT_DIR\n",
    "    )\n",
    "    train_loader, val_loader, test_loader = get_transformer_loaders(\n",
    "        full_dataset,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        seed=config[\"seed\"],\n",
    "        split_type=SPLIT_TYPE\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Errore: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec45f22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP DATASET ITEM [0]\n",
      "================================================================================\n",
      "Features shape:       torch.Size([39, 1024]) (durata_step, n_features)\n",
      "Step duration:        39 secondi\n",
      "Label:                0 (OK)\n",
      "Step ID:              1\n",
      "Video ID:             1_10\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# V2: quando accedi a dataset[idx], dove idx √® l'indice dello STEP\n",
    "full_dataset.print_item(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e3ffcd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: c:\\Users\\marco\\Desktop\\Marco\\Programmazione\\C\\EsPoli\\Advanced Machine Learning\\MistakeDetection\\data\\omnivore\\1_7_360p.mp4_1s_1s.npz\n",
      "Chiavi presenti nel file: ['arr_0']\n",
      "\n",
      "Array 'arr_0' - shape: (604, 1024), dtype: float32\n",
      "[[ 0.6910985   0.09298898 -0.6608225  ... -0.75679165  1.2401273\n",
      "  -0.5683658 ]\n",
      " [ 0.40254688 -0.4466254  -0.8645446  ... -1.2709565   0.7917245\n",
      "  -0.5052321 ]\n",
      " [ 0.643613   -0.48683766 -0.88651866 ... -1.0358062   0.658605\n",
      "  -0.27201462]]\n"
     ]
    }
   ],
   "source": [
    "from utils.inspect_npz import inspect_npz_from_dataset\n",
    "\n",
    "dataset_folder = DATASET_SOURCE.value\n",
    "npz_filename = \"1_7_360p.mp4_1s_1s.npz\"\n",
    "\n",
    "# Ispezione del file .npz\n",
    "inspect_npz_from_dataset(full_dataset.features_dir(), npz_filename, n_rows=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37a3db8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\marco\\Desktop\\Marco\\Programmazione\\C\\EsPoli\\Advanced Machine Learning\\MistakeDetection\\notebooks\\wandb\\run-20251215_133528-hruuydk0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/s339450-politecnico-di-torino/mistake-detection/runs/hruuydk0' target=\"_blank\">baseline-LSTM-v3-omnivore-step_id</a></strong> to <a href='https://wandb.ai/s339450-politecnico-di-torino/mistake-detection' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/s339450-politecnico-di-torino/mistake-detection' target=\"_blank\">https://wandb.ai/s339450-politecnico-di-torino/mistake-detection</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/s339450-politecnico-di-torino/mistake-detection/runs/hruuydk0' target=\"_blank\">https://wandb.ai/s339450-politecnico-di-torino/mistake-detection/runs/hruuydk0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ W&B Run: baseline-LSTM-v3-omnivore-step_id (ID: hruuydk0)\n"
     ]
    }
   ],
   "source": [
    "# Inizializzazione W&B\n",
    "run = wandb.init(\n",
    "    project=\"mistake-detection\",\n",
    "    name=f\"baseline-LSTM-v3-{DATASET_SOURCE.value}-{SPLIT_TYPE.value}\",\n",
    "    config=config,\n",
    "    tags=[\"baseline\", \"LSTM\", DATASET_SOURCE.value],\n",
    "    notes=f\"Baseline LSTM with {DATASET_SOURCE.value} features for mistake detection and {SPLIT_TYPE.value} split\"\n",
    ")\n",
    "\n",
    "print(f\"üöÄ W&B Run: {run.name} (ID: {run.id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0043d918",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "model = BaselineV3_LSTM(DATASET_SOURCE.input_dims()).to(device)\n",
    "\n",
    "# Watch del modello per tracciare gradienti e parametri\n",
    "wandb.watch(model, log=\"all\", log_freq=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a61f3b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peso classe positiva: 1.5\n"
     ]
    }
   ],
   "source": [
    "lr = config[\"learning_rate\"]\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "\n",
    "# Quanto pesa la classe \"positiva\" = classe \"1\" = classe \"error\":\n",
    "# - CASO 1: rapporto effettivo del dataset\n",
    "#train_pos_weight = train_cnt_0 / train_cnt_1\n",
    "\n",
    "# - CASO 2: rapporto usato nel paper\n",
    "train_pos_weight = config[\"pos_weight\"]\n",
    "\n",
    "print(f\"Peso classe positiva: {train_pos_weight}\")\n",
    "train_pos_weight = torch.tensor([train_pos_weight], device=device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=train_pos_weight)\n",
    "\n",
    "epochs = config[\"epochs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91624240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Train Loss: 0.8320 - Val Loss: 0.8608 - Val Acc: 0.6500 - Val F1: 0.5333 - Val Precision: 0.8000 - Val Recall: 0.4000 - Val AUC: 0.7300\n",
      "‚úÖ Nuovo miglior modello salvato! avg_val_loss: 0.8608\n",
      "Epoch 2/10 - Train Loss: 0.8166 - Val Loss: 0.8446 - Val Acc: 0.7500 - Val F1: 0.7059 - Val Precision: 0.8571 - Val Recall: 0.6000 - Val AUC: 0.7900\n",
      "‚úÖ Nuovo miglior modello salvato! avg_val_loss: 0.8446\n",
      "Epoch 3/10 - Train Loss: 0.7860 - Val Loss: 0.8103 - Val Acc: 0.7000 - Val F1: 0.6667 - Val Precision: 0.7500 - Val Recall: 0.6000 - Val AUC: 0.8200\n",
      "‚úÖ Nuovo miglior modello salvato! avg_val_loss: 0.8103\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m inputs = inputs.to(device)\n\u001b[32m     25\u001b[39m labels = labels.to(device).float()\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m probs, logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# probs: scalare, logits: scalare\u001b[39;00m\n\u001b[32m     29\u001b[39m loss = criterion(logits, labels)\n\u001b[32m     30\u001b[39m total_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marco\\Desktop\\Marco\\Programmazione\\C\\EsPoli\\Advanced Machine Learning\\MistakeDetection\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marco\\Desktop\\Marco\\Programmazione\\C\\EsPoli\\Advanced Machine Learning\\MistakeDetection\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1881\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m   1880\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1881\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1883\u001b[39m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[32m   1884\u001b[39m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[32m   1885\u001b[39m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[32m   1886\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marco\\Desktop\\Marco\\Programmazione\\C\\EsPoli\\Advanced Machine Learning\\MistakeDetection\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842\u001b[39m, in \u001b[36mModule._call_impl.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1840\u001b[39m     hook_result = hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[32m   1841\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1842\u001b[39m     hook_result = \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1844\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1845\u001b[39m     result = hook_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marco\\Desktop\\Marco\\Programmazione\\C\\EsPoli\\Advanced Machine Learning\\MistakeDetection\\venv\\Lib\\site-packages\\wandb\\integration\\torch\\wandb_torch.py:113\u001b[39m, in \u001b[36mTorchHistory.add_log_parameters_hook.<locals>.<lambda>\u001b[39m\u001b[34m(mod, inp, outp)\u001b[39m\n\u001b[32m    110\u001b[39m log_track_params = log_track_init(log_freq)\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    112\u001b[39m     hook = module.register_forward_hook(\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m mod, inp, outp: \u001b[43mparameter_log_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_track_params\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m     )\n\u001b[32m    117\u001b[39m     \u001b[38;5;28mself\u001b[39m._hook_handles[\u001b[33m\"\u001b[39m\u001b[33mparameters/\u001b[39m\u001b[33m\"\u001b[39m + prefix] = hook\n\u001b[32m    118\u001b[39m     module._wandb_hook_names.append(\u001b[33m\"\u001b[39m\u001b[33mparameters/\u001b[39m\u001b[33m\"\u001b[39m + prefix)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marco\\Desktop\\Marco\\Programmazione\\C\\EsPoli\\Advanced Machine Learning\\MistakeDetection\\venv\\Lib\\site-packages\\wandb\\integration\\torch\\wandb_torch.py:108\u001b[39m, in \u001b[36mTorchHistory.add_log_parameters_hook.<locals>.parameter_log_hook\u001b[39m\u001b[34m(module, input_, output, log_track)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    107\u001b[39m     data = parameter\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlog_tensor_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparameters/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marco\\Desktop\\Marco\\Programmazione\\C\\EsPoli\\Advanced Machine Learning\\MistakeDetection\\venv\\Lib\\site-packages\\wandb\\integration\\torch\\wandb_torch.py:229\u001b[39m, in \u001b[36mTorchHistory.log_tensor_stats\u001b[39m\u001b[34m(self, tensor, name)\u001b[39m\n\u001b[32m    227\u001b[39m     bins = torch.Tensor([tmin, tmax])\n\u001b[32m    228\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m     tensor = \u001b[43mflat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhistc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbins\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_num_bins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mtmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mtmax\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    230\u001b[39m     tensor = tensor.cpu().detach().clone()\n\u001b[32m    231\u001b[39m     bins = torch.linspace(tmin, tmax, steps=\u001b[38;5;28mself\u001b[39m._num_bins + \u001b[32m1\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "best_avg_val_loss = np.inf\n",
    "final_val_acc = 0\n",
    "final_val_f1 = 0\n",
    "final_val_precision = 0\n",
    "final_val_recall = 0\n",
    "final_val_auc = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # -------------------------\n",
    "    #        TRAIN\n",
    "    # -------------------------\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    train_preds_list = []\n",
    "    train_targets_list = []\n",
    "    train_probs_list = []\n",
    "\n",
    "    for inputs, labels, _, _ in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device).float()\n",
    "\n",
    "        probs, logits = model(inputs)   # probs: scalare, logits: scalare\n",
    "        \n",
    "        loss = criterion(logits, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Metriche train - ogni batch ha una sola predizione (scalare)\n",
    "        with torch.no_grad():\n",
    "            pred = (probs >= 0.5).long().item()  # converti a scalare Python\n",
    "            \n",
    "            train_preds_list.append(pred)\n",
    "            train_targets_list.append(labels.item())\n",
    "            train_probs_list.append(probs.item())\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # Metriche di training - converti liste a numpy\n",
    "    train_preds = np.array(train_preds_list)\n",
    "    train_targets = np.array(train_targets_list)\n",
    "    train_probs = np.array(train_probs_list)\n",
    "\n",
    "    train_acc = accuracy_score(train_targets, train_preds)\n",
    "    train_f1 = f1_score(train_targets, train_preds, zero_division=0)\n",
    "    train_precision = precision_score(train_targets, train_preds, zero_division=0)\n",
    "    train_recall = recall_score(train_targets, train_preds, zero_division=0)\n",
    "\n",
    "    # AUC train (usa probabilit√†, NON predizioni)\n",
    "    try:\n",
    "        train_auc = roc_auc_score(train_targets, train_probs)\n",
    "    except ValueError:\n",
    "        train_auc = 0.0  # Caso raro con classe mancante nel batch\n",
    "\n",
    "    # -------------------------\n",
    "    #        EVAL\n",
    "    # -------------------------\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, _, _ in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device).float()\n",
    "\n",
    "            probs, logits = model(inputs)   # probs: scalare, logits: scalare\n",
    "\n",
    "            val_loss = criterion(logits, labels)\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "            # metriche - converti a scalari Python\n",
    "            pred = (probs >= 0.5).long().item()\n",
    "            \n",
    "            all_preds.append(pred)\n",
    "            all_targets.append(labels.item())\n",
    "            all_probs.append(probs.item())\n",
    "\n",
    "        # Converti liste a numpy\n",
    "        all_preds = np.array(all_preds)\n",
    "        all_targets = np.array(all_targets)\n",
    "        all_probs = np.array(all_probs)\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_acc = accuracy_score(all_targets, all_preds)\n",
    "        val_f1 = f1_score(all_targets, all_preds, zero_division=0)\n",
    "        val_precision = precision_score(all_targets, all_preds, zero_division=0)\n",
    "        val_recall = recall_score(all_targets, all_preds, zero_division=0)\n",
    "\n",
    "        # AUC validation\n",
    "        try:\n",
    "            val_auc = roc_auc_score(all_targets, all_probs)\n",
    "        except ValueError:\n",
    "            val_auc = 0.0\n",
    "\n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(all_targets, all_preds)\n",
    "\n",
    "        # Log su W&B\n",
    "        wandb.log({\n",
    "            # Training metrics\n",
    "            \"train/loss\": avg_train_loss,\n",
    "            \"train/accuracy\": train_acc,\n",
    "            \"train/f1\": train_f1,\n",
    "            \"train/precision\": train_precision,\n",
    "            \"train/recall\": train_recall,\n",
    "            \"train/auc\": train_auc,\n",
    "\n",
    "            # Validation metrics\n",
    "            \"val/loss\": avg_val_loss,\n",
    "            \"val/accuracy\": val_acc,\n",
    "            \"val/f1\": val_f1,\n",
    "            \"val/precision\": val_precision,\n",
    "            \"val/recall\": val_recall,\n",
    "            \"val/auc\": val_auc,\n",
    "\n",
    "            # Confusion Matrix\n",
    "            \"val/confusion_matrix\": wandb.plot.confusion_matrix(\n",
    "                probs=None,\n",
    "                y_true=all_targets,\n",
    "                preds=all_preds,\n",
    "                class_names=[\"No Error\", \"Error\"]\n",
    "            ),\n",
    "\n",
    "            \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "            \"epoch\": epoch + 1\n",
    "        })\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} \"\n",
    "            f\"- Train Loss: {avg_train_loss:.4f} \"\n",
    "            f\"- Val Loss: {avg_val_loss:.4f} \"\n",
    "            f\"- Val Acc: {val_acc:.4f} \"\n",
    "            f\"- Val F1: {val_f1:.4f} \"\n",
    "            f\"- Val Precision: {val_precision:.4f} \"\n",
    "            f\"- Val Recall: {val_recall:.4f} \"\n",
    "            f\"- Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        # Salvataggio miglior modello\n",
    "        if avg_val_loss < best_avg_val_loss:\n",
    "            best_avg_val_loss = avg_val_loss\n",
    "            final_val_acc = val_acc\n",
    "            final_val_f1 = val_f1\n",
    "            final_val_precision = val_precision\n",
    "            final_val_recall = val_recall\n",
    "            final_val_auc = val_auc\n",
    "            checkpoint_path = os.path.join(ROOT_DIR, \"checkpoints\", f\"best_model_avg_val_loss_{best_avg_val_loss:.4f}.pth\")\n",
    "            os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_f1': val_f1,\n",
    "                'val_acc': val_acc,\n",
    "                'val_auc': val_auc,\n",
    "            }, checkpoint_path)\n",
    "\n",
    "            artifact = wandb.Artifact(\n",
    "                name=f\"model-{run.id}\",\n",
    "                type=\"model\",\n",
    "                description=f\"Best model with avg_val_loss={best_avg_val_loss:.4f}\",\n",
    "                metadata={\n",
    "                    \"epoch\": epoch + 1,\n",
    "                    \"val_f1\": val_f1,\n",
    "                    \"val_acc\": val_acc,\n",
    "                    \"val_auc\": val_auc,\n",
    "                    \"architecture\": config[\"architecture\"]\n",
    "                }\n",
    "            )\n",
    "            artifact.add_file(checkpoint_path)\n",
    "            wandb.log_artifact(artifact)\n",
    "\n",
    "            print(f\"‚úÖ Nuovo miglior modello salvato! avg_val_loss: {best_avg_val_loss:.4f}\")\n",
    "\n",
    "print(\"\\nüéâ Training completato!\")\n",
    "print(f\"Miglior avg_val_loss Score: {best_avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3caf3f5",
   "metadata": {},
   "source": [
    "# Test Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16440fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modello migliore caricato da: c:\\Users\\marco\\Desktop\\Marco\\Programmazione\\C\\EsPoli\\Advanced Machine Learning\\MistakeDetection\\checkpoints\\best_model_avg_val_loss_0.8103.pth\n",
      "   Epoch: 3, Val F1: 0.6667, Val Acc: 0.7000\n",
      "\n",
      "üìä Test Results:\n",
      "Test Loss: 0.7823\n",
      "Test Accuracy: 0.6750\n",
      "Test F1: 0.5517\n",
      "Test Precision: 0.6154\n",
      "Test Recall: 0.5000\n",
      "Test AUC: 0.7448\n",
      "\n",
      "üìà Confusion Matrix:\n",
      "[[19  5]\n",
      " [ 8  8]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Fatal error while uploading data. Some run data will not be synced, but it will still be written to disk. Use `wandb sync` at the end of the run to try uploading.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "#        TEST\n",
    "# -------------------------\n",
    "# Carica il miglior modello salvato\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"‚úÖ Modello migliore caricato da: {checkpoint_path}\")\n",
    "print(f\"   Epoch: {checkpoint['epoch']}, Val F1: {checkpoint['val_f1']:.4f}, Val Acc: {checkpoint['val_acc']:.4f}\")\n",
    "\n",
    "model.eval()\n",
    "total_test_loss = 0\n",
    "test_preds_list = []\n",
    "test_targets_list = []\n",
    "test_probs_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels, _, _ in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device).float()\n",
    "\n",
    "        probs, logits = model(inputs)   # probs: scalare, logits: scalare\n",
    "\n",
    "        test_loss = criterion(logits, labels)\n",
    "        total_test_loss += test_loss.item()\n",
    "\n",
    "        # metriche - converti a scalari Python\n",
    "        pred = (probs >= 0.5).long().item()\n",
    "        \n",
    "        test_preds_list.append(pred)\n",
    "        test_targets_list.append(labels.item())\n",
    "        test_probs_list.append(probs.item())\n",
    "\n",
    "    # Converti liste a numpy\n",
    "    test_preds = np.array(test_preds_list)\n",
    "    test_targets = np.array(test_targets_list)\n",
    "    test_probs = np.array(test_probs_list)\n",
    "\n",
    "    avg_test_loss = total_test_loss / len(test_loader)\n",
    "    test_acc = accuracy_score(test_targets, test_preds)\n",
    "    test_f1 = f1_score(test_targets, test_preds, zero_division=0)\n",
    "    test_precision = precision_score(test_targets, test_preds, zero_division=0)\n",
    "    test_recall = recall_score(test_targets, test_preds, zero_division=0)\n",
    "\n",
    "    # AUC test\n",
    "    try:\n",
    "        test_auc = roc_auc_score(test_targets, test_probs)\n",
    "    except ValueError:\n",
    "        test_auc = 0.0\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm_test = confusion_matrix(test_targets, test_preds)\n",
    "\n",
    "    # Log su W&B\n",
    "    wandb.log({\n",
    "        \"test/loss\": avg_test_loss,\n",
    "        \"test/accuracy\": test_acc,\n",
    "        \"test/f1\": test_f1,\n",
    "        \"test/precision\": test_precision,\n",
    "        \"test/recall\": test_recall,\n",
    "        \"test/auc\": test_auc,\n",
    "\n",
    "        # Confusion Matrix\n",
    "        \"test/confusion_matrix\": wandb.plot.confusion_matrix(\n",
    "            probs=None,\n",
    "            y_true=test_targets,\n",
    "            preds=test_preds,\n",
    "            class_names=[\"No Error\", \"Error\"]\n",
    "        )\n",
    "    })\n",
    "\n",
    "    print(f\"\\nüìä Test Results:\")\n",
    "    print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Test F1: {test_f1:.4f}\")\n",
    "    print(f\"Test Precision: {test_precision:.4f}\")\n",
    "    print(f\"Test Recall: {test_recall:.4f}\")\n",
    "    print(f\"Test AUC: {test_auc:.4f}\")\n",
    "    print(f\"\\nüìà Confusion Matrix:\")\n",
    "    print(cm_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
