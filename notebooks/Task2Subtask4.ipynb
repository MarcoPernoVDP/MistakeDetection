{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00db5f23",
   "metadata": {},
   "source": [
    "# Environement Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d301206b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiente locale rilevato.\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "try:\n",
    "    from google.colab import drive, userdata\n",
    "    IS_COLAB = True\n",
    "except ImportError:\n",
    "    IS_COLAB = False\n",
    "\n",
    "REPO_NAME = 'MistakeDetection'\n",
    "\n",
    "if IS_COLAB:\n",
    "    print(\"‚òÅÔ∏è Colab rilevato.\")\n",
    "    if not os.path.exists('/content/drive'): drive.mount('/content/drive')\n",
    "\n",
    "    GITHUB_USER = 'MarcoPernoVDP'\n",
    "    try:\n",
    "        TOKEN = userdata.get('GITHUB_TOKEN')\n",
    "        REPO_URL = f'https://{TOKEN}@github.com/{GITHUB_USER}/{REPO_NAME}.git'\n",
    "    except:\n",
    "        REPO_URL = f'https://github.com/{GITHUB_USER}/{REPO_NAME}.git'\n",
    "\n",
    "    ROOT_DIR = f'/content/{REPO_NAME}'\n",
    "    if not os.path.exists(ROOT_DIR):\n",
    "        !git clone {REPO_URL}\n",
    "    else:\n",
    "        %cd {ROOT_DIR}\n",
    "        !git pull\n",
    "        %cd /content\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"Ambiente locale rilevato.\")\n",
    "    ROOT_DIR = os.getcwd()\n",
    "    while not os.path.exists(os.path.join(ROOT_DIR, '.gitignore')) and ROOT_DIR != os.path.dirname(ROOT_DIR):\n",
    "        ROOT_DIR = os.path.dirname(ROOT_DIR)\n",
    "\n",
    "if ROOT_DIR not in sys.path:\n",
    "    sys.path.append(ROOT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf787bb",
   "metadata": {},
   "source": [
    "# Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5150539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Progetto in: c:\\Users\\marco\\Desktop\\Marco\\Programmazione\\C\\EsPoli\\Advanced Machine Learning\\MistakeDetection\n",
      "source_path: c:\\Users\\marco\\Desktop\\Marco\\Programmazione\\C\\EsPoli\\Advanced Machine Learning\\MistakeDetection\\_file\n",
      "Setup Dati da: c:\\Users\\marco\\Desktop\\Marco\\Programmazione\\C\\EsPoli\\Advanced Machine Learning\\MistakeDetection\\_file\n",
      "Inizio setup dati...\n",
      "   Sorgente: c:\\Users\\marco\\Desktop\\Marco\\Programmazione\\C\\EsPoli\\Advanced Machine Learning\\MistakeDetection\\_file\n",
      "   Destinazione: c:\\Users\\marco\\Desktop\\Marco\\Programmazione\\C\\EsPoli\\Advanced Machine Learning\\MistakeDetection\\data\n",
      "Copia cartella: annotation_json...\n",
      "Copia cartella: omnivore...\n",
      "Estrazione ZIP: omnivore.zip...\n",
      "‚úÖ Setup completato! Dati pronti in: c:\\Users\\marco\\Desktop\\Marco\\Programmazione\\C\\EsPoli\\Advanced Machine Learning\\MistakeDetection\\data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\marco\\_netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (ConnectionError), entering retry loop.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandB Logged in.\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "from utils import setup_project\n",
    "# Ora puoi passare agli import del modello\n",
    "from dataset.capitain_cook_4d_mlp_dataset import CaptainCook4DMLP_Dataset, DatasetSource\n",
    "from models.BaselineV2_Transformer import BaselineV2_Transformer\n",
    "from dataset.utils import SplitType\n",
    "\n",
    "# Esegue: Setup Dati (unzip/copy), Login WandB, Setup Device\n",
    "device = setup_project.initialize(ROOT_DIR)\n",
    "\n",
    "# Import wandb\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e42308",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a7e990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurazione esperimento\n",
    "DATASET_SOURCE = DatasetSource.HIERO\n",
    "\n",
    "config = {\n",
    "    \"dataset\": \"CaptainCook4D\",\n",
    "    \"feature_extractor\": DATASET_SOURCE.value,\n",
    "    \"input_dim\": DATASET_SOURCE.input_dims(),\n",
    "    \"batch_size\": 1,  # DEVE essere 1 per sequenze di lunghezza variabile\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"epochs\": 30,\n",
    "    \"pos_weight\": 0.75,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"loss_function\": \"BCEWithLogitsLoss\",\n",
    "    \"seed\": 42,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a88f766",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2f0f9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading video embeddings...\n",
      "Loading Hungarian matching results...\n",
      "Loading error annotations...\n",
      "Dataset initialized with 384 samples\n"
     ]
    }
   ],
   "source": [
    "from dataset.dagnn_dataset import DAGNNDataset\n",
    "\n",
    "dataset = DAGNNDataset(\n",
    "    video_embeddings_path=os.path.join(ROOT_DIR, \"data\", \"hiero_all_video_steps.npz\"),\n",
    "    recipe_embeddings_dir=os.path.join(ROOT_DIR, \"data\", \"recipe_text_step_embeddings\"),\n",
    "    hungarian_results_path=os.path.join(ROOT_DIR, \"hungarian_results\", \"hungarian_matching_results.json\"),\n",
    "    annotation_path=os.path.join(ROOT_DIR, \"data\", \"annotation_json\", \"video_level_annotations.json\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c017fcea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_features': tensor([[-0.0229,  0.0128, -0.0307,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.0103, -0.0238,  0.0335,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0272, -0.0033, -0.0309,  ...,  0.1494, -0.1945,  0.2338],\n",
       "         ...,\n",
       "         [ 0.0383, -0.0292, -0.0189,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0742, -0.0109,  0.0238,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0136,  0.0395,  0.0408,  ...,  0.0000,  0.0000,  0.0000]]),\n",
       " 'edge_index': tensor([[ 7, 10, 20,  9,  9,  5,  3, 21, 22,  8, 11, 18, 15, 13, 12,  1,  2,  4,\n",
       "           6, 23, 14,  8, 16, 19,  0,  0,  0,  0,  0,  0,  0,  0, 16, 17],\n",
       "         [ 2,  2,  2,  3,  5,  8,  8,  9,  9, 11, 13, 13, 14, 15, 16, 16, 16, 16,\n",
       "          16, 16, 17, 18, 21, 23,  1,  4,  6,  7, 10, 12, 19, 20, 22, 24]]),\n",
       " 'video_key': '16_39_360p_224.mp4_1s_1s',\n",
       " 'recipe_name': 'scrambledeggs',\n",
       " 'label': tensor(1),\n",
       " 'match_mask': tensor([False, False,  True, False,  True, False, False,  True, False, False,\n",
       "          True,  True, False,  True, False, False, False,  True, False,  True,\n",
       "         False, False, False, False, False]),\n",
       " 'n_nodes': 25,\n",
       " 'metadata': {'edges': [[7, 2],\n",
       "   [10, 2],\n",
       "   [20, 2],\n",
       "   [9, 3],\n",
       "   [9, 5],\n",
       "   [5, 8],\n",
       "   [3, 8],\n",
       "   [21, 9],\n",
       "   [22, 9],\n",
       "   [8, 11],\n",
       "   [11, 13],\n",
       "   [18, 13],\n",
       "   [15, 14],\n",
       "   [13, 15],\n",
       "   [12, 16],\n",
       "   [1, 16],\n",
       "   [2, 16],\n",
       "   [4, 16],\n",
       "   [6, 16],\n",
       "   [23, 16],\n",
       "   [14, 17],\n",
       "   [8, 18],\n",
       "   [16, 21],\n",
       "   [19, 23],\n",
       "   [0, 1],\n",
       "   [0, 4],\n",
       "   [0, 6],\n",
       "   [0, 7],\n",
       "   [0, 10],\n",
       "   [0, 12],\n",
       "   [0, 19],\n",
       "   [0, 20],\n",
       "   [16, 22],\n",
       "   [17, 24]],\n",
       "  'step_ids': ['0',\n",
       "   '1',\n",
       "   '2',\n",
       "   '3',\n",
       "   '4',\n",
       "   '5',\n",
       "   '6',\n",
       "   '7',\n",
       "   '8',\n",
       "   '9',\n",
       "   '10',\n",
       "   '11',\n",
       "   '12',\n",
       "   '13',\n",
       "   '14',\n",
       "   '15',\n",
       "   '16',\n",
       "   '17',\n",
       "   '18',\n",
       "   '19',\n",
       "   '20',\n",
       "   '21',\n",
       "   '22',\n",
       "   '23',\n",
       "   '24'],\n",
       "  'original_steps': {'0': 'START',\n",
       "   '1': 'Chop-Chop 1 tsp cilantro',\n",
       "   '2': 'Whisk-Whisk the egg mixture in the bowl',\n",
       "   '3': 'Add-Add garlic to the pan',\n",
       "   '4': 'Chop-Chop 1 green chilli',\n",
       "   '5': 'Add-Add chilli to the pan',\n",
       "   '6': 'Chop-Chop 1/4 medium onion',\n",
       "   '7': 'add-add 1/3 tsp salt to the bowl',\n",
       "   '8': 'Cook-Cook for 1 minute, mixing everything',\n",
       "   '9': 'Saute-Saute the onions on medium heat until they are soft and translucent',\n",
       "   '10': 'Crack-Crack one egg in the bowl',\n",
       "   '11': 'Add-Add 1/8 tsp of turmeric to the pan',\n",
       "   '12': 'Chop-Chop 1/4 tomato',\n",
       "   '13': 'Cook-Cook covered for 1 minute or until the tomatoes soften',\n",
       "   '14': 'mixing-Keep mixing with a spatula for 3 minutes or until the eggs are almost cooked',\n",
       "   '15': 'pour-Slowly pour the whisked eggs into the pan',\n",
       "   '16': 'Heat-Heat 2 tbsp oil in a heavy-bottomed or nonstick pan on medium heat',\n",
       "   '17': 'Garnish-Garnish with 1 tbsp chopped cilantro and serve',\n",
       "   '18': 'Add-Add tomatoes to the pan',\n",
       "   '19': 'Peel-Peel 2 garlic cloves',\n",
       "   '20': 'add-add 1 tbsp milk to the bowl',\n",
       "   '21': 'add-add 1/3 tsp salt to the pan',\n",
       "   '22': 'add-add chopped onions to the pan',\n",
       "   '23': 'Mince-Mince peeled garlic cloves',\n",
       "   '24': 'END'},\n",
       "  'texts': ['START',\n",
       "   'Chop-Chop 1 tsp cilantro',\n",
       "   'Whisk-Whisk the egg mixture in the bowl',\n",
       "   'Add-Add garlic to the pan',\n",
       "   'Chop-Chop 1 green chilli',\n",
       "   'Add-Add chilli to the pan',\n",
       "   'Chop-Chop 1/4 medium onion',\n",
       "   'add-add 1/3 tsp salt to the bowl',\n",
       "   'Cook-Cook for 1 minute, mixing everything',\n",
       "   'Saute-Saute the onions on medium heat until they are soft and translucent',\n",
       "   'Crack-Crack one egg in the bowl',\n",
       "   'Add-Add 1/8 tsp of turmeric to the pan',\n",
       "   'Chop-Chop 1/4 tomato',\n",
       "   'Cook-Cook covered for 1 minute or until the tomatoes soften',\n",
       "   'mixing-Keep mixing with a spatula for 3 minutes or until the eggs are almost cooked',\n",
       "   'pour-Slowly pour the whisked eggs into the pan',\n",
       "   'Heat-Heat 2 tbsp oil in a heavy-bottomed or nonstick pan on medium heat',\n",
       "   'Garnish-Garnish with 1 tbsp chopped cilantro and serve',\n",
       "   'Add-Add tomatoes to the pan',\n",
       "   'Peel-Peel 2 garlic cloves',\n",
       "   'add-add 1 tbsp milk to the bowl',\n",
       "   'add-add 1/3 tsp salt to the pan',\n",
       "   'add-add chopped onions to the pan',\n",
       "   'Mince-Mince peeled garlic cloves',\n",
       "   'END'],\n",
       "  'num_embeddings': 25,\n",
       "  'embedding_dim': 768}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898401a7",
   "metadata": {},
   "source": [
    "# Leave-One-Out Cross-Validation Setup\n",
    "\n",
    "Raggruppiamo i video per ricetta per fare LOO CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "291b839f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LEAVE-ONE-OUT CROSS-VALIDATION SETUP\n",
      "================================================================================\n",
      "Total videos: 384\n",
      "Total recipes: 24\n",
      "\n",
      "Videos per recipe:\n",
      "  blenderbananapancakes         : 19 videos\n",
      "  breakfastburritos             : 16 videos\n",
      "  broccolistirfry               : 16 videos\n",
      "  buttercorncup                 : 14 videos\n",
      "  capresebruschetta             : 18 videos\n",
      "  cheesepimiento                : 15 videos\n",
      "  coffee                        : 15 videos\n",
      "  cucumberraita                 : 20 videos\n",
      "  dressedupmeatballs            : 16 videos\n",
      "  herbomeletwithfriedtomatoes   : 17 videos\n",
      "  microwaveeggsandwich          : 18 videos\n",
      "  microwavefrenchtoast          : 14 videos\n",
      "  microwavemugpizza             : 13 videos\n",
      "  mugcake                       : 17 videos\n",
      "  panfriedtofu                  : 15 videos\n",
      "  pinwheels                     : 12 videos\n",
      "  ramen                         : 17 videos\n",
      "  sautedmushrooms               : 14 videos\n",
      "  scrambledeggs                 : 16 videos\n",
      "  spicedhotchocolate            : 16 videos\n",
      "  spicytunaavocadowraps         : 18 videos\n",
      "  tomatochutney                 : 15 videos\n",
      "  tomatomozzarellasalad         : 18 videos\n",
      "  zoodles                       : 15 videos\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# Raggruppa i video per ricetta\n",
    "recipe_to_indices = defaultdict(list)\n",
    "for idx in range(len(dataset)):\n",
    "    sample = dataset.samples[idx]\n",
    "    recipe_name = sample['recipe_name']\n",
    "    recipe_to_indices[recipe_name].append(idx)\n",
    "\n",
    "# Ordina le ricette per avere un ordine consistente\n",
    "recipes = sorted(recipe_to_indices.keys())\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"LEAVE-ONE-OUT CROSS-VALIDATION SETUP\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Total videos: {len(dataset)}\")\n",
    "print(f\"Total recipes: {len(recipes)}\")\n",
    "print(f\"\\nVideos per recipe:\")\n",
    "for recipe_name in recipes:\n",
    "    print(f\"  {recipe_name:<30}: {len(recipe_to_indices[recipe_name])} videos\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b96e646",
   "metadata": {},
   "source": [
    "# DAGNN Model\n",
    "\n",
    "Implementiamo la DAGNN per error detection con:\n",
    "- ProjectionLayer per ridurre dimensioni (1536 ‚Üí 256)\n",
    "- Graph Convolutional layers\n",
    "- Global pooling\n",
    "- Binary classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84ebf70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marco\\Desktop\\Marco\\Programmazione\\C\\EsPoli\\Advanced Machine Learning\\MistakeDetection\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ DAGNN model implementato\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "class DAGNN(nn.Module):\n",
    "    \"\"\"\n",
    "    DAGNN model for cooking mistake detection.\n",
    "    \n",
    "    Architecture:\n",
    "    1. ProjectionLayer: [1536] ‚Üí [256] (learnable)\n",
    "    2. GCN layers on the recipe graph\n",
    "    3. Global pooling over nodes\n",
    "    4. Binary classifier (error/no error)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int = 1536,\n",
    "        hidden_dim: int = 256,\n",
    "        num_gnn_layers: int = 3,\n",
    "        dropout: float = 0.3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Feature projection (learnable combination of text + visual)\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "        )\n",
    "        \n",
    "        # GNN layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        for i in range(num_gnn_layers):\n",
    "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Binary classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, 1),  # Binary output\n",
    "        )\n",
    "    \n",
    "    def forward(self, batch_data):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch_data: Batched PyG Data object with:\n",
    "                - x: [total_nodes, 1536] node features\n",
    "                - edge_index: [2, total_edges] edges\n",
    "                - batch: [total_nodes] batch assignment\n",
    "        \n",
    "        Returns:\n",
    "            logits: [batch_size, 1] - logits for binary classification\n",
    "            probs: [batch_size, 1] - probabilities after sigmoid\n",
    "        \"\"\"\n",
    "        x = batch_data.x\n",
    "        edge_index = batch_data.edge_index\n",
    "        batch = batch_data.batch\n",
    "        \n",
    "        # 1. Project features [total_nodes, 1536] ‚Üí [total_nodes, 256]\n",
    "        x = self.projection(x)\n",
    "        \n",
    "        # 2. GNN layers\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        # 3. Global pooling (one embedding per graph)\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, 256]\n",
    "        \n",
    "        # 4. Classification\n",
    "        logits = self.classifier(x)  # [batch_size, 1]\n",
    "        probs = torch.sigmoid(logits)\n",
    "        \n",
    "        return probs, logits\n",
    "\n",
    "print(\"‚úÖ DAGNN model implementato\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760e60d9",
   "metadata": {},
   "source": [
    "# Helper Functions\n",
    "\n",
    "Funzioni per convertire batch in formato PyTorch Geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8a809b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Helper functions implementate\n"
     ]
    }
   ],
   "source": [
    "from dataset.dagnn_dataset import collate_fn\n",
    "\n",
    "def collate_to_pyg(batch_dict):\n",
    "    \"\"\"\n",
    "    Convert batch from DAGNNDataset to PyTorch Geometric format.\n",
    "    \n",
    "    Args:\n",
    "        batch_dict: Dictionary from DAGNN collate_fn\n",
    "    \n",
    "    Returns:\n",
    "        Batched PyG Data object\n",
    "    \"\"\"\n",
    "    graphs = []\n",
    "    \n",
    "    for i in range(len(batch_dict['node_features'])):\n",
    "        graph = Data(\n",
    "            x=batch_dict['node_features'][i],        # [N_i, 1536]\n",
    "            edge_index=batch_dict['edge_index'][i],  # [2, E_i]\n",
    "            y=batch_dict['labels'][i],               # Scalar\n",
    "        )\n",
    "        graphs.append(graph)\n",
    "    \n",
    "    # Batch graphs\n",
    "    batched = Batch.from_data_list(graphs)\n",
    "    \n",
    "    return batched\n",
    "\n",
    "print(\"‚úÖ Helper functions implementate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba74f92",
   "metadata": {},
   "source": [
    "# Leave-One-Out Cross-Validation Training\n",
    "\n",
    "Training con LOO CV: ogni ricetta usata come test set una volta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce621a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (ConnectionError), entering retry loop.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Inizializzazione W&B per l'intero esperimento LOO\n",
    "run = wandb.init(\n",
    "    project=\"mistake-detection\",\n",
    "    name=f\"LOO-Task2Subtask4-DAGNN-{DATASET_SOURCE.value}\",\n",
    "    config={\n",
    "        **config,\n",
    "        \"model\": \"DAGNN\",\n",
    "        \"hidden_dim\": 256,\n",
    "        \"num_gnn_layers\": 3,\n",
    "        \"dropout\": 0.3,\n",
    "        \"batch_size\": 4,  # Per grafi possiamo batchare\n",
    "    },\n",
    "    tags=[\"leave-one-out\", \"Task2Subtask4\", \"DAGNN\", DATASET_SOURCE.value],\n",
    "    notes=f\"Leave-One-Out CV with DAGNN for mistake detection using {DATASET_SOURCE.value} features\"\n",
    ")\n",
    "\n",
    "print(f\"üöÄ W&B Run: {run.name} (ID: {run.id})\")\n",
    "\n",
    "# Aggiorna config\n",
    "config.update({\n",
    "    \"model\": \"DAGNN\",\n",
    "    \"hidden_dim\": 256,\n",
    "    \"num_gnn_layers\": 3,\n",
    "    \"dropout\": 0.3,\n",
    "    \"batch_size\": 4,\n",
    "})\n",
    "\n",
    "# Statistiche per aggregare i risultati di tutti i fold\n",
    "all_fold_results = []\n",
    "\n",
    "# LOO: per ogni ricetta, usala come test set\n",
    "for fold_idx, test_recipe_name in enumerate(recipes):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FOLD {fold_idx + 1}/{len(recipes)} - Testing on Recipe: {test_recipe_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Indici del test set (ricetta corrente)\n",
    "    test_indices = recipe_to_indices[test_recipe_name]\n",
    "    \n",
    "    # Indici del training set (tutte le altre ricette)\n",
    "    train_indices = []\n",
    "    for recipe_name in recipes:\n",
    "        if recipe_name != test_recipe_name:\n",
    "            train_indices.extend(recipe_to_indices[recipe_name])\n",
    "    \n",
    "    print(f\"Train videos: {len(train_indices)} | Test videos: {len(test_indices)}\")\n",
    "    \n",
    "    # Crea i subset\n",
    "    train_dataset = Subset(dataset, train_indices)\n",
    "    test_dataset = Subset(dataset, test_indices)\n",
    "    \n",
    "    # Crea i DataLoader con collate_fn custom\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=config[\"batch_size\"], \n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=config[\"batch_size\"], \n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    # Inizializza un nuovo modello per questo fold\n",
    "    model = DAGNN(\n",
    "        input_dim=1536,\n",
    "        hidden_dim=config[\"hidden_dim\"],\n",
    "        num_gnn_layers=config[\"num_gnn_layers\"],\n",
    "        dropout=config[\"dropout\"]\n",
    "    ).to(device)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "    \n",
    "    # Loss function con pos_weight\n",
    "    train_pos_weight = torch.tensor([config[\"pos_weight\"]], device=device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=train_pos_weight)\n",
    "    \n",
    "    # Training loop per questo fold\n",
    "    best_train_loss = np.inf\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        # TRAIN\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        train_preds_list = []\n",
    "        train_targets_list = []\n",
    "        train_probs_list = []\n",
    "        \n",
    "        for batch_dict in train_loader:\n",
    "            # Converti a PyG format\n",
    "            pyg_batch = collate_to_pyg(batch_dict).to(device)\n",
    "            \n",
    "            # Forward\n",
    "            probs, logits = model(pyg_batch)\n",
    "            \n",
    "            # Loss\n",
    "            labels = pyg_batch.y.float().unsqueeze(1)  # [batch_size, 1]\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            # Backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Metriche\n",
    "            total_loss += loss.item()\n",
    "            preds = (probs >= 0.5).long().cpu().numpy().flatten()\n",
    "            targets = labels.long().cpu().numpy().flatten()\n",
    "            probs_np = probs.detach().cpu().numpy().flatten()\n",
    "            \n",
    "            train_preds_list.extend(preds)\n",
    "            train_targets_list.extend(targets)\n",
    "            train_probs_list.extend(probs_np)\n",
    "        \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        \n",
    "        # Metriche di training\n",
    "        train_preds = np.array(train_preds_list)\n",
    "        train_targets = np.array(train_targets_list)\n",
    "        train_probs = np.array(train_probs_list)\n",
    "        \n",
    "        train_acc = accuracy_score(train_targets, train_preds)\n",
    "        train_f1 = f1_score(train_targets, train_preds, zero_division=0)\n",
    "        \n",
    "        # Log su W&B per questo fold\n",
    "        wandb.log({\n",
    "            f\"fold_{fold_idx+1}/train_loss\": avg_train_loss,\n",
    "            f\"fold_{fold_idx+1}/train_accuracy\": train_acc,\n",
    "            f\"fold_{fold_idx+1}/train_f1\": train_f1,\n",
    "            f\"fold_{fold_idx+1}/epoch\": epoch + 1\n",
    "        })\n",
    "        \n",
    "        print(f\"  Epoch {epoch+1}/{config['epochs']} - Loss: {avg_train_loss:.4f} - Acc: {train_acc:.4f} - F1: {train_f1:.4f}\")\n",
    "        \n",
    "        # Salva il miglior modello per questo fold\n",
    "        if avg_train_loss < best_train_loss:\n",
    "            best_train_loss = avg_train_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "    \n",
    "    # Carica il miglior modello\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    # TEST per questo fold\n",
    "    model.eval()\n",
    "    test_preds_list = []\n",
    "    test_targets_list = []\n",
    "    test_probs_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_dict in test_loader:\n",
    "            # Converti a PyG format\n",
    "            pyg_batch = collate_to_pyg(batch_dict).to(device)\n",
    "            \n",
    "            # Forward\n",
    "            probs, logits = model(pyg_batch)\n",
    "            \n",
    "            # Predictions\n",
    "            labels = pyg_batch.y.long().cpu().numpy()\n",
    "            preds = (probs >= 0.5).long().cpu().numpy().flatten()\n",
    "            probs_np = probs.cpu().numpy().flatten()\n",
    "            \n",
    "            test_preds_list.extend(preds)\n",
    "            test_targets_list.extend(labels)\n",
    "            test_probs_list.extend(probs_np)\n",
    "    \n",
    "    # Metriche di test per questo fold\n",
    "    test_preds = np.array(test_preds_list)\n",
    "    test_targets = np.array(test_targets_list)\n",
    "    test_probs = np.array(test_probs_list)\n",
    "    \n",
    "    test_acc = accuracy_score(test_targets, test_preds)\n",
    "    test_f1 = f1_score(test_targets, test_preds, zero_division=0)\n",
    "    test_precision = precision_score(test_targets, test_preds, zero_division=0)\n",
    "    test_recall = recall_score(test_targets, test_preds, zero_division=0)\n",
    "    \n",
    "    try:\n",
    "        test_auc = roc_auc_score(test_targets, test_probs)\n",
    "    except ValueError:\n",
    "        test_auc = 0.0\n",
    "    \n",
    "    # Salva i risultati di questo fold\n",
    "    fold_result = {\n",
    "        'fold': fold_idx + 1,\n",
    "        'test_recipe': test_recipe_name,\n",
    "        'accuracy': test_acc,\n",
    "        'f1': test_f1,\n",
    "        'precision': test_precision,\n",
    "        'recall': test_recall,\n",
    "        'auc': test_auc,\n",
    "        'test_targets': test_targets,\n",
    "        'test_preds': test_preds\n",
    "    }\n",
    "    all_fold_results.append(fold_result)\n",
    "    \n",
    "    # Log su W&B\n",
    "    wandb.log({\n",
    "        f\"fold_{fold_idx+1}/test_accuracy\": test_acc,\n",
    "        f\"fold_{fold_idx+1}/test_f1\": test_f1,\n",
    "        f\"fold_{fold_idx+1}/test_precision\": test_precision,\n",
    "        f\"fold_{fold_idx+1}/test_recall\": test_recall,\n",
    "        f\"fold_{fold_idx+1}/test_auc\": test_auc,\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n  Test Results for Recipe {test_recipe_name}:\")\n",
    "    print(f\"    Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"    F1: {test_f1:.4f}\")\n",
    "    print(f\"    Precision: {test_precision:.4f}\")\n",
    "    print(f\"    Recall: {test_recall:.4f}\")\n",
    "    print(f\"    AUC: {test_auc:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"üéâ Leave-One-Out Cross-Validation completato!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e8bd58",
   "metadata": {},
   "source": [
    "# Results Analysis\n",
    "\n",
    "Analisi dei risultati aggregati su tutti i fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa915363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcola le statistiche aggregate su tutti i fold\n",
    "accuracies = [r['accuracy'] for r in all_fold_results]\n",
    "f1_scores = [r['f1'] for r in all_fold_results]\n",
    "precisions = [r['precision'] for r in all_fold_results]\n",
    "recalls = [r['recall'] for r in all_fold_results]\n",
    "aucs = [r['auc'] for r in all_fold_results]\n",
    "\n",
    "# Medie e deviazioni standard\n",
    "mean_acc = np.mean(accuracies)\n",
    "std_acc = np.std(accuracies)\n",
    "mean_f1 = np.mean(f1_scores)\n",
    "std_f1 = np.std(f1_scores)\n",
    "mean_precision = np.mean(precisions)\n",
    "std_precision = np.std(precisions)\n",
    "mean_recall = np.mean(recalls)\n",
    "std_recall = np.std(recalls)\n",
    "mean_auc = np.mean(aucs)\n",
    "std_auc = np.std(aucs)\n",
    "\n",
    "# Stampa i risultati aggregati\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"AGGREGATED RESULTS ACROSS ALL FOLDS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nMetric            | Mean      | Std Dev\")\n",
    "print(f\"{'-'*80}\")\n",
    "print(f\"Accuracy          | {mean_acc:.4f}    | {std_acc:.4f}\")\n",
    "print(f\"F1 Score          | {mean_f1:.4f}    | {std_f1:.4f}\")\n",
    "print(f\"Precision         | {mean_precision:.4f}    | {std_precision:.4f}\")\n",
    "print(f\"Recall            | {mean_recall:.4f}    | {std_recall:.4f}\")\n",
    "print(f\"AUC               | {mean_auc:.4f}    | {std_auc:.4f}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Stampa i risultati per ogni fold\n",
    "print(f\"\\nRESULTS PER FOLD:\")\n",
    "print(f\"{'-'*80}\")\n",
    "print(f\"Fold | Recipe                         | Accuracy | F1       | Precision | Recall   | AUC\")\n",
    "print(f\"{'-'*80}\")\n",
    "for result in all_fold_results:\n",
    "    print(f\"{result['fold']:<4} | {result['test_recipe']:<30} | {result['accuracy']:.4f}   | {result['f1']:.4f}   | {result['precision']:.4f}    | {result['recall']:.4f}   | {result['auc']:.4f}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Log delle metriche aggregate su W&B\n",
    "wandb.log({\n",
    "    \"overall/mean_accuracy\": mean_acc,\n",
    "    \"overall/std_accuracy\": std_acc,\n",
    "    \"overall/mean_f1\": mean_f1,\n",
    "    \"overall/std_f1\": std_f1,\n",
    "    \"overall/mean_precision\": mean_precision,\n",
    "    \"overall/std_precision\": std_precision,\n",
    "    \"overall/mean_recall\": mean_recall,\n",
    "    \"overall/std_recall\": std_recall,\n",
    "    \"overall/mean_auc\": mean_auc,\n",
    "    \"overall/std_auc\": std_auc,\n",
    "})\n",
    "\n",
    "# Crea una tabella per W&B con i risultati per fold\n",
    "fold_table_data = []\n",
    "for result in all_fold_results:\n",
    "    fold_table_data.append([\n",
    "        result['fold'],\n",
    "        result['test_recipe'],\n",
    "        result['accuracy'],\n",
    "        result['f1'],\n",
    "        result['precision'],\n",
    "        result['recall'],\n",
    "        result['auc']\n",
    "    ])\n",
    "\n",
    "wandb.log({\n",
    "    \"fold_results_table\": wandb.Table(\n",
    "        columns=[\"Fold\", \"Test Recipe\", \"Accuracy\", \"F1\", \"Precision\", \"Recall\", \"AUC\"],\n",
    "        data=fold_table_data\n",
    "    )\n",
    "})\n",
    "\n",
    "# Confusion Matrix aggregata (concatena tutti i target e le predizioni)\n",
    "all_targets = np.concatenate([r['test_targets'] for r in all_fold_results])\n",
    "all_preds = np.concatenate([r['test_preds'] for r in all_fold_results])\n",
    "\n",
    "cm_overall = confusion_matrix(all_targets, all_preds)\n",
    "print(f\"\\nOVERALL CONFUSION MATRIX:\")\n",
    "print(cm_overall)\n",
    "\n",
    "wandb.log({\n",
    "    \"overall/confusion_matrix\": wandb.plot.confusion_matrix(\n",
    "        probs=None,\n",
    "        y_true=all_targets,\n",
    "        preds=all_preds,\n",
    "        class_names=[\"No Error\", \"Error\"]\n",
    "    )\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3490441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chiudi il run di W&B\n",
    "wandb.finish()\n",
    "print(\"üèÅ W&B run terminato\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
