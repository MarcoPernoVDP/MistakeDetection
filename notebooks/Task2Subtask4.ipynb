{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00db5f23",
   "metadata": {
    "id": "00db5f23"
   },
   "source": [
    "# Environement Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d301206b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d301206b",
    "outputId": "1795f47e-e742-49cb-c3f5-27df02e04491"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚òÅÔ∏è Colab rilevato.\n",
      "Mounted at /content/drive\n",
      "Cloning into 'MistakeDetection'...\n",
      "remote: Enumerating objects: 734, done.\u001b[K\n",
      "remote: Counting objects: 100% (201/201), done.\u001b[K\n",
      "remote: Compressing objects: 100% (154/154), done.\u001b[K\n",
      "remote: Total 734 (delta 121), reused 96 (delta 44), pack-reused 533 (from 1)\u001b[K\n",
      "Receiving objects: 100% (734/734), 86.51 MiB | 29.66 MiB/s, done.\n",
      "Resolving deltas: 100% (393/393), done.\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "try:\n",
    "    from google.colab import drive, userdata\n",
    "    IS_COLAB = True\n",
    "except ImportError:\n",
    "    IS_COLAB = False\n",
    "\n",
    "REPO_NAME = 'MistakeDetection'\n",
    "\n",
    "if IS_COLAB:\n",
    "    print(\"‚òÅÔ∏è Colab rilevato.\")\n",
    "    if not os.path.exists('/content/drive'): drive.mount('/content/drive')\n",
    "\n",
    "    GITHUB_USER = 'MarcoPernoVDP'\n",
    "    try:\n",
    "        TOKEN = userdata.get('GITHUB_TOKEN')\n",
    "        REPO_URL = f'https://{TOKEN}@github.com/{GITHUB_USER}/{REPO_NAME}.git'\n",
    "    except:\n",
    "        REPO_URL = f'https://github.com/{GITHUB_USER}/{REPO_NAME}.git'\n",
    "\n",
    "    ROOT_DIR = f'/content/{REPO_NAME}'\n",
    "    if not os.path.exists(ROOT_DIR):\n",
    "        !git clone {REPO_URL}\n",
    "    else:\n",
    "        %cd {ROOT_DIR}\n",
    "        !git pull\n",
    "        %cd /content\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"Ambiente locale rilevato.\")\n",
    "    ROOT_DIR = os.getcwd()\n",
    "    while not os.path.exists(os.path.join(ROOT_DIR, '.gitignore')) and ROOT_DIR != os.path.dirname(ROOT_DIR):\n",
    "        ROOT_DIR = os.path.dirname(ROOT_DIR)\n",
    "\n",
    "if ROOT_DIR not in sys.path:\n",
    "    sys.path.append(ROOT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf787bb",
   "metadata": {
    "id": "edf787bb"
   },
   "source": [
    "# Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5150539",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b5150539",
    "outputId": "150de30b-a9da-40fd-b480-dcc64978f11f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Progetto in: /content/MistakeDetection\n",
      "source_path: /content/drive/MyDrive/MistakeDetection\n",
      "Setup Dati da: /content/drive/MyDrive/MistakeDetection\n",
      "Inizio setup dati...\n",
      "   Sorgente: /content/drive/MyDrive/MistakeDetection\n",
      "   Destinazione: /content/MistakeDetection/data\n",
      "Estrazione ZIP: omnivore.zip...\n",
      "Copia cartella: annotation_json...\n",
      "Estrazione ZIP: slowfast.zip...\n",
      "Estrazione ZIP: 3dresnet.zip...\n",
      "Estrazione ZIP: x3d.zip...\n",
      "Estrazione ZIP: omnivore_test.zip...\n",
      "Estrazione ZIP: error_recognition_best.zip...\n",
      "Estrazione ZIP: features.zip...\n",
      "Estrazione ZIP: perceptionencoder.zip...\n",
      "Estrazione ZIP: egovlp.zip...\n",
      "Copia cartella: hungarian_results...\n",
      "Copia cartella: task_graphs...\n",
      "Copia cartella: recipe_text_step_embeddings...\n",
      "‚úÖ Setup completato! Dati pronti in: /content/MistakeDetection/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
      "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33ms339450\u001b[0m (\u001b[33ms339450-politecnico-di-torino\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandB Logged in.\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "from utils import setup_project\n",
    "# Ora puoi passare agli import del modello\n",
    "from dataset.capitain_cook_4d_mlp_dataset import CaptainCook4DMLP_Dataset, DatasetSource\n",
    "from models.BaselineV2_Transformer import BaselineV2_Transformer\n",
    "from dataset.utils import SplitType\n",
    "\n",
    "# Esegue: Setup Dati (unzip/copy), Login WandB, Setup Device\n",
    "device = setup_project.initialize(ROOT_DIR)\n",
    "\n",
    "# Import wandb\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e42308",
   "metadata": {
    "id": "a3e42308"
   },
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a7e990",
   "metadata": {
    "id": "a5a7e990"
   },
   "outputs": [],
   "source": [
    "# Configurazione esperimento\n",
    "DATASET_SOURCE = DatasetSource.HIERO\n",
    "\n",
    "config = {\n",
    "    \"dataset\": \"CaptainCook4D\",\n",
    "    \"feature_extractor\": DATASET_SOURCE.value,\n",
    "    \"input_dim\": DATASET_SOURCE.input_dims(),\n",
    "    \"batch_size\": 1,  # DEVE essere 1 per sequenze di lunghezza variabile\n",
    "    \"learning_rate\": 5e-5,  # Ridotto da 1e-4\n",
    "    \"epochs\": 5,  # Ridotto da 10 - early stopping implicito\n",
    "    \"pos_weight\": 0.75,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"loss_function\": \"BCEWithLogitsLoss\",\n",
    "    \"seed\": 42,\n",
    "    \"weight_decay\": 1e-3,  # Aggiunto per regolarizzazione L2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a88f766",
   "metadata": {
    "id": "4a88f766"
   },
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2f0f9e2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a2f0f9e2",
    "outputId": "aee03b0a-ca63-4cde-c970-b0cca7dc50cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading video embeddings...\n",
      "Loading Hungarian matching results...\n",
      "Loading error annotations...\n",
      "Dataset initialized with 384 samples\n"
     ]
    }
   ],
   "source": [
    "from dataset.dagnn_dataset import DAGNNDataset\n",
    "\n",
    "if IS_COLAB:\n",
    "  dataset = DAGNNDataset(\n",
    "      video_embeddings_path=os.path.join(\"/content/drive/MyDrive/MistakeDetection\", \"hiero_all_video_steps.npz\"),\n",
    "      recipe_embeddings_dir=os.path.join(\"/content/drive/MyDrive/MistakeDetection\", \"recipe_text_step_embeddings\"),\n",
    "      hungarian_results_path=os.path.join(\"/content/drive/MyDrive/MistakeDetection\", \"hungarian_results\", \"hungarian_matching_results.json\"),\n",
    "      annotation_path=os.path.join(\"/content/drive/MyDrive/MistakeDetection\", \"annotation_json\", \"video_level_annotations.json\"),\n",
    "  )\n",
    "else:\n",
    "  dataset = DAGNNDataset(\n",
    "      video_embeddings_path=os.path.join(ROOT_DIR, \"data\", \"hiero_all_video_steps.npz\"),\n",
    "      recipe_embeddings_dir=os.path.join(ROOT_DIR, \"data\", \"recipe_text_step_embeddings\"),\n",
    "      hungarian_results_path=os.path.join(ROOT_DIR, \"hungarian_results\", \"hungarian_matching_results.json\"),\n",
    "      annotation_path=os.path.join(ROOT_DIR, \"data\", \"annotation_json\", \"video_level_annotations.json\"),\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c017fcea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c017fcea",
    "outputId": "110cfdd9-da45-4510-ee61-d6923fec3aad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([21, 1536])\n",
      "tensor([[14, 15,  1,  7, 18,  2, 11,  5,  3, 17, 19, 13, 16,  8, 10,  4,  6, 12,\n",
      "          0,  9, 15],\n",
      "        [ 1,  2,  3,  4,  5, 20,  7,  8,  9, 10, 11, 12, 13, 14, 15, 17, 18, 19,\n",
      "         16, 20,  6]])\n"
     ]
    }
   ],
   "source": [
    "print(dataset[1]['node_features'].shape)  # Esempio di accesso ai dati\n",
    "print(dataset[1]['edge_index'])  # Esempio di accesso agli indici degli edge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898401a7",
   "metadata": {
    "id": "898401a7"
   },
   "source": [
    "# Leave-One-Out Cross-Validation Setup\n",
    "\n",
    "Raggruppiamo i video per ricetta per fare LOO CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "291b839f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "291b839f",
    "outputId": "c8f27b52-4cfc-4036-b7d4-bfc36ac3a542"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LEAVE-ONE-OUT CROSS-VALIDATION SETUP\n",
      "================================================================================\n",
      "Total videos: 384\n",
      "Total recipes: 24\n",
      "\n",
      "Videos per recipe:\n",
      "  blenderbananapancakes         : 19 videos\n",
      "  breakfastburritos             : 16 videos\n",
      "  broccolistirfry               : 16 videos\n",
      "  buttercorncup                 : 14 videos\n",
      "  capresebruschetta             : 18 videos\n",
      "  cheesepimiento                : 15 videos\n",
      "  coffee                        : 15 videos\n",
      "  cucumberraita                 : 20 videos\n",
      "  dressedupmeatballs            : 16 videos\n",
      "  herbomeletwithfriedtomatoes   : 17 videos\n",
      "  microwaveeggsandwich          : 18 videos\n",
      "  microwavefrenchtoast          : 14 videos\n",
      "  microwavemugpizza             : 13 videos\n",
      "  mugcake                       : 17 videos\n",
      "  panfriedtofu                  : 15 videos\n",
      "  pinwheels                     : 12 videos\n",
      "  ramen                         : 17 videos\n",
      "  sautedmushrooms               : 14 videos\n",
      "  scrambledeggs                 : 16 videos\n",
      "  spicedhotchocolate            : 16 videos\n",
      "  spicytunaavocadowraps         : 18 videos\n",
      "  tomatochutney                 : 15 videos\n",
      "  tomatomozzarellasalad         : 18 videos\n",
      "  zoodles                       : 15 videos\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# Raggruppa i video per ricetta\n",
    "recipe_to_indices = defaultdict(list)\n",
    "for idx in range(len(dataset)):\n",
    "    sample = dataset.samples[idx]\n",
    "    recipe_name = sample['recipe_name']\n",
    "    recipe_to_indices[recipe_name].append(idx)\n",
    "\n",
    "# Ordina le ricette per avere un ordine consistente\n",
    "recipes = sorted(recipe_to_indices.keys())\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"LEAVE-ONE-OUT CROSS-VALIDATION SETUP\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Total videos: {len(dataset)}\")\n",
    "print(f\"Total recipes: {len(recipes)}\")\n",
    "print(f\"\\nVideos per recipe:\")\n",
    "for recipe_name in recipes:\n",
    "    print(f\"  {recipe_name:<30}: {len(recipe_to_indices[recipe_name])} videos\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b96e646",
   "metadata": {
    "id": "6b96e646"
   },
   "source": [
    "# DAGNN Model\n",
    "\n",
    "Implementiamo la DAGNN per error detection con:\n",
    "- ProjectionLayer per ridurre dimensioni (1536 ‚Üí 256)\n",
    "- Graph Convolutional layers\n",
    "- Global pooling\n",
    "- Binary classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "P08LwiQCC_2u",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "P08LwiQCC_2u",
    "outputId": "5f9b0384-b555-46e6-d33c-81d165cc3afe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch_geometric\n",
      "  Downloading torch_geometric-2.7.0-py3-none-any.whl.metadata (63 kB)\n",
      "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/63.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.13.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2025.3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.1.6)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.0.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (5.9.5)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.2.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.32.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.22.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch_geometric) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2025.11.12)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch_geometric) (4.15.0)\n",
      "Downloading torch_geometric-2.7.0-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch_geometric\n",
      "Successfully installed torch_geometric-2.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ebf70e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "84ebf70e",
    "outputId": "8b9a921c-2ce2-4de2-c691-847c8dd06758"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ DAGNN model implementato\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "class DAGNN(nn.Module):\n",
    "    \"\"\"\n",
    "    DAGNN model for cooking mistake detection.\n",
    "\n",
    "    Architecture:\n",
    "    1. ProjectionLayer: [1536] ‚Üí [128] (ridotto da 256)\n",
    "    2. 2 GCN layers (ridotto da 3)\n",
    "    3. Global pooling over nodes\n",
    "    4. Binary classifier (error/no error)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int = 1536,\n",
    "        hidden_dim: int = 128,  # Ridotto da 256\n",
    "        num_gnn_layers: int = 2,  # Ridotto da 3\n",
    "        dropout: float = 0.5,  # Aumentato da 0.3\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Feature projection (learnable combination of text + visual)\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Dropout(dropout),  # Aggiunto dropout dopo projection\n",
    "        )\n",
    "\n",
    "        # GNN layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()  # Aggiunto Layer Norm per ogni GNN layer\n",
    "        for i in range(num_gnn_layers):\n",
    "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
    "            self.norms.append(nn.LayerNorm(hidden_dim))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Binary classifier - pi√π semplice\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, 1),  # Binary output\n",
    "        )\n",
    "\n",
    "    def forward(self, batch_data):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch_data: Batched PyG Data object with:\n",
    "                - x: [total_nodes, 1536] node features\n",
    "                - edge_index: [2, total_edges] edges\n",
    "                - batch: [total_nodes] batch assignment\n",
    "\n",
    "        Returns:\n",
    "            logits: [batch_size, 1] - logits for binary classification\n",
    "            probs: [batch_size, 1] - probabilities after sigmoid\n",
    "        \"\"\"\n",
    "        x = batch_data.x\n",
    "        edge_index = batch_data.edge_index\n",
    "        batch = batch_data.batch\n",
    "\n",
    "        # 1. Project features [total_nodes, 1536] ‚Üí [total_nodes, 128]\n",
    "        x = self.projection(x)\n",
    "\n",
    "        # 2. GNN layers with normalization\n",
    "        for conv, norm in zip(self.convs, self.norms):\n",
    "            x = conv(x, edge_index)\n",
    "            x = norm(x)  # Layer norm per stabilit√†\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        # 3. Global pooling (one embedding per graph)\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, 128]\n",
    "\n",
    "        # 4. Classification\n",
    "        logits = self.classifier(x)  # [batch_size, 1]\n",
    "        probs = torch.sigmoid(logits)\n",
    "\n",
    "        return probs, logits\n",
    "\n",
    "print(\"‚úÖ DAGNN model implementato con regolarizzazione aumentata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760e60d9",
   "metadata": {
    "id": "760e60d9"
   },
   "source": [
    "# Helper Functions\n",
    "\n",
    "Funzioni per convertire batch in formato PyTorch Geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8a809b4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e8a809b4",
    "outputId": "cb843419-6cb4-4d47-88a1-03c8e83049a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Helper functions aggiornate con validazione edge_index\n"
     ]
    }
   ],
   "source": [
    "from dataset.dagnn_dataset import collate_fn\n",
    "\n",
    "def collate_to_pyg(batch_dict):\n",
    "    \"\"\"\n",
    "    Convert batch from DAGNNDataset to PyTorch Geometric format.\n",
    "\n",
    "    Args:\n",
    "        batch_dict: Dictionary from DAGNN collate_fn\n",
    "\n",
    "    Returns:\n",
    "        Batched PyG Data object\n",
    "    \"\"\"\n",
    "    graphs = []\n",
    "\n",
    "    for i in range(len(batch_dict['node_features'])):\n",
    "        # Verifica che edge_index sia nel formato corretto\n",
    "        edge_index = batch_dict['edge_index'][i]\n",
    "        num_nodes = batch_dict['node_features'][i].shape[0]\n",
    "\n",
    "        # Debug: controlla che gli indici siano validi\n",
    "        if edge_index.numel() > 0:\n",
    "            max_idx = edge_index.max().item()\n",
    "            if max_idx >= num_nodes:\n",
    "                print(f\"‚ö†Ô∏è Warning: edge_index contiene indice {max_idx} ma ci sono solo {num_nodes} nodi\")\n",
    "                # Filtra edge invalidi\n",
    "                valid_edges = (edge_index[0] < num_nodes) & (edge_index[1] < num_nodes)\n",
    "                edge_index = edge_index[:, valid_edges]\n",
    "\n",
    "        graph = Data(\n",
    "            x=batch_dict['node_features'][i],        # [N_i, 1536]\n",
    "            edge_index=edge_index,                    # [2, E_i]\n",
    "            y=batch_dict['labels'][i],               # Scalar\n",
    "        )\n",
    "        graphs.append(graph)\n",
    "\n",
    "    # Batch graphs\n",
    "    batched = Batch.from_data_list(graphs)\n",
    "\n",
    "    return batched\n",
    "\n",
    "print(\"‚úÖ Helper functions aggiornate con validazione edge_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba74f92",
   "metadata": {
    "id": "fba74f92"
   },
   "source": [
    "# Leave-One-Out Cross-Validation Training\n",
    "\n",
    "Training con LOO CV: ogni ricetta usata come test set una volta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce621a33",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ce621a33",
    "outputId": "4f44487b-4d1a-44f8-b1a3-7347adbb7535"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>fold_1/epoch</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà</td></tr><tr><td>fold_1/test_accuracy</td><td>‚ñÅ</td></tr><tr><td>fold_1/test_auc</td><td>‚ñÅ</td></tr><tr><td>fold_1/test_f1</td><td>‚ñÅ</td></tr><tr><td>fold_1/test_precision</td><td>‚ñÅ</td></tr><tr><td>fold_1/test_recall</td><td>‚ñÅ</td></tr><tr><td>fold_1/train_accuracy</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà</td></tr><tr><td>fold_1/train_f1</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà</td></tr><tr><td>fold_1/train_loss</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÅ</td></tr><tr><td>fold_2/epoch</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà</td></tr><tr><td>+21</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>fold_1/epoch</td><td>13</td></tr><tr><td>fold_1/test_accuracy</td><td>0.68421</td></tr><tr><td>fold_1/test_auc</td><td>0.67857</td></tr><tr><td>fold_1/test_f1</td><td>0.75</td></tr><tr><td>fold_1/test_precision</td><td>0.75</td></tr><tr><td>fold_1/test_recall</td><td>0.75</td></tr><tr><td>fold_1/train_accuracy</td><td>0.93699</td></tr><tr><td>fold_1/train_f1</td><td>0.94458</td></tr><tr><td>fold_1/train_loss</td><td>0.1597</td></tr><tr><td>fold_2/epoch</td><td>13</td></tr><tr><td>+21</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LOO-Task2Subtask4-DAGNN-hiero</strong> at: <a href='https://wandb.ai/s339450-politecnico-di-torino/mistake-detection/runs/forzmnao' target=\"_blank\">https://wandb.ai/s339450-politecnico-di-torino/mistake-detection/runs/forzmnao</a><br> View project at: <a href='https://wandb.ai/s339450-politecnico-di-torino/mistake-detection' target=\"_blank\">https://wandb.ai/s339450-politecnico-di-torino/mistake-detection</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251222_124126-forzmnao/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20251222_124330-ekce6ygy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/s339450-politecnico-di-torino/mistake-detection/runs/ekce6ygy' target=\"_blank\">LOO-Task2Subtask4-DAGNN-hiero</a></strong> to <a href='https://wandb.ai/s339450-politecnico-di-torino/mistake-detection' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/s339450-politecnico-di-torino/mistake-detection' target=\"_blank\">https://wandb.ai/s339450-politecnico-di-torino/mistake-detection</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/s339450-politecnico-di-torino/mistake-detection/runs/ekce6ygy' target=\"_blank\">https://wandb.ai/s339450-politecnico-di-torino/mistake-detection/runs/ekce6ygy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ W&B Run: LOO-Task2Subtask4-DAGNN-hiero (ID: ekce6ygy)\n",
      "\n",
      "================================================================================\n",
      "FOLD 1/24 - Testing on Recipe: blenderbananapancakes\n",
      "================================================================================\n",
      "Train videos: 365 | Test videos: 19\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 1/10 - Loss: 0.5988 - Acc: 0.4904 - F1: 0.5105\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 2/10 - Loss: 0.5978 - Acc: 0.4767 - F1: 0.4796\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 3/10 - Loss: 0.5918 - Acc: 0.5452 - F1: 0.5608\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 4/10 - Loss: 0.5846 - Acc: 0.5808 - F1: 0.6467\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 5/10 - Loss: 0.5726 - Acc: 0.6027 - F1: 0.6697\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 6/10 - Loss: 0.5440 - Acc: 0.6329 - F1: 0.6599\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 7/10 - Loss: 0.5100 - Acc: 0.6740 - F1: 0.7173\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 8/10 - Loss: 0.4472 - Acc: 0.7452 - F1: 0.7726\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 9/10 - Loss: 0.3718 - Acc: 0.7973 - F1: 0.8204\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 10/10 - Loss: 0.2627 - Acc: 0.8795 - F1: 0.8927\n",
      "\n",
      "  Test Results for Recipe blenderbananapancakes:\n",
      "    Accuracy: 0.4211\n",
      "    F1: 0.2667\n",
      "    Precision: 0.6667\n",
      "    Recall: 0.1667\n",
      "    AUC: 0.5833\n",
      "\n",
      "================================================================================\n",
      "FOLD 2/24 - Testing on Recipe: breakfastburritos\n",
      "================================================================================\n",
      "Train videos: 368 | Test videos: 16\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 1/10 - Loss: 0.6000 - Acc: 0.5136 - F1: 0.5788\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 2/10 - Loss: 0.5985 - Acc: 0.4674 - F1: 0.4096\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 3/10 - Loss: 0.5947 - Acc: 0.5000 - F1: 0.5106\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 4/10 - Loss: 0.5876 - Acc: 0.5761 - F1: 0.6158\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 5/10 - Loss: 0.5649 - Acc: 0.6277 - F1: 0.6807\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 6/10 - Loss: 0.5365 - Acc: 0.6495 - F1: 0.6861\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 7/10 - Loss: 0.4799 - Acc: 0.7092 - F1: 0.7529\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 8/10 - Loss: 0.4006 - Acc: 0.7717 - F1: 0.8028\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 9/10 - Loss: 0.3286 - Acc: 0.8261 - F1: 0.8469\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 10/10 - Loss: 0.2165 - Acc: 0.8995 - F1: 0.9113\n",
      "\n",
      "  Test Results for Recipe breakfastburritos:\n",
      "    Accuracy: 0.5625\n",
      "    F1: 0.6316\n",
      "    Precision: 0.6667\n",
      "    Recall: 0.6000\n",
      "    AUC: 0.4500\n",
      "\n",
      "================================================================================\n",
      "FOLD 3/24 - Testing on Recipe: broccolistirfry\n",
      "================================================================================\n",
      "Train videos: 368 | Test videos: 16\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 1/10 - Loss: 0.6000 - Acc: 0.5000 - F1: 0.5760\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 2/10 - Loss: 0.5968 - Acc: 0.4755 - F1: 0.5013\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 3/10 - Loss: 0.5878 - Acc: 0.5625 - F1: 0.5707\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 4/10 - Loss: 0.5847 - Acc: 0.5272 - F1: 0.5628\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 5/10 - Loss: 0.5754 - Acc: 0.6141 - F1: 0.6502\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 6/10 - Loss: 0.5546 - Acc: 0.6250 - F1: 0.6714\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 7/10 - Loss: 0.5020 - Acc: 0.7065 - F1: 0.7568\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 8/10 - Loss: 0.4355 - Acc: 0.7582 - F1: 0.7906\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 9/10 - Loss: 0.3558 - Acc: 0.7880 - F1: 0.8160\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 10/10 - Loss: 0.2480 - Acc: 0.8777 - F1: 0.8956\n",
      "\n",
      "  Test Results for Recipe broccolistirfry:\n",
      "    Accuracy: 0.5000\n",
      "    F1: 0.2000\n",
      "    Precision: 0.2500\n",
      "    Recall: 0.1667\n",
      "    AUC: 0.4167\n",
      "\n",
      "================================================================================\n",
      "FOLD 4/24 - Testing on Recipe: buttercorncup\n",
      "================================================================================\n",
      "Train videos: 370 | Test videos: 14\n",
      "  Epoch 1/10 - Loss: 0.6014 - Acc: 0.4973 - F1: 0.5991\n",
      "  Epoch 2/10 - Loss: 0.5978 - Acc: 0.5459 - F1: 0.6515\n",
      "  Epoch 3/10 - Loss: 0.5931 - Acc: 0.4946 - F1: 0.5066\n",
      "  Epoch 4/10 - Loss: 0.5843 - Acc: 0.5432 - F1: 0.5678\n",
      "  Epoch 5/10 - Loss: 0.5649 - Acc: 0.6135 - F1: 0.6416\n",
      "  Epoch 6/10 - Loss: 0.5135 - Acc: 0.6703 - F1: 0.7136\n",
      "  Epoch 7/10 - Loss: 0.4739 - Acc: 0.7216 - F1: 0.7542\n",
      "  Epoch 8/10 - Loss: 0.3736 - Acc: 0.8000 - F1: 0.8255\n",
      "  Epoch 9/10 - Loss: 0.2998 - Acc: 0.8405 - F1: 0.8618\n",
      "  Epoch 10/10 - Loss: 0.2016 - Acc: 0.9000 - F1: 0.9125\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "\n",
      "  Test Results for Recipe buttercorncup:\n",
      "    Accuracy: 0.6429\n",
      "    F1: 0.7826\n",
      "    Precision: 0.6429\n",
      "    Recall: 1.0000\n",
      "    AUC: 0.4889\n",
      "\n",
      "================================================================================\n",
      "FOLD 5/24 - Testing on Recipe: capresebruschetta\n",
      "================================================================================\n",
      "Train videos: 366 | Test videos: 18\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 1/10 - Loss: 0.6004 - Acc: 0.4945 - F1: 0.5293\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 2/10 - Loss: 0.5970 - Acc: 0.4645 - F1: 0.4815\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 3/10 - Loss: 0.5911 - Acc: 0.5328 - F1: 0.5693\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 4/10 - Loss: 0.5850 - Acc: 0.5792 - F1: 0.6262\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 5/10 - Loss: 0.5499 - Acc: 0.6366 - F1: 0.6616\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 6/10 - Loss: 0.5129 - Acc: 0.6448 - F1: 0.6860\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 7/10 - Loss: 0.4632 - Acc: 0.7322 - F1: 0.7621\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 8/10 - Loss: 0.3745 - Acc: 0.7923 - F1: 0.8146\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 9/10 - Loss: 0.2867 - Acc: 0.8525 - F1: 0.8714\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 10/10 - Loss: 0.2151 - Acc: 0.9126 - F1: 0.9227\n",
      "\n",
      "  Test Results for Recipe capresebruschetta:\n",
      "    Accuracy: 0.3889\n",
      "    F1: 0.4211\n",
      "    Precision: 0.5714\n",
      "    Recall: 0.3333\n",
      "    AUC: 0.3472\n",
      "\n",
      "================================================================================\n",
      "FOLD 6/24 - Testing on Recipe: cheesepimiento\n",
      "================================================================================\n",
      "Train videos: 369 | Test videos: 15\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 1/10 - Loss: 0.6022 - Acc: 0.4336 - F1: 0.3907\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 2/10 - Loss: 0.5963 - Acc: 0.5014 - F1: 0.5132\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 3/10 - Loss: 0.5946 - Acc: 0.5041 - F1: 0.5222\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 4/10 - Loss: 0.5910 - Acc: 0.5556 - F1: 0.5684\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 5/10 - Loss: 0.5896 - Acc: 0.5257 - F1: 0.5383\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 6/10 - Loss: 0.5650 - Acc: 0.5962 - F1: 0.6375\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 7/10 - Loss: 0.5423 - Acc: 0.6450 - F1: 0.6749\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 8/10 - Loss: 0.4895 - Acc: 0.7046 - F1: 0.7348\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 9/10 - Loss: 0.4095 - Acc: 0.7724 - F1: 0.7981\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 10/10 - Loss: 0.3154 - Acc: 0.8320 - F1: 0.8524\n",
      "\n",
      "  Test Results for Recipe cheesepimiento:\n",
      "    Accuracy: 0.5333\n",
      "    F1: 0.6316\n",
      "    Precision: 0.6000\n",
      "    Recall: 0.6667\n",
      "    AUC: 0.5556\n",
      "\n",
      "================================================================================\n",
      "FOLD 7/24 - Testing on Recipe: coffee\n",
      "================================================================================\n",
      "Train videos: 369 | Test videos: 15\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 1/10 - Loss: 0.5893 - Acc: 0.5745 - F1: 0.6609\n",
      "Debugging sample: 13_5_360p_224.mp4_1s_1s buttercorncup\n",
      "  Epoch 2/10 - Loss: 0.5994 - Acc: 0.4472 - F1: 0.3964\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-4108638224.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_dict\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;31m# Converti a PyG format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mpyg_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollate_to_pyg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;31m# Forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-3325885390.py\u001b[0m in \u001b[0;36mcollate_to_pyg\u001b[0;34m(batch_dict)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# Batch graphs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mbatched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_data_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraphs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch_geometric/data/batch.py\u001b[0m in \u001b[0;36mfrom_data_list\u001b[0;34m(cls, data_list, follow_batch, exclude_keys)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mWill\u001b[0m \u001b[0mexclude\u001b[0m \u001b[0many\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mexclude_keys\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \"\"\"\n\u001b[0;32m---> 97\u001b[0;31m         batch, slice_dict, inc_dict = collate(\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mdata_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch_geometric/data/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(cls, data_list, increment, add_batch, follow_batch, exclude_keys)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;31m# Collate attributes into a unified representation:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             value, slices, incs = _collate(attr, values, data_list, stores,\n\u001b[0m\u001b[1;32m    110\u001b[0m                                            increment)\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch_geometric/data/collate.py\u001b[0m in \u001b[0;36m_collate\u001b[0;34m(key, values, data_list, stores, increment)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0msizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat_dim\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mincrement\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mincs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_incs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch_geometric/utils/functions.py\u001b[0m in \u001b[0;36mcumsum\u001b[0;34m(x, dim)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Inizializzazione W&B per l'intero esperimento LOO\n",
    "run = wandb.init(\n",
    "    project=\"mistake-detection\",\n",
    "    name=f\"LOO-Task2Subtask4-DAGNN-{DATASET_SOURCE.value}-regularized\",\n",
    "    config={\n",
    "        **config,\n",
    "        \"model\": \"DAGNN\",\n",
    "        \"hidden_dim\": 128,  # Aggiornato\n",
    "        \"num_gnn_layers\": 2,  # Aggiornato\n",
    "        \"dropout\": 0.5,  # Aggiornato\n",
    "    },\n",
    "    tags=[\"leave-one-out\", \"Task2Subtask4\", \"DAGNN\", DATASET_SOURCE.value, \"regularized\"],\n",
    "    notes=f\"Leave-One-Out CV with DAGNN (regularized) for mistake detection using {DATASET_SOURCE.value} features\"\n",
    ")\n",
    "\n",
    "print(f\"üöÄ W&B Run: {run.name} (ID: {run.id})\")\n",
    "\n",
    "# Aggiorna config\n",
    "config.update({\n",
    "    \"model\": \"DAGNN\",\n",
    "    \"hidden_dim\": 128,\n",
    "    \"num_gnn_layers\": 2,\n",
    "    \"dropout\": 0.5,\n",
    "})\n",
    "\n",
    "# Statistiche per aggregare i risultati di tutti i fold\n",
    "all_fold_results = []\n",
    "\n",
    "# LOO: per ogni ricetta, usala come test set\n",
    "for fold_idx, test_recipe_name in enumerate(recipes):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FOLD {fold_idx + 1}/{len(recipes)} - Testing on Recipe: {test_recipe_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    # Indici del test set (ricetta corrente)\n",
    "    test_indices = recipe_to_indices[test_recipe_name]\n",
    "\n",
    "    # Indici del training set (tutte le altre ricette)\n",
    "    train_indices = []\n",
    "    for recipe_name in recipes:\n",
    "        if recipe_name != test_recipe_name:\n",
    "            train_indices.extend(recipe_to_indices[recipe_name])\n",
    "\n",
    "    print(f\"Train videos: {len(train_indices)} | Test videos: {len(test_indices)}\")\n",
    "\n",
    "    # Crea i subset\n",
    "    train_dataset = Subset(dataset, train_indices)\n",
    "    test_dataset = Subset(dataset, test_indices)\n",
    "\n",
    "    # Crea i DataLoader con collate_fn custom\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    # Inizializza un nuovo modello per questo fold\n",
    "    model = DAGNN(\n",
    "        input_dim=1536,\n",
    "        hidden_dim=config[\"hidden_dim\"],\n",
    "        num_gnn_layers=config[\"num_gnn_layers\"],\n",
    "        dropout=config[\"dropout\"]\n",
    "    ).to(device)\n",
    "\n",
    "    # Optimizer con weight decay\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), \n",
    "        lr=config[\"learning_rate\"],\n",
    "        weight_decay=config.get(\"weight_decay\", 1e-3)  # L2 regularization\n",
    "    )\n",
    "\n",
    "    # Loss function con pos_weight\n",
    "    train_pos_weight = torch.tensor([config[\"pos_weight\"]], device=device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=train_pos_weight)\n",
    "\n",
    "    # Training loop per questo fold\n",
    "    best_train_loss = np.inf\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        # TRAIN\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        train_preds_list = []\n",
    "        train_targets_list = []\n",
    "        train_probs_list = []\n",
    "\n",
    "        for batch_dict in train_loader:\n",
    "            # Converti a PyG format\n",
    "            pyg_batch = collate_to_pyg(batch_dict).to(device)\n",
    "\n",
    "            # Forward\n",
    "            probs, logits = model(pyg_batch)\n",
    "\n",
    "            # Loss\n",
    "            labels = pyg_batch.y.float().unsqueeze(1)  # [batch_size, 1]\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            # Backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping per stabilit√†\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "            # Metriche\n",
    "            total_loss += loss.item()\n",
    "            preds = (probs >= 0.5).long().cpu().numpy().flatten()\n",
    "            targets = labels.long().cpu().numpy().flatten()\n",
    "            probs_np = probs.detach().cpu().numpy().flatten()\n",
    "\n",
    "            train_preds_list.extend(preds)\n",
    "            train_targets_list.extend(targets)\n",
    "            train_probs_list.extend(probs_np)\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # Metriche di training\n",
    "        train_preds = np.array(train_preds_list)\n",
    "        train_targets = np.array(train_targets_list)\n",
    "        train_probs = np.array(train_probs_list)\n",
    "\n",
    "        train_acc = accuracy_score(train_targets, train_preds)\n",
    "        train_f1 = f1_score(train_targets, train_preds, zero_division=0)\n",
    "\n",
    "        # Log su W&B per questo fold\n",
    "        wandb.log({\n",
    "            f\"fold_{fold_idx+1}/train_loss\": avg_train_loss,\n",
    "            f\"fold_{fold_idx+1}/train_accuracy\": train_acc,\n",
    "            f\"fold_{fold_idx+1}/train_f1\": train_f1,\n",
    "            f\"fold_{fold_idx+1}/epoch\": epoch + 1\n",
    "        })\n",
    "\n",
    "        print(f\"  Epoch {epoch+1}/{config['epochs']} - Loss: {avg_train_loss:.4f} - Acc: {train_acc:.4f} - F1: {train_f1:.4f}\")\n",
    "\n",
    "        # Salva il miglior modello per questo fold\n",
    "        if avg_train_loss < best_train_loss:\n",
    "            best_train_loss = avg_train_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "\n",
    "    # Carica il miglior modello\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    # TEST per questo fold\n",
    "    model.eval()\n",
    "    test_preds_list = []\n",
    "    test_targets_list = []\n",
    "    test_probs_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_dict in test_loader:\n",
    "            # Converti a PyG format\n",
    "            pyg_batch = collate_to_pyg(batch_dict).to(device)\n",
    "\n",
    "            # Forward\n",
    "            probs, logits = model(pyg_batch)\n",
    "\n",
    "            # Predictions\n",
    "            labels = pyg_batch.y.long().cpu().numpy()\n",
    "            preds = (probs >= 0.5).long().cpu().numpy().flatten()\n",
    "            probs_np = probs.cpu().numpy().flatten()\n",
    "\n",
    "            test_preds_list.extend(preds)\n",
    "            test_targets_list.extend(labels)\n",
    "            test_probs_list.extend(probs_np)\n",
    "\n",
    "    # Metriche di test per questo fold\n",
    "    test_preds = np.array(test_preds_list)\n",
    "    test_targets = np.array(test_targets_list)\n",
    "    test_probs = np.array(test_probs_list)\n",
    "\n",
    "    test_acc = accuracy_score(test_targets, test_preds)\n",
    "    test_f1 = f1_score(test_targets, test_preds, zero_division=0)\n",
    "    test_precision = precision_score(test_targets, test_preds, zero_division=0)\n",
    "    test_recall = recall_score(test_targets, test_preds, zero_division=0)\n",
    "\n",
    "    try:\n",
    "        test_auc = roc_auc_score(test_targets, test_probs)\n",
    "    except ValueError:\n",
    "        test_auc = 0.0\n",
    "\n",
    "    # Salva i risultati di questo fold\n",
    "    fold_result = {\n",
    "        'fold': fold_idx + 1,\n",
    "        'test_recipe': test_recipe_name,\n",
    "        'accuracy': test_acc,\n",
    "        'f1': test_f1,\n",
    "        'precision': test_precision,\n",
    "        'recall': test_recall,\n",
    "        'auc': test_auc,\n",
    "        'test_targets': test_targets,\n",
    "        'test_preds': test_preds\n",
    "    }\n",
    "    all_fold_results.append(fold_result)\n",
    "\n",
    "    # Log su W&B\n",
    "    wandb.log({\n",
    "        f\"fold_{fold_idx+1}/test_accuracy\": test_acc,\n",
    "        f\"fold_{fold_idx+1}/test_f1\": test_f1,\n",
    "        f\"fold_{fold_idx+1}/test_precision\": test_precision,\n",
    "        f\"fold_{fold_idx+1}/test_recall\": test_recall,\n",
    "        f\"fold_{fold_idx+1}/test_auc\": test_auc,\n",
    "    })\n",
    "\n",
    "    print(f\"\\n  Test Results for Recipe {test_recipe_name}:\")\n",
    "    print(f\"    Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"    F1: {test_f1:.4f}\")\n",
    "    print(f\"    Precision: {test_precision:.4f}\")\n",
    "    print(f\"    Recall: {test_recall:.4f}\")\n",
    "    print(f\"    AUC: {test_auc:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"üéâ Leave-One-Out Cross-Validation completato!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e8bd58",
   "metadata": {
    "id": "56e8bd58"
   },
   "source": [
    "# Results Analysis\n",
    "\n",
    "Analisi dei risultati aggregati su tutti i fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa915363",
   "metadata": {
    "id": "aa915363"
   },
   "outputs": [],
   "source": [
    "# Calcola le statistiche aggregate su tutti i fold\n",
    "accuracies = [r['accuracy'] for r in all_fold_results]\n",
    "f1_scores = [r['f1'] for r in all_fold_results]\n",
    "precisions = [r['precision'] for r in all_fold_results]\n",
    "recalls = [r['recall'] for r in all_fold_results]\n",
    "aucs = [r['auc'] for r in all_fold_results]\n",
    "\n",
    "# Medie e deviazioni standard\n",
    "mean_acc = np.mean(accuracies)\n",
    "std_acc = np.std(accuracies)\n",
    "mean_f1 = np.mean(f1_scores)\n",
    "std_f1 = np.std(f1_scores)\n",
    "mean_precision = np.mean(precisions)\n",
    "std_precision = np.std(precisions)\n",
    "mean_recall = np.mean(recalls)\n",
    "std_recall = np.std(recalls)\n",
    "mean_auc = np.mean(aucs)\n",
    "std_auc = np.std(aucs)\n",
    "\n",
    "# Stampa i risultati aggregati\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"AGGREGATED RESULTS ACROSS ALL FOLDS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nMetric            | Mean      | Std Dev\")\n",
    "print(f\"{'-'*80}\")\n",
    "print(f\"Accuracy          | {mean_acc:.4f}    | {std_acc:.4f}\")\n",
    "print(f\"F1 Score          | {mean_f1:.4f}    | {std_f1:.4f}\")\n",
    "print(f\"Precision         | {mean_precision:.4f}    | {std_precision:.4f}\")\n",
    "print(f\"Recall            | {mean_recall:.4f}    | {std_recall:.4f}\")\n",
    "print(f\"AUC               | {mean_auc:.4f}    | {std_auc:.4f}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Stampa i risultati per ogni fold\n",
    "print(f\"\\nRESULTS PER FOLD:\")\n",
    "print(f\"{'-'*80}\")\n",
    "print(f\"Fold | Recipe                         | Accuracy | F1       | Precision | Recall   | AUC\")\n",
    "print(f\"{'-'*80}\")\n",
    "for result in all_fold_results:\n",
    "    print(f\"{result['fold']:<4} | {result['test_recipe']:<30} | {result['accuracy']:.4f}   | {result['f1']:.4f}   | {result['precision']:.4f}    | {result['recall']:.4f}   | {result['auc']:.4f}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Log delle metriche aggregate su W&B\n",
    "wandb.log({\n",
    "    \"overall/mean_accuracy\": mean_acc,\n",
    "    \"overall/std_accuracy\": std_acc,\n",
    "    \"overall/mean_f1\": mean_f1,\n",
    "    \"overall/std_f1\": std_f1,\n",
    "    \"overall/mean_precision\": mean_precision,\n",
    "    \"overall/std_precision\": std_precision,\n",
    "    \"overall/mean_recall\": mean_recall,\n",
    "    \"overall/std_recall\": std_recall,\n",
    "    \"overall/mean_auc\": mean_auc,\n",
    "    \"overall/std_auc\": std_auc,\n",
    "})\n",
    "\n",
    "# Crea una tabella per W&B con i risultati per fold\n",
    "fold_table_data = []\n",
    "for result in all_fold_results:\n",
    "    fold_table_data.append([\n",
    "        result['fold'],\n",
    "        result['test_recipe'],\n",
    "        result['accuracy'],\n",
    "        result['f1'],\n",
    "        result['precision'],\n",
    "        result['recall'],\n",
    "        result['auc']\n",
    "    ])\n",
    "\n",
    "wandb.log({\n",
    "    \"fold_results_table\": wandb.Table(\n",
    "        columns=[\"Fold\", \"Test Recipe\", \"Accuracy\", \"F1\", \"Precision\", \"Recall\", \"AUC\"],\n",
    "        data=fold_table_data\n",
    "    )\n",
    "})\n",
    "\n",
    "# Confusion Matrix aggregata (concatena tutti i target e le predizioni)\n",
    "all_targets = np.concatenate([r['test_targets'] for r in all_fold_results])\n",
    "all_preds = np.concatenate([r['test_preds'] for r in all_fold_results])\n",
    "\n",
    "cm_overall = confusion_matrix(all_targets, all_preds)\n",
    "print(f\"\\nOVERALL CONFUSION MATRIX:\")\n",
    "print(cm_overall)\n",
    "\n",
    "wandb.log({\n",
    "    \"overall/confusion_matrix\": wandb.plot.confusion_matrix(\n",
    "        probs=None,\n",
    "        y_true=all_targets,\n",
    "        preds=all_preds,\n",
    "        class_names=[\"No Error\", \"Error\"]\n",
    "    )\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3490441",
   "metadata": {
    "id": "d3490441"
   },
   "outputs": [],
   "source": [
    "# Chiudi il run di W&B\n",
    "wandb.finish()\n",
    "print(\"üèÅ W&B run terminato\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
