{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6RX9Vq-icJw",
        "outputId": "47865e03-6fcc-462b-f69e-a97fa19fb238"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬ Downloading https://github.com/jaimergp/miniforge/releases/download/24.11.2-1_colab/Miniforge3-colab-24.11.2-1_colab-Linux-x86_64.sh...\n",
            "ðŸ“¦ Installing...\n",
            "ðŸ“Œ Adjusting configuration...\n",
            "ðŸ©¹ Patching environment...\n",
            "â² Done in 0:00:10\n",
            "ðŸ” Restarting kernel...\n"
          ]
        }
      ],
      "source": [
        "# 1. Installa Conda su Colab\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FeL4K0H9i3y1",
        "outputId": "102d7fa3-416e-4f73-e993-e06b365e3aac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ¨ðŸ°âœ¨ Everything looks OK!\n",
            "Cloning into 'EgoVLP'...\n",
            "remote: Enumerating objects: 268, done.\u001b[K\n",
            "remote: Counting objects: 100% (159/159), done.\u001b[K\n",
            "remote: Compressing objects: 100% (111/111), done.\u001b[K\n",
            "remote: Total 268 (delta 85), reused 100 (delta 47), pack-reused 109 (from 1)\u001b[K\n",
            "Receiving objects: 100% (268/268), 2.00 MiB | 5.85 MiB/s, done.\n",
            "Resolving deltas: 100% (114/114), done.\n",
            "/content/EgoVLP\n",
            "Channels:\n",
            " - pytorch\n",
            " - conda-forge\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Solving environment: | \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - cudatoolkit=11.3\n",
            "    - pytorch\n",
            "    - torchaudio\n",
            "    - torchvision\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    _openmp_mutex-4.5          |       7_kmp_llvm           8 KB  conda-forge\n",
            "    aom-3.6.1                  |       h59595ed_0         2.6 MB  conda-forge\n",
            "    blas-2.116                 |              mkl          13 KB  conda-forge\n",
            "    blas-devel-3.9.0           |   16_linux64_mkl          12 KB  conda-forge\n",
            "    ca-certificates-2025.11.12 |       hbd8a1cb_0         149 KB  conda-forge\n",
            "    certifi-2025.11.12         |     pyhd8ed1ab_0         153 KB  conda-forge\n",
            "    conda-24.11.3              |  py311h38be061_0         1.1 MB  conda-forge\n",
            "    cpython-3.11.14            |  py311hd8ed1ab_2          46 KB  conda-forge\n",
            "    cudatoolkit-11.3.1         |      hb98b00a_13       603.5 MB  conda-forge\n",
            "    ffmpeg-4.4.2               | gpl_hdf48244_113         9.0 MB  conda-forge\n",
            "    filelock-3.20.1            |     pyhd8ed1ab_0          18 KB  conda-forge\n",
            "    font-ttf-dejavu-sans-mono-2.37|       hab24e00_0         388 KB  conda-forge\n",
            "    font-ttf-inconsolata-3.000 |       h77eed37_0          94 KB  conda-forge\n",
            "    font-ttf-source-code-pro-2.038|       h77eed37_0         684 KB  conda-forge\n",
            "    font-ttf-ubuntu-0.83       |       h77eed37_3         1.5 MB  conda-forge\n",
            "    fontconfig-2.15.0          |       h7e30c49_1         259 KB  conda-forge\n",
            "    fonts-conda-ecosystem-1    |                0           4 KB  conda-forge\n",
            "    fonts-conda-forge-1        |       hc364b38_1           4 KB  conda-forge\n",
            "    freetype-2.14.1            |       ha770c72_0         169 KB  conda-forge\n",
            "    giflib-5.2.2               |       hd590300_0          75 KB  conda-forge\n",
            "    gmp-6.3.0                  |       hac33072_2         449 KB  conda-forge\n",
            "    gmpy2-2.2.1                |  py311h92a432a_2         198 KB  conda-forge\n",
            "    gnutls-3.7.9               |       hb077bed_0         1.9 MB  conda-forge\n",
            "    jinja2-3.1.6               |     pyhcf101f3_1         118 KB  conda-forge\n",
            "    lame-3.100                 |    h166bdaf_1003         496 KB  conda-forge\n",
            "    lcms2-2.17                 |       h717163a_0         242 KB  conda-forge\n",
            "    lerc-4.0.0                 |       h0aef613_1         258 KB  conda-forge\n",
            "    libblas-3.9.0              |   16_linux64_mkl          13 KB  conda-forge\n",
            "    libcblas-3.9.0             |   16_linux64_mkl          12 KB  conda-forge\n",
            "    libdeflate-1.23            |       h86f0d12_0          71 KB  conda-forge\n",
            "    libdrm-2.4.125             |       hb03c661_1         304 KB  conda-forge\n",
            "    libegl-1.7.0               |       ha4b6fd6_2          44 KB  conda-forge\n",
            "    libfreetype-2.14.1         |       ha770c72_0           7 KB  conda-forge\n",
            "    libfreetype6-2.14.1        |       h73754d4_0         378 KB  conda-forge\n",
            "    libgfortran-14.2.0         |       h69a702a_2          52 KB  conda-forge\n",
            "    libgfortran-ng-14.2.0      |       h69a702a_2          53 KB  conda-forge\n",
            "    libgfortran5-14.2.0        |       hf1ad2bd_2         1.4 MB  conda-forge\n",
            "    libgl-1.7.0                |       ha4b6fd6_2         132 KB  conda-forge\n",
            "    libglvnd-1.7.0             |       ha4b6fd6_2         129 KB  conda-forge\n",
            "    libglx-1.7.0               |       ha4b6fd6_2          74 KB  conda-forge\n",
            "    libhwloc-2.11.2            |default_h0d58e46_1001         2.3 MB  conda-forge\n",
            "    libidn2-2.3.8              |       hfac485b_1         136 KB  conda-forge\n",
            "    libjpeg-turbo-3.1.2        |       hb03c661_0         619 KB  conda-forge\n",
            "    liblapack-3.9.0            |   16_linux64_mkl          12 KB  conda-forge\n",
            "    liblapacke-3.9.0           |   16_linux64_mkl          12 KB  conda-forge\n",
            "    libpciaccess-0.18          |       hb9d3cd8_0          28 KB  conda-forge\n",
            "    libpng-1.6.53              |       h421ea60_0         310 KB  conda-forge\n",
            "    libtasn1-4.20.0            |       hb03c661_1         115 KB  conda-forge\n",
            "    libtiff-4.7.0              |       hd9ff511_3         418 KB  conda-forge\n",
            "    libunistring-0.9.10        |       h7f98852_0         1.4 MB  conda-forge\n",
            "    libva-2.22.0               |       h4f16b4b_2         212 KB  conda-forge\n",
            "    libvpx-1.13.1              |       h59595ed_0         982 KB  conda-forge\n",
            "    libwebp-1.6.0              |       h9635ea4_0          92 KB  conda-forge\n",
            "    libwebp-base-1.6.0         |       hd42ef1d_0         419 KB  conda-forge\n",
            "    libxcb-1.17.0              |       h8a09558_0         387 KB  conda-forge\n",
            "    llvm-openmp-15.0.7         |       h0cdce71_0         3.1 MB  conda-forge\n",
            "    markupsafe-3.0.3           |  py311h3778330_0          25 KB  conda-forge\n",
            "    mkl-2022.1.0               |     h84fe81f_915       199.6 MB  conda-forge\n",
            "    mkl-devel-2022.1.0         |     ha770c72_916          25 KB  conda-forge\n",
            "    mkl-include-2022.1.0       |     h84fe81f_915         745 KB  conda-forge\n",
            "    mpc-1.3.1                  |       h24ddda3_1         114 KB  conda-forge\n",
            "    mpfr-4.2.1                 |       h90cbb55_3         620 KB  conda-forge\n",
            "    mpmath-1.3.0               |     pyhd8ed1ab_1         429 KB  conda-forge\n",
            "    nettle-3.9.1               |       h7ab15ed_0         988 KB  conda-forge\n",
            "    networkx-3.6.1             |     pyhcf101f3_0         1.5 MB  conda-forge\n",
            "    numpy-2.3.5                |  py311h2e04523_0         9.0 MB  conda-forge\n",
            "    openh264-2.3.1             |       hcb278e6_2         702 KB  conda-forge\n",
            "    openjpeg-2.5.3             |       h55fea9a_1         349 KB  conda-forge\n",
            "    openssl-3.6.0              |       h26f9b46_0         3.0 MB  conda-forge\n",
            "    p11-kit-0.24.1             |       hc5aa10d_0         4.5 MB  conda-forge\n",
            "    pillow-11.3.0              |  py311h98278a2_3        1021 KB  conda-forge\n",
            "    pthread-stubs-0.4          |    hb9d3cd8_1002           8 KB  conda-forge\n",
            "    pytorch-2.5.1              |     py3.11_cpu_0        92.0 MB  pytorch\n",
            "    pytorch-mutex-1.0          |              cpu           3 KB  pytorch\n",
            "    pyyaml-6.0.3               |  py311h3778330_0         207 KB  conda-forge\n",
            "    svt-av1-1.4.1              |       hcb278e6_0         2.4 MB  conda-forge\n",
            "    sympy-1.14.0               |   pyh2585a3b_105         4.4 MB  conda-forge\n",
            "    tbb-2021.13.0              |       hceb3a55_1         172 KB  conda-forge\n",
            "    torchaudio-2.5.1           |        py311_cpu         5.1 MB  pytorch\n",
            "    torchvision-0.20.1         |        py311_cpu         7.1 MB  pytorch\n",
            "    typing_extensions-4.15.0   |     pyhcf101f3_0          50 KB  conda-forge\n",
            "    wayland-1.23.1             |       h3e06ad9_0         314 KB  conda-forge\n",
            "    wayland-protocols-1.47     |       hd8ed1ab_0         137 KB  conda-forge\n",
            "    x264-1!164.3095            |       h166bdaf_2         877 KB  conda-forge\n",
            "    x265-3.5                   |       h924138e_3         3.2 MB  conda-forge\n",
            "    xorg-libx11-1.8.12         |       h4f16b4b_0         816 KB  conda-forge\n",
            "    xorg-libxau-1.0.12         |       hb03c661_1          15 KB  conda-forge\n",
            "    xorg-libxdmcp-1.1.5        |       hb03c661_1          20 KB  conda-forge\n",
            "    xorg-libxext-1.3.6         |       hb9d3cd8_0          49 KB  conda-forge\n",
            "    xorg-libxfixes-6.0.2       |       hb03c661_0          20 KB  conda-forge\n",
            "    yaml-0.2.5                 |       h280c20c_3          83 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:       976.0 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  aom                conda-forge/linux-64::aom-3.6.1-h59595ed_0 \n",
            "  blas               conda-forge/linux-64::blas-2.116-mkl \n",
            "  blas-devel         conda-forge/linux-64::blas-devel-3.9.0-16_linux64_mkl \n",
            "  cpython            conda-forge/noarch::cpython-3.11.14-py311hd8ed1ab_2 \n",
            "  cudatoolkit        conda-forge/linux-64::cudatoolkit-11.3.1-hb98b00a_13 \n",
            "  ffmpeg             conda-forge/linux-64::ffmpeg-4.4.2-gpl_hdf48244_113 \n",
            "  filelock           conda-forge/noarch::filelock-3.20.1-pyhd8ed1ab_0 \n",
            "  font-ttf-dejavu-s~ conda-forge/noarch::font-ttf-dejavu-sans-mono-2.37-hab24e00_0 \n",
            "  font-ttf-inconsol~ conda-forge/noarch::font-ttf-inconsolata-3.000-h77eed37_0 \n",
            "  font-ttf-source-c~ conda-forge/noarch::font-ttf-source-code-pro-2.038-h77eed37_0 \n",
            "  font-ttf-ubuntu    conda-forge/noarch::font-ttf-ubuntu-0.83-h77eed37_3 \n",
            "  fontconfig         conda-forge/linux-64::fontconfig-2.15.0-h7e30c49_1 \n",
            "  fonts-conda-ecosy~ conda-forge/noarch::fonts-conda-ecosystem-1-0 \n",
            "  fonts-conda-forge  conda-forge/noarch::fonts-conda-forge-1-hc364b38_1 \n",
            "  freetype           conda-forge/linux-64::freetype-2.14.1-ha770c72_0 \n",
            "  giflib             conda-forge/linux-64::giflib-5.2.2-hd590300_0 \n",
            "  gmp                conda-forge/linux-64::gmp-6.3.0-hac33072_2 \n",
            "  gmpy2              conda-forge/linux-64::gmpy2-2.2.1-py311h92a432a_2 \n",
            "  gnutls             conda-forge/linux-64::gnutls-3.7.9-hb077bed_0 \n",
            "  jinja2             conda-forge/noarch::jinja2-3.1.6-pyhcf101f3_1 \n",
            "  lame               conda-forge/linux-64::lame-3.100-h166bdaf_1003 \n",
            "  lcms2              conda-forge/linux-64::lcms2-2.17-h717163a_0 \n",
            "  lerc               conda-forge/linux-64::lerc-4.0.0-h0aef613_1 \n",
            "  libblas            conda-forge/linux-64::libblas-3.9.0-16_linux64_mkl \n",
            "  libcblas           conda-forge/linux-64::libcblas-3.9.0-16_linux64_mkl \n",
            "  libdeflate         conda-forge/linux-64::libdeflate-1.23-h86f0d12_0 \n",
            "  libdrm             conda-forge/linux-64::libdrm-2.4.125-hb03c661_1 \n",
            "  libegl             conda-forge/linux-64::libegl-1.7.0-ha4b6fd6_2 \n",
            "  libfreetype        conda-forge/linux-64::libfreetype-2.14.1-ha770c72_0 \n",
            "  libfreetype6       conda-forge/linux-64::libfreetype6-2.14.1-h73754d4_0 \n",
            "  libgfortran        conda-forge/linux-64::libgfortran-14.2.0-h69a702a_2 \n",
            "  libgfortran-ng     conda-forge/linux-64::libgfortran-ng-14.2.0-h69a702a_2 \n",
            "  libgfortran5       conda-forge/linux-64::libgfortran5-14.2.0-hf1ad2bd_2 \n",
            "  libgl              conda-forge/linux-64::libgl-1.7.0-ha4b6fd6_2 \n",
            "  libglvnd           conda-forge/linux-64::libglvnd-1.7.0-ha4b6fd6_2 \n",
            "  libglx             conda-forge/linux-64::libglx-1.7.0-ha4b6fd6_2 \n",
            "  libhwloc           conda-forge/linux-64::libhwloc-2.11.2-default_h0d58e46_1001 \n",
            "  libidn2            conda-forge/linux-64::libidn2-2.3.8-hfac485b_1 \n",
            "  libjpeg-turbo      conda-forge/linux-64::libjpeg-turbo-3.1.2-hb03c661_0 \n",
            "  liblapack          conda-forge/linux-64::liblapack-3.9.0-16_linux64_mkl \n",
            "  liblapacke         conda-forge/linux-64::liblapacke-3.9.0-16_linux64_mkl \n",
            "  libpciaccess       conda-forge/linux-64::libpciaccess-0.18-hb9d3cd8_0 \n",
            "  libpng             conda-forge/linux-64::libpng-1.6.53-h421ea60_0 \n",
            "  libtasn1           conda-forge/linux-64::libtasn1-4.20.0-hb03c661_1 \n",
            "  libtiff            conda-forge/linux-64::libtiff-4.7.0-hd9ff511_3 \n",
            "  libunistring       conda-forge/linux-64::libunistring-0.9.10-h7f98852_0 \n",
            "  libva              conda-forge/linux-64::libva-2.22.0-h4f16b4b_2 \n",
            "  libvpx             conda-forge/linux-64::libvpx-1.13.1-h59595ed_0 \n",
            "  libwebp            conda-forge/linux-64::libwebp-1.6.0-h9635ea4_0 \n",
            "  libwebp-base       conda-forge/linux-64::libwebp-base-1.6.0-hd42ef1d_0 \n",
            "  libxcb             conda-forge/linux-64::libxcb-1.17.0-h8a09558_0 \n",
            "  llvm-openmp        conda-forge/linux-64::llvm-openmp-15.0.7-h0cdce71_0 \n",
            "  markupsafe         conda-forge/linux-64::markupsafe-3.0.3-py311h3778330_0 \n",
            "  mkl                conda-forge/linux-64::mkl-2022.1.0-h84fe81f_915 \n",
            "  mkl-devel          conda-forge/linux-64::mkl-devel-2022.1.0-ha770c72_916 \n",
            "  mkl-include        conda-forge/linux-64::mkl-include-2022.1.0-h84fe81f_915 \n",
            "  mpc                conda-forge/linux-64::mpc-1.3.1-h24ddda3_1 \n",
            "  mpfr               conda-forge/linux-64::mpfr-4.2.1-h90cbb55_3 \n",
            "  mpmath             conda-forge/noarch::mpmath-1.3.0-pyhd8ed1ab_1 \n",
            "  nettle             conda-forge/linux-64::nettle-3.9.1-h7ab15ed_0 \n",
            "  networkx           conda-forge/noarch::networkx-3.6.1-pyhcf101f3_0 \n",
            "  numpy              conda-forge/linux-64::numpy-2.3.5-py311h2e04523_0 \n",
            "  openh264           conda-forge/linux-64::openh264-2.3.1-hcb278e6_2 \n",
            "  openjpeg           conda-forge/linux-64::openjpeg-2.5.3-h55fea9a_1 \n",
            "  p11-kit            conda-forge/linux-64::p11-kit-0.24.1-hc5aa10d_0 \n",
            "  pillow             conda-forge/linux-64::pillow-11.3.0-py311h98278a2_3 \n",
            "  pthread-stubs      conda-forge/linux-64::pthread-stubs-0.4-hb9d3cd8_1002 \n",
            "  pytorch            pytorch/linux-64::pytorch-2.5.1-py3.11_cpu_0 \n",
            "  pytorch-mutex      pytorch/noarch::pytorch-mutex-1.0-cpu \n",
            "  pyyaml             conda-forge/linux-64::pyyaml-6.0.3-py311h3778330_0 \n",
            "  svt-av1            conda-forge/linux-64::svt-av1-1.4.1-hcb278e6_0 \n",
            "  sympy              conda-forge/noarch::sympy-1.14.0-pyh2585a3b_105 \n",
            "  tbb                conda-forge/linux-64::tbb-2021.13.0-hceb3a55_1 \n",
            "  torchaudio         pytorch/linux-64::torchaudio-2.5.1-py311_cpu \n",
            "  torchvision        pytorch/linux-64::torchvision-0.20.1-py311_cpu \n",
            "  typing_extensions  conda-forge/noarch::typing_extensions-4.15.0-pyhcf101f3_0 \n",
            "  wayland            conda-forge/linux-64::wayland-1.23.1-h3e06ad9_0 \n",
            "  wayland-protocols  conda-forge/noarch::wayland-protocols-1.47-hd8ed1ab_0 \n",
            "  x264               conda-forge/linux-64::x264-1!164.3095-h166bdaf_2 \n",
            "  x265               conda-forge/linux-64::x265-3.5-h924138e_3 \n",
            "  xorg-libx11        conda-forge/linux-64::xorg-libx11-1.8.12-h4f16b4b_0 \n",
            "  xorg-libxau        conda-forge/linux-64::xorg-libxau-1.0.12-hb03c661_1 \n",
            "  xorg-libxdmcp      conda-forge/linux-64::xorg-libxdmcp-1.1.5-hb03c661_1 \n",
            "  xorg-libxext       conda-forge/linux-64::xorg-libxext-1.3.6-hb9d3cd8_0 \n",
            "  xorg-libxfixes     conda-forge/linux-64::xorg-libxfixes-6.0.2-hb03c661_0 \n",
            "  yaml               conda-forge/linux-64::yaml-0.2.5-h280c20c_3 \n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates    conda-forge/linux-64::ca-certificates~ --> conda-forge/noarch::ca-certificates-2025.11.12-hbd8a1cb_0 \n",
            "  certifi                           2024.12.14-pyhd8ed1ab_0 --> 2025.11.12-pyhd8ed1ab_0 \n",
            "  conda                             24.11.2-py311h38be061_1 --> 24.11.3-py311h38be061_0 \n",
            "  openssl                                  3.4.0-h7b32b05_1 --> 3.6.0-h26f9b46_0 \n",
            "\n",
            "The following packages will be DOWNGRADED:\n",
            "\n",
            "  _openmp_mutex                                   4.5-2_gnu --> 4.5-7_kmp_llvm \n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages:\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :   0% 0/1 [00:00<?, ?it/s]\n",
            "mkl-2022.1.0         | 199.6 MB  | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "pytorch-2.5.1        | 92.0 MB   | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "ffmpeg-4.4.2         | 9.0 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "numpy-2.3.5          | 9.0 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "torchvision-0.20.1   | 7.1 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "torchaudio-2.5.1     | 5.1 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "p11-kit-0.24.1       | 4.5 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "sympy-1.14.0         | 4.4 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "x265-3.5             | 3.2 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "llvm-openmp-15.0.7   | 3.1 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "openssl-3.6.0        | 3.0 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "aom-3.6.1            | 2.6 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "svt-av1-1.4.1        | 2.4 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libhwloc-2.11.2      | 2.3 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "gnutls-3.7.9         | 1.9 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "font-ttf-ubuntu-0.83 | 1.5 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "networkx-3.6.1       | 1.5 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgfortran5-14.2.0  | 1.4 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :   0% 5.177860700169071e-05/1 [00:00<32:44, 1965.04s/it]\n",
            "\n",
            "pytorch-2.5.1        | 92.0 MB   | :   0% 0.0006792758560333613/1 [00:00<02:29, 150.05s/it]\u001b[A\u001b[A\n",
            "mkl-2022.1.0         | 199.6 MB  | :   0% 7.826994939611777e-05/1 [00:00<22:59, 1379.21s/it]\u001b[A\n",
            "\n",
            "\n",
            "ffmpeg-4.4.2         | 9.0 MB    | :   1% 0.00866660678752309/1 [00:00<00:11, 11.97s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :   0% 0.004452960202145401/1 [00:00<00:38, 38.61s/it]   \n",
            "\n",
            "pytorch-2.5.1        | 92.0 MB   | :   3% 0.027680491133359474/1 [00:00<00:06,  6.24s/it]  \u001b[A\u001b[A\n",
            "mkl-2022.1.0         | 199.6 MB  | :   1% 0.011818762358813783/1 [00:00<00:14, 14.90s/it]   \u001b[A\n",
            "\n",
            "\n",
            "ffmpeg-4.4.2         | 9.0 MB    | :  31% 0.3050645589208128/1 [00:00<00:00,  1.75it/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :   1% 0.01092528607735674/1 [00:00<00:22, 22.96s/it] \n",
            "\n",
            "pytorch-2.5.1        | 92.0 MB   | :   6% 0.058927180510894094/1 [00:00<00:04,  4.36s/it]\u001b[A\u001b[A\n",
            "mkl-2022.1.0         | 199.6 MB  | :   3% 0.027785832035621807/1 [00:00<00:08,  9.24s/it]\u001b[A\n",
            "\n",
            "\n",
            "ffmpeg-4.4.2         | 9.0 MB    | :  64% 0.6413289022767087/1 [00:00<00:00,  2.48it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "numpy-2.3.5          | 9.0 MB    | :  57% 0.5651364210945535/1 [00:00<00:00,  2.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :   2% 0.016310261205532573/1 [00:00<00:20, 21.24s/it]\n",
            "mkl-2022.1.0         | 199.6 MB  | :   4% 0.043361551965449247/1 [00:00<00:07,  7.90s/it]\u001b[A\n",
            "\n",
            "\n",
            "ffmpeg-4.4.2         | 9.0 MB    | :  97% 0.9741266029175955/1 [00:00<00:00,  2.81it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "numpy-2.3.5          | 9.0 MB    | :  96% 0.9569181117920048/1 [00:00<00:00,  2.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :   2% 0.022471915438733767/1 [00:00<00:18, 19.21s/it]\n",
            "mkl-2022.1.0         | 199.6 MB  | :   6% 0.05893727189527668/1 [00:00<00:06,  7.35s/it] \u001b[A\n",
            "\n",
            "\n",
            "\n",
            "numpy-2.3.5          | 9.0 MB    | : 100% 1.0/1 [00:00<00:00,  2.87it/s]               \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :   3% 0.02959147390146624/1 [00:00<00:16, 17.24s/it] \n",
            "mkl-2022.1.0         | 199.6 MB  | :   8% 0.07999188828283237/1 [00:00<00:05,  6.48s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "torchvision-0.20.1   | 7.1 MB    | :   0% 0.0021898511679094845/1 [00:00<04:44, 285.32s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "ffmpeg-4.4.2         | 9.0 MB    | : 100% 1.0/1 [00:00<00:00,  2.81it/s]               \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "torchaudio-2.5.1     | 5.1 MB    | :   0% 0.0030817255405904105/1 [00:00<03:44, 224.76s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "pytorch-2.5.1        | 92.0 MB   | :  21% 0.20768859298220022/1 [00:00<00:02,  3.21s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :   4% 0.035416567189156443/1 [00:00<00:19, 20.39s/it]\n",
            "mkl-2022.1.0         | 199.6 MB  | :  10% 0.09541106831386756/1 [00:00<00:06,  7.30s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "torchaudio-2.5.1     | 5.1 MB    | :  46% 0.4560953800073808/1 [00:00<00:00,  1.29s/it]    \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "torchvision-0.20.1   | 7.1 MB    | :  69% 0.6941828202273067/1 [00:00<00:00,  1.26it/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "mkl-2022.1.0         | 199.6 MB  | :  11% 0.10942138925577265/1 [00:00<00:06,  7.34s/it]\u001b[A\n",
            "\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :   4% 0.040542649282323824/1 [00:00<00:21, 22.49s/it]\n",
            "mkl-2022.1.0         | 199.6 MB  | :  12% 0.12327517029888549/1 [00:01<00:07,  8.30s/it]\u001b[A\n",
            "\n",
            "pytorch-2.5.1        | 92.0 MB   | :  27% 0.267974325205161/1 [00:01<00:03,  4.59s/it]  \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "torchaudio-2.5.1     | 5.1 MB    | : 100% 1.0/1 [00:01<00:00,  1.18it/s]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :   5% 0.04520272391247599/1 [00:01<00:28, 29.51s/it] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "p11-kit-0.24.1       | 4.5 MB    | :   0% 0.0034841064226090947/1 [00:01<05:36, 338.00s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "mkl-2022.1.0         | 199.6 MB  | :  14% 0.1357200922528682/1 [00:01<00:08,  9.90s/it] \u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "torchvision-0.20.1   | 7.1 MB    | : 100% 1.0/1 [00:01<00:00,  1.01it/s]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "torchvision-0.20.1   | 7.1 MB    | : 100% 1.0/1 [00:01<00:00,  1.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "pytorch-2.5.1        | 92.0 MB   | :  29% 0.29208861809434533/1 [00:01<00:03,  5.39s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "p11-kit-0.24.1       | 4.5 MB    | :  22% 0.22298281104698206/1 [00:01<00:03,  4.17s/it]   \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :   5% 0.04906023013410195/1 [00:01<00:34, 36.79s/it]\n",
            "mkl-2022.1.0         | 199.6 MB  | :  15% 0.14652134526953248/1 [00:01<00:10, 11.74s/it]\u001b[A\n",
            "\n",
            "pytorch-2.5.1        | 92.0 MB   | :  31% 0.31280653170336287/1 [00:01<00:03,  5.59s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "p11-kit-0.24.1       | 4.5 MB    | :  49% 0.4947431120104914/1 [00:01<00:00,  1.78s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "sympy-1.14.0         | 4.4 MB    | :  30% 0.29810894158303225/1 [00:01<00:02,  3.39s/it]   \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "p11-kit-0.24.1       | 4.5 MB    | :  79% 0.7908921579322645/1 [00:01<00:00,  1.06s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "sympy-1.14.0         | 4.4 MB    | :  58% 0.5784733033099316/1 [00:01<00:00,  1.66s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "pytorch-2.5.1        | 92.0 MB   | :  33% 0.3319960746363053/1 [00:01<00:03,  5.77s/it] \u001b[A\u001b[A\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :   5% 0.05224461446470593/1 [00:01<00:38, 40.89s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "sympy-1.14.0         | 4.4 MB    | :  90% 0.9014246566915499/1 [00:01<00:00,  1.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "pytorch-2.5.1        | 92.0 MB   | :  35% 0.3539027209933812/1 [00:01<00:03,  5.55s/it]\u001b[A\u001b[A\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :   6% 0.055014769939296375/1 [00:01<00:40, 42.68s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "p11-kit-0.24.1       | 4.5 MB    | : 100% 1.0/1 [00:01<00:00,  1.06s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "pytorch-2.5.1        | 92.0 MB   | :  37% 0.372752625998307/1 [00:01<00:03,  5.56s/it] \u001b[A\u001b[A\n",
            "mkl-2022.1.0         | 199.6 MB  | :  17% 0.17274177831723192/1 [00:01<00:10, 13.14s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "x265-3.5             | 3.2 MB    | :   0% 0.004880274801411181/1 [00:01<06:02, 363.96s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :   6% 0.057577810985880065/1 [00:01<00:43, 45.74s/it]\n",
            "\n",
            "pytorch-2.5.1        | 92.0 MB   | :  39% 0.3912628930752161/1 [00:01<00:03,  5.99s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "llvm-openmp-15.0.7   | 3.1 MB    | :   1% 0.005012289041185573/1 [00:01<06:10, 371.89s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "mkl-2022.1.0         | 199.6 MB  | :  18% 0.1806470432062398/1 [00:01<00:11, 13.70s/it] \u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :   6% 0.05990784830095615/1 [00:01<00:44, 47.28s/it] \n",
            "\n",
            "pytorch-2.5.1        | 92.0 MB   | :  41% 0.41198080668423365/1 [00:01<00:03,  5.69s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "llvm-openmp-15.0.7   | 3.1 MB    | :  44% 0.43606914658314483/1 [00:01<00:01,  3.23s/it]  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "x265-3.5             | 3.2 MB    | :  94% 0.9370127618709467/1 [00:01<00:00,  1.30s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :   6% 0.06254855725804237/1 [00:02<00:42, 44.84s/it]\n",
            "\n",
            "pytorch-2.5.1        | 92.0 MB   | :  43% 0.43015143583312604/1 [00:02<00:03,  5.86s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "x265-3.5             | 3.2 MB    | : 100% 1.0/1 [00:02<00:00,  1.30s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :   7% 0.06619894905166157/1 [00:02<00:36, 38.83s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "openssl-3.6.0        | 3.0 MB    | :   1% 0.005175966758061148/1 [00:02<06:48, 410.61s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "pytorch-2.5.1        | 92.0 MB   | :  45% 0.4491711598020602/1 [00:02<00:03,  5.71s/it] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "llvm-openmp-15.0.7   | 3.1 MB    | : 100% 1.0/1 [00:02<00:00,  1.37s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "llvm-openmp-15.0.7   | 3.1 MB    | : 100% 1.0/1 [00:02<00:00,  1.37s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "mkl-2022.1.0         | 199.6 MB  | :  20% 0.20475418762024408/1 [00:02<00:10, 13.73s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :   7% 0.0691244403472571/1 [00:02<00:34, 37.60s/it] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "openssl-3.6.0        | 3.0 MB    | :  48% 0.48136490849968677/1 [00:02<00:01,  3.31s/it]  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "pytorch-2.5.1        | 92.0 MB   | :  47% 0.46988907341107766/1 [00:02<00:02,  5.44s/it]\u001b[A\u001b[A\n",
            "mkl-2022.1.0         | 199.6 MB  | :  21% 0.21399004164898597/1 [00:02<00:10, 12.83s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :   7% 0.07197226373235009/1 [00:02<00:34, 36.99s/it]\n",
            "\n",
            "pytorch-2.5.1        | 92.0 MB   | :  49% 0.48856915945199514/1 [00:02<00:02,  5.52s/it]\u001b[A\u001b[A\n",
            "mkl-2022.1.0         | 199.6 MB  | :  23% 0.2254174542608192/1 [00:02<00:08, 11.43s/it] \u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "openssl-3.6.0        | 3.0 MB    | : 100% 1.0/1 [00:02<00:00,  1.54s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :   8% 0.07528609458045829/1 [00:02<00:32, 35.55s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "aom-3.6.1            | 2.6 MB    | : 100% 1.0/1 [00:02<00:00,  2.38s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "pytorch-2.5.1        | 92.0 MB   | :  51% 0.5069096075648959/1 [00:02<00:02,  5.60s/it] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "svt-av1-1.4.1        | 2.4 MB    | :   1% 0.006584708798246436/1 [00:02<06:17, 379.77s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "mkl-2022.1.0         | 199.6 MB  | :  24% 0.23668832697386014/1 [00:02<00:08, 10.61s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :   8% 0.07885881846357495/1 [00:02<00:30, 33.10s/it]\n",
            "\n",
            "pytorch-2.5.1        | 92.0 MB   | :  53% 0.5257595125698217/1 [00:02<00:02,  5.52s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "svt-av1-1.4.1        | 2.4 MB    | :  65% 0.6453014622281507/1 [00:02<00:01,  2.88s/it]   \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "mkl-2022.1.0         | 199.6 MB  | :  25% 0.24803746963629722/1 [00:02<00:07, 10.07s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :   8% 0.08209498140118061/1 [00:02<00:29, 32.46s/it]\n",
            "\n",
            "pytorch-2.5.1        | 92.0 MB   | :  54% 0.5440999606827224/1 [00:02<00:02,  5.79s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :   9% 0.0852275871247829/1 [00:02<00:29, 32.79s/it] \n",
            "mkl-2022.1.0         | 199.6 MB  | :  26% 0.2581342931083964/1 [00:02<00:07, 10.70s/it] \u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "svt-av1-1.4.1        | 2.4 MB    | : 100% 1.0/1 [00:02<00:00,  2.88s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "gnutls-3.7.9         | 1.9 MB    | :   1% 0.0082959692344305/1 [00:02<05:32, 335.23s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "pytorch-2.5.1        | 92.0 MB   | :  57% 0.56583678807579/1 [00:02<00:02,  5.41s/it]  \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :   9% 0.0883084142413835/1 [00:02<00:29, 32.79s/it]\n",
            "mkl-2022.1.0         | 199.6 MB  | :  27% 0.2676832269347228/1 [00:02<00:08, 11.05s/it]\u001b[A\n",
            "\n",
            "pytorch-2.5.1        | 92.0 MB   | :  59% 0.5863848827207991/1 [00:02<00:02,  5.26s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "gnutls-3.7.9         | 1.9 MB    | : 100% 1.0/1 [00:02<00:00,  2.11s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "gnutls-3.7.9         | 1.9 MB    | : 100% 1.0/1 [00:02<00:00,  2.11s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "mkl-2022.1.0         | 199.6 MB  | :  28% 0.27684081101406854/1 [00:02<00:08, 11.21s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "font-ttf-ubuntu-0.83 | 1.5 MB    | : 100% 1.0/1 [00:02<00:00,  2.14s/it]                  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :   9% 0.0913892413579841/1 [00:03<00:34, 37.46s/it]\n",
            "\n",
            "pytorch-2.5.1        | 92.0 MB   | :  61% 0.6055744256537416/1 [00:03<00:02,  5.35s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "networkx-3.6.1       | 1.5 MB    | :   1% 0.01032102650873514/1 [00:03<04:48, 291.87s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgfortran5-14.2.0  | 1.4 MB    | :   1% 0.011206734985068174/1 [00:03<04:26, 269.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :   9% 0.09452184708158638/1 [00:03<00:32, 35.98s/it]\n",
            "\n",
            "\n",
            "ffmpeg-4.4.2         | 9.0 MB    | : 100% 1.0/1 [00:03<00:00,  2.81it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "pytorch-2.5.1        | 92.0 MB   | :  62% 0.6244243306586674/1 [00:03<00:02,  5.36s/it]\u001b[A\u001b[A\n",
            "mkl-2022.1.0         | 199.6 MB  | :  30% 0.2992260165413582/1 [00:03<00:07, 10.04s/it] \u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgfortran5-14.2.0  | 1.4 MB    | : 100% 1.0/1 [00:03<00:00,  2.28s/it]                  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgfortran5-14.2.0  | 1.4 MB    | : 100% 1.0/1 [00:03<00:00,  2.28s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "networkx-3.6.1       | 1.5 MB    | : 100% 1.0/1 [00:03<00:00,  2.28s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  10% 0.09739555977018022/1 [00:03<00:35, 39.63s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "pytorch-2.5.1        | 92.0 MB   | :  64% 0.6432742356635931/1 [00:03<00:02,  6.43s/it]\u001b[A\u001b[A\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  10% 0.1000103794237656/1 [00:03<00:36, 40.56s/it] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "pytorch-2.5.1        | 92.0 MB   | :  66% 0.6631430544525689/1 [00:03<00:02,  6.05s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  10% 0.10254753116684845/1 [00:03<00:38, 42.72s/it]\n",
            "\n",
            "pytorch-2.5.1        | 92.0 MB   | :  68% 0.6828420542775364/1 [00:03<00:01,  5.79s/it]\u001b[A\u001b[A\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  11% 0.10552480106944566/1 [00:03<00:36, 40.59s/it]\n",
            "\n",
            "pytorch-2.5.1        | 92.0 MB   | :  70% 0.7008428644624205/1 [00:03<00:01,  6.30s/it]\u001b[A\u001b[A\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  11% 0.10857973888254542/1 [00:03<00:33, 38.12s/it]\n",
            "\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  11% 0.11153111948164178/1 [00:03<00:32, 36.85s/it]\n",
            "mkl-2022.1.0         | 199.6 MB  | :  35% 0.34814473491393183/1 [00:03<00:08, 12.40s/it]\u001b[A\n",
            "\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  11% 0.11430127495623224/1 [00:03<00:34, 38.47s/it]\n",
            "mkl-2022.1.0         | 199.6 MB  | :  36% 0.357537128841466/1 [00:03<00:07, 11.91s/it]  \u001b[A\n",
            "\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  12% 0.11714909834132523/1 [00:04<00:33, 37.57s/it]\n",
            "mkl-2022.1.0         | 199.6 MB  | :  37% 0.366225093224435/1 [00:04<00:07, 11.83s/it]\u001b[A\n",
            "\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  12% 0.11984158590541315/1 [00:04<00:33, 38.36s/it]\n",
            "mkl-2022.1.0         | 199.6 MB  | :  37% 0.37499132755680026/1 [00:04<00:07, 11.73s/it]\u001b[A\n",
            "\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  12% 0.12315541675352135/1 [00:04<00:31, 35.76s/it]\n",
            "mkl-2022.1.0         | 199.6 MB  | :  38% 0.3839923717373538/1 [00:04<00:07, 11.59s/it] \u001b[A\n",
            "\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  13% 0.12628802247712365/1 [00:04<00:30, 34.66s/it]\n",
            "mkl-2022.1.0         | 199.6 MB  | :  39% 0.39275860606971896/1 [00:04<00:07, 11.65s/it]\u001b[A\n",
            "\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  13% 0.1292911816832217/1 [00:04<00:29, 34.27s/it] \n",
            "mkl-2022.1.0         | 199.6 MB  | :  40% 0.4035598590863832/1 [00:04<00:06, 10.90s/it] \u001b[A\n",
            "\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  13% 0.13314868790484766/1 [00:04<00:27, 31.32s/it]\n",
            "mkl-2022.1.0         | 199.6 MB  | :  41% 0.41279571311512514/1 [00:04<00:06, 10.90s/it]\u001b[A\n",
            "\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  14% 0.1370061941264736/1 [00:04<00:25, 29.59s/it] \n",
            "mkl-2022.1.0         | 199.6 MB  | :  42% 0.4242231257269583/1 [00:04<00:05, 10.24s/it] \u001b[A\n",
            "\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  14% 0.14236527995114862/1 [00:04<00:21, 25.28s/it]\n",
            "mkl-2022.1.0         | 199.6 MB  | :  43% 0.4340851393508692/1 [00:04<00:05, 10.26s/it]\u001b[A\n",
            "\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  15% 0.1473360262233109/1 [00:04<00:20, 23.52s/it] \n",
            "mkl-2022.1.0         | 199.6 MB  | :  45% 0.44645179135545576/1 [00:04<00:05,  9.52s/it]\u001b[A\n",
            "\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  15% 0.15160776130095038/1 [00:04<00:20, 24.62s/it]\n",
            "mkl-2022.1.0         | 199.6 MB  | :  46% 0.45701823452393164/1 [00:04<00:05,  9.94s/it]\u001b[A\n",
            "\n",
            "pytorch-2.5.1        | 92.0 MB   | :  96% 0.9615149742152229/1 [00:04<00:00,  4.68s/it]\u001b[A\u001b[A\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  16% 0.15569827125408395/1 [00:05<00:22, 26.95s/it]\n",
            "\n",
            "pytorch-2.5.1        | 92.0 MB   | :  99% 0.9873274567444906/1 [00:05<00:00,  4.42s/it]\u001b[A\u001b[A\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  16% 0.1594781095652074/1 [00:05<00:23, 27.43s/it] \n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  16% 0.16318027996582826/1 [00:05<00:24, 28.81s/it]\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  17% 0.16670122524194322/1 [00:05<00:24, 29.78s/it]\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  17% 0.17009272400055397/1 [00:05<00:26, 31.77s/it]\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  17% 0.17343244415216302/1 [00:05<00:25, 31.43s/it]\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  18% 0.17679805360727294/1 [00:05<00:25, 31.37s/it]\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  18% 0.181975914307442/1 [00:05<00:21, 26.86s/it]  \n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  19% 0.18681721406210008/1 [00:05<00:20, 24.84s/it]\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  19% 0.19176207103076154/1 [00:06<00:18, 23.34s/it]\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  20% 0.19611147401890355/1 [00:06<00:21, 26.61s/it]\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  20% 0.2000207588475312/1 [00:06<00:22, 27.60s/it] \n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  20% 0.20374881855165294/1 [00:06<00:23, 29.94s/it]\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  21% 0.20833122527130257/1 [00:06<00:21, 27.37s/it]\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  21% 0.2127324068664463/1 [00:06<00:20, 25.96s/it] \n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  22% 0.21809149269112127/1 [00:06<00:18, 23.45s/it]\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  22% 0.22275156732127344/1 [00:06<00:17, 22.85s/it]\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  23% 0.22723041682691966/1 [00:06<00:17, 22.89s/it]\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  23% 0.23227883100958452/1 [00:07<00:16, 21.96s/it]\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  24% 0.23973495041782797/1 [00:07<00:14, 18.52s/it]\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  25% 0.24519759345650635/1 [00:07<00:14, 18.74s/it]\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  25% 0.2505825685846822/1 [00:07<00:14, 19.10s/it] \n",
            "mkl-2022.1.0         | 199.6 MB  | :  78% 0.7779250270480145/1 [00:07<00:01,  5.78s/it]\u001b[A\n",
            "\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  26% 0.25671833351438256/1 [00:07<00:13, 18.24s/it]\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  26% 0.2622327551600626/1 [00:07<00:14, 20.03s/it] \n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  27% 0.2673329479497291/1 [00:07<00:15, 21.70s/it]\n",
            "\n",
            "\n",
            "\n",
            "numpy-2.3.5          | 9.0 MB    | : 100% 1.0/1 [00:07<00:00,  2.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  28% 0.2765236506925292/1 [00:08<00:18, 25.88s/it] \n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  28% 0.2813131718401856/1 [00:08<00:17, 24.56s/it]\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  29% 0.2859214678633361/1 [00:08<00:16, 23.75s/it]\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  29% 0.2902449815479773/1 [00:08<00:16, 23.88s/it]\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  30% 0.29503450269563364/1 [00:08<00:16, 23.06s/it]\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  30% 0.2994356842907774/1 [00:08<00:16, 23.53s/it] \n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  31% 0.3050536631504608/1 [00:08<00:14, 21.58s/it]\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  31% 0.3097655163876147/1 [00:08<00:15, 22.87s/it]\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  31% 0.31421847658976004/1 [00:08<00:15, 23.30s/it]\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  32% 0.31856787957790206/1 [00:08<00:16, 24.09s/it]\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  34% 0.3425672639231857/1 [00:09<00:14, 22.02s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  56% 0.5638949195519126/1 [00:13<00:06, 16.01s/it]\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  60% 0.600994291468624/1 [00:13<00:04, 12.16s/it] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  68% 0.6815876932667556/1 [00:14<00:03, 11.78s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  85% 0.8461659956216295/1 [00:16<00:01,  8.95s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  88% 0.878890075246698/1 [00:16<00:01,  9.44s/it] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  90% 0.9000416362068887/1 [00:16<00:01, 10.06s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "openssl-3.6.0        | 3.0 MB    | : 100% 1.0/1 [00:16<00:00,  1.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  91% 0.910034907358215/1 [00:16<00:01, 11.19s/it] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  94% 0.9361831038940689/1 [00:17<00:00, 12.84s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  98% 0.9815152743240491/1 [00:17<00:00, 11.50s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | :  99% 0.9908872021913551/1 [00:17<00:00, 11.24s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "font-ttf-ubuntu-0.83 | 1.5 MB    | : 100% 1.0/1 [00:17<00:00,  2.14s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgfortran5-14.2.0  | 1.4 MB    | : 100% 1.0/1 [00:17<00:00,  2.28s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "networkx-3.6.1       | 1.5 MB    | : 100% 1.0/1 [00:18<00:00,  2.28s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "gnutls-3.7.9         | 1.9 MB    | : 100% 1.0/1 [00:18<00:00,  2.11s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | : 100% 1.0/1 [00:25<00:00, 254.07s/it]\n",
            "\n",
            "cudatoolkit-11.3.1   | 603.5 MB  | : 100% 1.0/1 [01:34<00:00, 254.07s/it]\n",
            "mkl-2022.1.0         | 199.6 MB  | : 100% 1.0/1 [01:43<00:00,  8.22s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                         \n",
            "                                                                        \u001b[A\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Preparing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Verifying transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Executing transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html\n",
            "\n",
            "\b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Installazione dipendenze Python...\n",
            "Collecting timm==0.4.12\n",
            "  Downloading timm-0.4.12-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
            "Collecting decord\n",
            "  Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl.metadata (422 bytes)\n",
            "Collecting pandas\n",
            "  Downloading pandas-2.3.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
            "Collecting h5py\n",
            "  Downloading h5py-3.15.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting regex\n",
            "  Downloading regex-2025.11.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/site-packages (4.67.1)\n",
            "Collecting opencv-python\n",
            "  Downloading opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.11/site-packages (from timm==0.4.12) (2.5.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/site-packages (from timm==0.4.12) (0.20.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/site-packages (from transformers) (3.20.1)\n",
            "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
            "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/site-packages (from transformers) (2.3.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/site-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/site-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
            "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
            "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting safetensors>=0.4.3 (from transformers)\n",
            "  Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting python-dateutil>=2.8.2 (from pandas)\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting pytz>=2020.1 (from pandas)\n",
            "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas)\n",
            "  Downloading tzdata-2025.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting wcwidth (from ftfy)\n",
            "  Downloading wcwidth-0.2.14-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Collecting numpy>=1.17 (from transformers)\n",
            "  Downloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
            "  Downloading fsspec-2025.12.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
            "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas)\n",
            "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting sympy==1.13.1 (from torch>=1.4->timm==0.4.12)\n",
            "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/site-packages (from torch>=1.4->timm==0.4.12) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/site-packages (from torch>=1.4->timm==0.4.12) (3.1.6)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/site-packages (from sympy==1.13.1->torch>=1.4->timm==0.4.12) (1.3.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests->transformers) (2025.11.12)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/site-packages (from torchvision->timm==0.4.12) (11.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/site-packages (from jinja2->torch>=1.4->timm==0.4.12) (3.0.3)\n",
            "Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m146.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m164.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.3.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h5py-3.15.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m132.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "Downloading regex-2025.11.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (800 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m800.4/800.4 kB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (67.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m147.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
            "Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m105.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m115.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tzdata-2025.3-py2.py3-none-any.whl (348 kB)\n",
            "Downloading wcwidth-0.2.14-py2.py3-none-any.whl (37 kB)\n",
            "Downloading fsspec-2025.12.0-py3-none-any.whl (201 kB)\n",
            "Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m113.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pytz, wcwidth, tzdata, sympy, six, safetensors, regex, numpy, hf-xet, fsspec, python-dateutil, opencv-python, huggingface-hub, h5py, ftfy, decord, tokenizers, pandas, transformers, timm\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.14.0\n",
            "    Uninstalling sympy-1.14.0:\n",
            "      Successfully uninstalled sympy-1.14.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.3.5\n",
            "    Uninstalling numpy-2.3.5:\n",
            "      Successfully uninstalled numpy-2.3.5\n",
            "Successfully installed decord-0.6.0 fsspec-2025.12.0 ftfy-6.3.1 h5py-3.15.1 hf-xet-1.2.0 huggingface-hub-0.36.0 numpy-2.2.6 opencv-python-4.12.0.88 pandas-2.3.3 python-dateutil-2.9.0.post0 pytz-2025.2 regex-2025.11.3 safetensors-0.7.0 six-1.17.0 sympy-1.13.1 timm-0.4.12 tokenizers-0.22.1 transformers-4.57.3 tzdata-2025.3 wcwidth-0.2.14\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dateutil",
                  "numpy",
                  "six",
                  "wcwidth"
                ]
              },
              "id": "f4cd4ca6d79f424ea6b7e91bcbc8fdbd"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# 2. Verifica Conda e Clona il Repo\n",
        "import condacolab\n",
        "condacolab.check()\n",
        "\n",
        "# Clona il repository ufficiale (se non esiste giÃ )\n",
        "import os\n",
        "if not os.path.exists(\"EgoVLP\"):\n",
        "    !git clone https://github.com/showlab/EgoVLP.git\n",
        "\n",
        "%cd EgoVLP\n",
        "\n",
        "# Installa le dipendenze richieste tramite Conda\n",
        "# Nota: PyTorch Ã¨ giÃ  stato parzialmente installato nel passaggio precedente, qui ci assicuriamo delle versioni\n",
        "!conda install -y pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch\n",
        "\n",
        "# --- FIX PER L'ERRORE TOKENIZERS ---\n",
        "# Rimuoviamo \"==4.12.5\" da transformers per lasciare che pip trovi una versione compatibile con Python 3.11\n",
        "# Rimuoviamo anche il blocco su timm se dovesse dare problemi, ma proviamo a mantenerlo per ora.\n",
        "print(\"Installazione dipendenze Python...\")\n",
        "!pip install timm==0.4.12 transformers decord pandas h5py ftfy regex tqdm opencv-python\n",
        "\n",
        "# Patch per il path\n",
        "import sys\n",
        "if \"/content/EgoVLP\" not in sys.path:\n",
        "    sys.path.append(\"/content/EgoVLP\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PARTE 1: VIDEO EXTRACTOR"
      ],
      "metadata": {
        "id": "OISVyuNmGd0m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fuM0Cp9istW",
        "outputId": "fa4dafab-7299-4cf0-ec83-2d91590e24be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "â¬‡ï¸ Download dei pesi EgoVLP_PT_BEST in corso...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1-cP3Gcg0NGDcMZalgJ_615BQdbFIbcj7\n",
            "From (redirected): https://drive.google.com/uc?id=1-cP3Gcg0NGDcMZalgJ_615BQdbFIbcj7&confirm=t&uuid=d2a1d366-34f4-4635-803c-94491e59b63a\n",
            "To: /content/EgoVLP/pretrained/model.pth\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.17G/2.17G [00:21<00:00, 102MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Pesi scaricati correttamente in pretrained/model.pth\n",
            "ðŸ“‚ Estrazione video da /content/drive/MyDrive/MistakeDetection/captain_cook_4d_gopro_resized_8.zip...\n",
            "âœ… Video estratti.\n"
          ]
        }
      ],
      "source": [
        "# 3. Preparazione Dati e Download Pesi Automatico\n",
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "import gdown # Libreria per scaricare da Google Drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- CONFIGURAZIONE PERCORSI ---\n",
        "# Dove si trova lo zip dei tuoi video su Drive (questo deve essere corretto!)\n",
        "PATH_ZIP_VIDEO = \"/content/drive/MyDrive/MistakeDetection/captain_cook_4d_gopro_resized_8.zip\"\n",
        "\n",
        "# Dove salvare lo zip finale delle features\n",
        "OUTPUT_ZIP_PATH = \"/content/drive/MyDrive/MistakeDetection/egovlp_8.zip\"\n",
        "# -------------------------------\n",
        "\n",
        "# 1. Setup Cartelle\n",
        "if not os.path.exists(\"pretrained\"):\n",
        "    os.makedirs(\"pretrained\")\n",
        "if not os.path.exists(\"data/videos\"):\n",
        "    os.makedirs(\"data/videos\")\n",
        "if not os.path.exists(\"data/features\"):\n",
        "    os.makedirs(\"data/features\")\n",
        "\n",
        "# 2. Download Automatico dei Pesi (EgoVLP_PT_BEST.pth)\n",
        "weights_path = \"pretrained/model.pth\"\n",
        "if not os.path.exists(weights_path):\n",
        "    print(\"â¬‡ï¸ Download dei pesi EgoVLP_PT_BEST in corso...\")\n",
        "    # ID del file ufficiale dal GitHub di EgoVLP\n",
        "    file_id = '1-cP3Gcg0NGDcMZalgJ_615BQdbFIbcj7'\n",
        "    url = f'https://drive.google.com/uc?id={file_id}'\n",
        "\n",
        "    try:\n",
        "        gdown.download(url, weights_path, quiet=False)\n",
        "        print(\"âœ… Pesi scaricati correttamente in pretrained/model.pth\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Errore nel download: {e}\")\n",
        "        print(\"Prova a scaricarli manualmente da: https://drive.google.com/file/d/1-cP3Gcg0NGDcMZalgJ_615BQdbFIbcj7/view\")\n",
        "else:\n",
        "    print(\"âœ… Pesi giÃ  presenti.\")\n",
        "\n",
        "# 3. Estrazione Video\n",
        "# Controlla se abbiamo giÃ  estratto per non rifarlo ogni volta\n",
        "if len(os.listdir(\"data/videos\")) < 5: # Se la cartella Ã¨ quasi vuota\n",
        "    if os.path.exists(PATH_ZIP_VIDEO):\n",
        "        print(f\"ðŸ“‚ Estrazione video da {PATH_ZIP_VIDEO}...\")\n",
        "        !unzip -q \"{PATH_ZIP_VIDEO}\" -d \"data/videos\"\n",
        "        print(\"âœ… Video estratti.\")\n",
        "    else:\n",
        "        print(f\"âŒ ERRORE: Non trovo il file zip dei video in: {PATH_ZIP_VIDEO}\")\n",
        "        print(\"Verifica il percorso nel tuo Google Drive!\")\n",
        "else:\n",
        "    print(\"âœ… Video giÃ  presenti in locale.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaj5kpsdrRr7",
        "outputId": "2f0d7947-f74c-4a3b-d6cf-c47fa6718c51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸ Download backbone ViT (richiesto dal codice originale)...\n",
            "--2025-12-20 21:18:39--  https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_p16_224-80ecf9dd.pth\n",
            "Resolving github.com (github.com)... 140.82.116.4\n",
            "Connecting to github.com (github.com)|140.82.116.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_p16_224-80ecf9dd.pth [following]\n",
            "--2025-12-20 21:18:39--  https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_p16_224-80ecf9dd.pth\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/168799526/65360900-1a09-11eb-8b86-f0a014a6f156?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-12-20T22%3A02%3A27Z&rscd=attachment%3B+filename%3Djx_vit_base_p16_224-80ecf9dd.pth&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-12-20T21%3A01%3A58Z&ske=2025-12-20T22%3A02%3A27Z&sks=b&skv=2018-11-09&sig=4zA9LtR6U7i2oKJtR8HiWjHXI3zRJvp5%2FmssL5uElQk%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NjI2OTExOSwibmJmIjoxNzY2MjY1NTE5LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.sBAjkOR-gL4eE-OFyYdTy4J5n9I4oFUWyGz63NqxkzI&response-content-disposition=attachment%3B%20filename%3Djx_vit_base_p16_224-80ecf9dd.pth&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-12-20 21:18:39--  https://release-assets.githubusercontent.com/github-production-release-asset/168799526/65360900-1a09-11eb-8b86-f0a014a6f156?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-12-20T22%3A02%3A27Z&rscd=attachment%3B+filename%3Djx_vit_base_p16_224-80ecf9dd.pth&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-12-20T21%3A01%3A58Z&ske=2025-12-20T22%3A02%3A27Z&sks=b&skv=2018-11-09&sig=4zA9LtR6U7i2oKJtR8HiWjHXI3zRJvp5%2FmssL5uElQk%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NjI2OTExOSwibmJmIjoxNzY2MjY1NTE5LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.sBAjkOR-gL4eE-OFyYdTy4J5n9I4oFUWyGz63NqxkzI&response-content-disposition=attachment%3B%20filename%3Djx_vit_base_p16_224-80ecf9dd.pth&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 346292833 (330M) [application/octet-stream]\n",
            "Saving to: â€˜pretrained/jx_vit_base_p16_224-80ecf9dd.pthâ€™\n",
            "\n",
            "pretrained/jx_vit_b 100%[===================>] 330.25M  94.2MB/s    in 3.5s    \n",
            "\n",
            "2025-12-20 21:18:43 (94.2 MB/s) - â€˜pretrained/jx_vit_base_p16_224-80ecf9dd.pthâ€™ saved [346292833/346292833]\n",
            "\n",
            "âœ… Backbone scaricato.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Crea la cartella se non esiste\n",
        "if not os.path.exists(\"pretrained\"):\n",
        "    os.makedirs(\"pretrained\")\n",
        "\n",
        "# Scarica il file dei pesi ImageNet richiesto hardcoded nel codice\n",
        "print(\"â¬‡ï¸ Download backbone ViT (richiesto dal codice originale)...\")\n",
        "!wget -nc https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_p16_224-80ecf9dd.pth -O pretrained/jx_vit_base_p16_224-80ecf9dd.pth\n",
        "print(\"âœ… Backbone scaricato.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torch torchvision torchaudio\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n"
      ],
      "metadata": {
        "id": "PshJTJAzMw_7",
        "outputId": "c0f0b824-9698-4e46-82e3-e89fbcfdb78f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.5.1\n",
            "Uninstalling torch-2.5.1:\n",
            "  Successfully uninstalled torch-2.5.1\n",
            "Found existing installation: torchvision 0.20.1\n",
            "Uninstalling torchvision-0.20.1:\n",
            "  Successfully uninstalled torchvision-0.20.1\n",
            "Found existing installation: torchaudio 2.5.1\n",
            "Uninstalling torchaudio-2.5.1:\n",
            "  Successfully uninstalled torchaudio-2.5.1\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp311-cp311-linux_x86_64.whl (780.5 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m780.5/780.5 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp311-cp311-linux_x86_64.whl (7.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m115.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/site-packages (from torch) (3.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/site-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/site-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/site-packages (from torch) (2025.12.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m152.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m171.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Collecting triton==3.1.0 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/site-packages (from torch) (1.13.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading https://download.pytorch.org/whl/nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/site-packages (from torchvision) (2.2.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/site-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/site-packages (from jinja2->torch) (3.0.3)\n",
            "Downloading https://download.pytorch.org/whl/nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 torch-2.5.1+cu121 torchaudio-2.5.1+cu121 torchvision-0.20.1+cu121 triton-3.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvqSzTke_G5s",
        "outputId": "799e1cab-da13-498e-ab6c-3c3be911c985"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing extract_custom.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile extract_custom.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import argparse\n",
        "import time\n",
        "import queue\n",
        "import threading\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from model.model import FrozenInTime\n",
        "from utils.util import state_dict_data_parallel_fix\n",
        "\n",
        "# --- CONFIGURAZIONE PARALLELISMO ---\n",
        "# Numero di video da leggere contemporaneamente (dipende dai core CPU)\n",
        "# Su Colab standard 2-3 Ã¨ ottimale. Su Pro anche 4-6.\n",
        "NUM_WORKERS = 3\n",
        "QUEUE_SIZE_PER_WORKER = 2\n",
        "GLOBAL_QUEUE_SIZE = NUM_WORKERS * QUEUE_SIZE_PER_WORKER\n",
        "\n",
        "# Parametri Normalizzazione (B, T, C, H, W)\n",
        "MEAN = torch.tensor([0.485, 0.456, 0.406]).view(1, 1, 3, 1, 1)\n",
        "STD = torch.tensor([0.229, 0.224, 0.225]).view(1, 1, 3, 1, 1)\n",
        "\n",
        "def load_model(ckpt_path, device):\n",
        "    print(f\"Costruzione modello FrozenInTime su {device}...\")\n",
        "    model = FrozenInTime(\n",
        "        video_params={\n",
        "            \"model\": \"SpaceTimeTransformer\",\n",
        "            \"arch_config\": \"base_patch16_224\",\n",
        "            \"num_frames\": 16,\n",
        "            \"pretrained\": True,\n",
        "            \"time_init\": \"zeros\"\n",
        "        },\n",
        "        text_params={\"model\": \"bert-base-uncased\", \"pretrained\": True, \"input\": \"text\"},\n",
        "        projection_dim=256,\n",
        "        load_checkpoint=None,\n",
        "        projection='minimal',\n",
        "        load_temporal_fix='zeros'\n",
        "    )\n",
        "    checkpoint = torch.load(ckpt_path, map_location='cpu', weights_only=False)\n",
        "    if 'state_dict' in checkpoint:\n",
        "        state_dict = checkpoint['state_dict']\n",
        "    else:\n",
        "        state_dict = checkpoint\n",
        "    new_state_dict = state_dict_data_parallel_fix(state_dict, model.state_dict())\n",
        "    model.load_state_dict(new_state_dict, strict=False)\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def preprocess_on_gpu(batch_uint8):\n",
        "    \"\"\"Normalizza batch uint8 [B, T, H, W, C] -> float32 [B, T, C, H, W]\"\"\"\n",
        "    batch = batch_uint8.float() / 255.0\n",
        "    batch = batch.permute(0, 1, 4, 2, 3).contiguous()\n",
        "    batch = (batch - MEAN) / STD\n",
        "    return batch\n",
        "\n",
        "def video_worker(vpath, clip_len, batch_size, global_queue):\n",
        "    \"\"\"\n",
        "    Worker che processa un singolo video e invia i batch alla coda globale.\n",
        "    Invia tuple: (vpath, batch_tensor, is_last_batch)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        cap = cv2.VideoCapture(vpath)\n",
        "        if not cap.isOpened():\n",
        "            # Segnala fine con errore\n",
        "            global_queue.put((vpath, None, True))\n",
        "            return\n",
        "\n",
        "        buffer = []\n",
        "        current_clip = []\n",
        "\n",
        "        # Pre-calcolo crop 224x224\n",
        "        resize_dim = 256\n",
        "        crop_dim = 224\n",
        "        start_y = (resize_dim - crop_dim) // 2\n",
        "        start_x = (resize_dim - crop_dim) // 2\n",
        "\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            # CPU Preprocessing\n",
        "            frame = cv2.resize(frame, (resize_dim, resize_dim), interpolation=cv2.INTER_LINEAR)\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frame = frame[start_y:start_y+crop_dim, start_x:start_x+crop_dim]\n",
        "\n",
        "            current_clip.append(frame)\n",
        "\n",
        "            if len(current_clip) == clip_len:\n",
        "                buffer.append(np.stack(current_clip))\n",
        "                current_clip = []\n",
        "\n",
        "                if len(buffer) >= batch_size:\n",
        "                    batch_np = np.stack(buffer)\n",
        "                    batch_tensor = torch.from_numpy(batch_np)\n",
        "                    # Mette in coda (blocca se piena per non saturare RAM)\n",
        "                    global_queue.put((vpath, batch_tensor, False))\n",
        "                    buffer = []\n",
        "\n",
        "        # Residui Clip\n",
        "        if len(current_clip) > 0:\n",
        "            last_frame = current_clip[-1]\n",
        "            pad_needed = clip_len - len(current_clip)\n",
        "            current_clip.extend([last_frame] * pad_needed)\n",
        "            buffer.append(np.stack(current_clip))\n",
        "\n",
        "        # Residui Batch\n",
        "        if len(buffer) > 0:\n",
        "            batch_np = np.stack(buffer)\n",
        "            batch_tensor = torch.from_numpy(batch_np)\n",
        "            global_queue.put((vpath, batch_tensor, False))\n",
        "\n",
        "        cap.release()\n",
        "        # Segnale di fine video\n",
        "        global_queue.put((vpath, None, True))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {vpath}: {e}\")\n",
        "        global_queue.put((vpath, None, True))\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--video_dir', type=str, required=True)\n",
        "    parser.add_argument('--output_dir', type=str, required=True)\n",
        "    parser.add_argument('--weights', type=str, required=True)\n",
        "    parser.add_argument('--batch_size', type=int, default=96)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        device = 'cuda'\n",
        "        print(f\"ðŸš€ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"âš¡ MULTI-VIDEO MODE: {NUM_WORKERS} Workers Paralleli | Batch {args.batch_size}\")\n",
        "    else:\n",
        "        device = 'cpu'\n",
        "        print(\"âš ï¸ CUDA non disponibile.\")\n",
        "\n",
        "    global MEAN, STD\n",
        "    MEAN = MEAN.to(device)\n",
        "    STD = STD.to(device)\n",
        "\n",
        "    model = load_model(args.weights, device)\n",
        "\n",
        "    # Lista video da processare\n",
        "    all_videos = []\n",
        "    for ext in ['*.mp4', '*.avi', '*.MOV']:\n",
        "        all_videos.extend(glob.glob(os.path.join(args.video_dir, \"**\", ext), recursive=True))\n",
        "\n",
        "    # Filtra quelli giÃ  fatti\n",
        "    videos_to_process = []\n",
        "    for vpath in all_videos:\n",
        "        vname = os.path.basename(vpath)\n",
        "        save_name = f\"{vname}_1s_1s.npz\"\n",
        "        save_path = os.path.join(args.output_dir, save_name)\n",
        "        if not os.path.exists(save_path):\n",
        "            videos_to_process.append(vpath)\n",
        "\n",
        "    print(f\"Trovati {len(all_videos)} video totali. Da elaborare: {len(videos_to_process)}\")\n",
        "\n",
        "    if not videos_to_process:\n",
        "        print(\"Tutti i video completati!\")\n",
        "        return\n",
        "\n",
        "    # Strutture dati condivise\n",
        "    global_queue = queue.Queue(maxsize=GLOBAL_QUEUE_SIZE)\n",
        "    results_buffer = {} # {vpath: [list_of_features]}\n",
        "    active_videos = set()\n",
        "    completed_videos = 0\n",
        "    start_times = {}\n",
        "\n",
        "    # Executor per i worker\n",
        "    executor = ThreadPoolExecutor(max_workers=NUM_WORKERS)\n",
        "\n",
        "    # Funzione helper per avviare un worker\n",
        "    video_iterator = iter(videos_to_process)\n",
        "\n",
        "    def start_next_worker():\n",
        "        try:\n",
        "            vpath = next(video_iterator)\n",
        "            active_videos.add(vpath)\n",
        "            results_buffer[vpath] = []\n",
        "            start_times[vpath] = time.time()\n",
        "            executor.submit(video_worker, vpath, 16, args.batch_size, global_queue)\n",
        "            print(f\"â–¶ï¸ Start: {os.path.basename(vpath)}\")\n",
        "            return True\n",
        "        except StopIteration:\n",
        "            return False\n",
        "\n",
        "    # Avvia i primi N worker\n",
        "    for _ in range(NUM_WORKERS):\n",
        "        start_next_worker()\n",
        "\n",
        "    # MAIN LOOP (GPU Consumer)\n",
        "    while active_videos:\n",
        "        try:\n",
        "            # Preleva un batch dalla coda (da qualsiasi video)\n",
        "            vpath, batch_cpu, is_last = global_queue.get(timeout=60) # Timeout per evitare deadlock\n",
        "\n",
        "            if is_last:\n",
        "                # Video finito: Salva e chiudi\n",
        "                features = np.concatenate(results_buffer[vpath], axis=0)\n",
        "\n",
        "                vname = os.path.basename(vpath)\n",
        "                save_name = f\"{vname}_1s_1s.npz\"\n",
        "                save_path = os.path.join(args.output_dir, save_name)\n",
        "                np.savez_compressed(save_path, features=features.astype(np.float32))\n",
        "\n",
        "                elapsed = time.time() - start_times[vpath]\n",
        "                fps = (features.shape[0] * 16) / elapsed\n",
        "                completed_videos += 1\n",
        "                print(f\"âœ… Saved: {vname} ({elapsed:.1f}s - {fps:.0f} fps) | Progress: {completed_videos}/{len(videos_to_process)}\")\n",
        "\n",
        "                # Pulizia\n",
        "                del results_buffer[vpath]\n",
        "                del start_times[vpath]\n",
        "                active_videos.remove(vpath)\n",
        "\n",
        "                # Avvia prossimo video\n",
        "                start_next_worker()\n",
        "                global_queue.task_done()\n",
        "                continue\n",
        "\n",
        "            if batch_cpu is not None:\n",
        "                # Inferenza GPU\n",
        "                batch_gpu = batch_cpu.to(device, non_blocking=True)\n",
        "                input_batch = preprocess_on_gpu(batch_gpu)\n",
        "\n",
        "                with torch.amp.autocast('cuda'):\n",
        "                    with torch.no_grad():\n",
        "                        v_embed = model.compute_video(input_batch)\n",
        "\n",
        "                # Salva risultato nel buffer specifico del video\n",
        "                results_buffer[vpath].append(v_embed.cpu().numpy().astype(np.float16))\n",
        "\n",
        "            global_queue.task_done()\n",
        "\n",
        "        except queue.Empty:\n",
        "            print(\"âš ï¸ Warning: GPU Queue empty for 60s. Workers might be stuck.\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Critical Error in Main Loop: {e}\")\n",
        "            break\n",
        "\n",
        "    executor.shutdown()\n",
        "    print(\"ðŸŽ‰ Tutti i video elaborati.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4Gyb1yOzmASW",
        "outputId": "5dd2536f-3bf8-4af6-81d7-f0c369248f82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
            "Installing collected packages: einops\n",
            "Successfully installed einops-0.8.1\n",
            "Collecting humanize\n",
            "  Downloading humanize-4.15.0-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting psutil\n",
            "  Downloading psutil-7.1.3-cp36-abi3-manylinux2010_x86_64.manylinux_2_12_x86_64.manylinux_2_28_x86_64.whl.metadata (23 kB)\n",
            "Downloading humanize-4.15.0-py3-none-any.whl (132 kB)\n",
            "Downloading psutil-7.1.3-cp36-abi3-manylinux2010_x86_64.manylinux_2_12_x86_64.manylinux_2_28_x86_64.whl (263 kB)\n",
            "Installing collected packages: psutil, humanize\n",
            "Successfully installed humanize-4.15.0 psutil-7.1.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "psutil"
                ]
              },
              "id": "5daa85df7dc5455dae333214d8923329"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ffmpeg-python\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting future (from ffmpeg-python)\n",
            "  Downloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Downloading future-1.0.0-py3-none-any.whl (491 kB)\n",
            "Installing collected packages: future, ffmpeg-python\n",
            "Successfully installed ffmpeg-python-0.2.0 future-1.0.0\n",
            "Collecting av\n",
            "  Downloading av-16.0.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
            "Collecting dominate\n",
            "  Downloading dominate-2.9.1-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.8.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
            "Collecting tensorboardx\n",
            "  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: numpy>=1.24.1 in /usr/local/lib/python3.11/site-packages (from scikit-learn) (2.2.6)\n",
            "Collecting scipy>=1.10.0 (from scikit-learn)\n",
            "  Downloading scipy-1.16.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
            "Collecting joblib>=1.3.0 (from scikit-learn)\n",
            "  Downloading joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting threadpoolctl>=3.2.0 (from scikit-learn)\n",
            "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/site-packages (from tensorboardx) (24.2)\n",
            "Collecting protobuf>=3.20 (from tensorboardx)\n",
            "  Downloading protobuf-6.33.2-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
            "Downloading av-16.0.1-cp311-cp311-manylinux_2_28_x86_64.whl (40.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.2/40.2 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dominate-2.9.1-py2.py3-none-any.whl (29 kB)\n",
            "Downloading scikit_learn-1.8.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (9.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m154.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n",
            "Downloading joblib-1.5.3-py3-none-any.whl (309 kB)\n",
            "Downloading protobuf-6.33.2-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
            "Downloading scipy-1.16.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m35.9/35.9 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: threadpoolctl, scipy, protobuf, joblib, dominate, av, tensorboardx, scikit-learn\n",
            "Successfully installed av-16.0.1 dominate-2.9.1 joblib-1.5.3 protobuf-6.33.2 scikit-learn-1.8.0 scipy-1.16.3 tensorboardx-2.6.4 threadpoolctl-3.6.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "65242db909d84355adee1c6c5773a2bc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ GPU: Tesla T4\n",
            "âš¡ MULTI-VIDEO MODE: 3 Workers Paralleli | Batch 64\n",
            "Costruzione modello FrozenInTime su cuda...\n",
            "config.json: 100% 570/570 [00:00<00:00, 3.95MB/s]\n",
            "model.safetensors: 100% 440M/440M [00:03<00:00, 139MB/s]\n",
            "/content/EgoVLP/model/model.py:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vit_model = torch.load(\"pretrained/jx_vit_base_p16_224-80ecf9dd.pth\", map_location=\"cpu\")\n",
            "######USING ATTENTION STYLE:  frozen-in-time\n",
            "Trovati 48 video totali. Da elaborare: 48\n",
            "â–¶ï¸ Start: 29_37_360p_224.mp4\n",
            "â–¶ï¸ Start: 28_50_360p_224.mp4\n",
            "â–¶ï¸ Start: 28_16_360p_224.mp4\n",
            "âœ… Saved: 29_37_360p_224.mp4 (358.1s - 61 fps) | Progress: 1/48\n",
            "â–¶ï¸ Start: 28_45_360p_224.mp4\n",
            "âœ… Saved: 28_50_360p_224.mp4 (365.5s - 62 fps) | Progress: 2/48\n",
            "â–¶ï¸ Start: 28_49_360p_224.mp4\n",
            "âœ… Saved: 28_16_360p_224.mp4 (453.0s - 72 fps) | Progress: 3/48\n",
            "â–¶ï¸ Start: 28_14_360p_224.mp4\n",
            "âœ… Saved: 28_45_360p_224.mp4 (407.9s - 62 fps) | Progress: 4/48\n",
            "â–¶ï¸ Start: 28_42_360p_224.mp4\n",
            "âœ… Saved: 28_49_360p_224.mp4 (550.6s - 63 fps) | Progress: 5/48\n",
            "â–¶ï¸ Start: 29_34_360p_224.mp4\n",
            "âœ… Saved: 28_14_360p_224.mp4 (513.8s - 64 fps) | Progress: 6/48\n",
            "â–¶ï¸ Start: 28_10_360p_224.mp4\n",
            "âœ… Saved: 29_34_360p_224.mp4 (342.7s - 58 fps) | Progress: 7/48\n",
            "â–¶ï¸ Start: 29_22_360p_224.mp4\n",
            "âœ… Saved: 28_42_360p_224.mp4 (493.7s - 63 fps) | Progress: 8/48\n",
            "â–¶ï¸ Start: 29_15_360p_224.mp4\n",
            "âœ… Saved: 28_10_360p_224.mp4 (513.6s - 65 fps) | Progress: 9/48\n",
            "â–¶ï¸ Start: 29_6_360p_224.mp4\n",
            "âœ… Saved: 29_15_360p_224.mp4 (498.7s - 60 fps) | Progress: 10/48\n",
            "â–¶ï¸ Start: 27_29_360p_224.mp4\n",
            "âœ… Saved: 29_22_360p_224.mp4 (602.5s - 64 fps) | Progress: 11/48\n",
            "â–¶ï¸ Start: 28_3_360p_224.mp4\n",
            "âœ… Saved: 29_6_360p_224.mp4 (602.3s - 64 fps) | Progress: 12/48\n",
            "â–¶ï¸ Start: 29_17_360p_224.mp4\n",
            "âœ… Saved: 27_29_360p_224.mp4 (379.2s - 62 fps) | Progress: 13/48\n",
            "â–¶ï¸ Start: 27_13_360p_224.mp4\n",
            "âœ… Saved: 28_3_360p_224.mp4 (492.9s - 62 fps) | Progress: 14/48\n",
            "â–¶ï¸ Start: 28_29_360p_224.mp4\n",
            "âœ… Saved: 29_17_360p_224.mp4 (522.2s - 63 fps) | Progress: 15/48\n",
            "â–¶ï¸ Start: 28_26_360p_224.mp4\n",
            "âœ… Saved: 28_29_360p_224.mp4 (300.0s - 58 fps) | Progress: 16/48\n",
            "â–¶ï¸ Start: 27_15_360p_224.mp4\n",
            "âœ… Saved: 27_13_360p_224.mp4 (537.2s - 64 fps) | Progress: 17/48\n",
            "â–¶ï¸ Start: 29_29_360p_224.mp4\n",
            "âœ… Saved: 28_26_360p_224.mp4 (380.4s - 64 fps) | Progress: 18/48\n",
            "â–¶ï¸ Start: 27_17_360p_224.mp4\n",
            "âœ… Saved: 27_15_360p_224.mp4 (336.9s - 59 fps) | Progress: 19/48\n",
            "â–¶ï¸ Start: 28_44_360p_224.mp4\n",
            "âœ… Saved: 29_29_360p_224.mp4 (323.1s - 60 fps) | Progress: 20/48\n",
            "â–¶ï¸ Start: 29_45_360p_224.mp4\n",
            "âœ… Saved: 28_44_360p_224.mp4 (410.2s - 59 fps) | Progress: 21/48\n",
            "â–¶ï¸ Start: 27_37_360p_224.mp4\n",
            "âœ… Saved: 27_17_360p_224.mp4 (417.0s - 66 fps) | Progress: 22/48\n",
            "â–¶ï¸ Start: 29_19_360p_224.mp4\n",
            "âœ… Saved: 29_45_360p_224.mp4 (475.2s - 67 fps) | Progress: 23/48\n",
            "â–¶ï¸ Start: 29_28_360p_224.mp4\n",
            "âœ… Saved: 27_37_360p_224.mp4 (268.7s - 58 fps) | Progress: 24/48\n",
            "â–¶ï¸ Start: 28_21_360p_224.mp4\n",
            "âœ… Saved: 29_28_360p_224.mp4 (360.8s - 57 fps) | Progress: 25/48\n",
            "â–¶ï¸ Start: 28_24_360p_224.mp4\n",
            "âœ… Saved: 29_19_360p_224.mp4 (463.3s - 68 fps) | Progress: 26/48\n",
            "â–¶ï¸ Start: 28_28_360p_224.mp4\n",
            "âœ… Saved: 28_21_360p_224.mp4 (455.7s - 63 fps) | Progress: 27/48\n",
            "â–¶ï¸ Start: 27_49_360p_224.mp4\n",
            "âœ… Saved: 28_28_360p_224.mp4 (395.5s - 59 fps) | Progress: 28/48\n",
            "â–¶ï¸ Start: 27_26_360p_224.mp4\n",
            "âœ… Saved: 28_24_360p_224.mp4 (478.5s - 67 fps) | Progress: 29/48\n",
            "â–¶ï¸ Start: 29_48_360p_224.mp4\n",
            "âœ… Saved: 27_49_360p_224.mp4 (260.9s - 61 fps) | Progress: 30/48\n",
            "â–¶ï¸ Start: 29_32_360p_224.mp4\n",
            "âœ… Saved: 29_48_360p_224.mp4 (298.9s - 58 fps) | Progress: 31/48\n",
            "â–¶ï¸ Start: 29_129_360p_224.mp4\n",
            "âœ… Saved: 27_26_360p_224.mp4 (351.0s - 63 fps) | Progress: 32/48\n",
            "â–¶ï¸ Start: 29_35_360p_224.mp4\n",
            "âœ… Saved: 29_32_360p_224.mp4 (415.4s - 66 fps) | Progress: 33/48\n",
            "â–¶ï¸ Start: 28_7_360p_224.mp4\n",
            "âœ… Saved: 29_35_360p_224.mp4 (324.7s - 59 fps) | Progress: 34/48\n",
            "â–¶ï¸ Start: 29_49_360p_224.mp4\n",
            "âœ… Saved: 29_129_360p_224.mp4 (434.7s - 64 fps) | Progress: 35/48\n",
            "â–¶ï¸ Start: 27_31_360p_224.mp4\n",
            "âœ… Saved: 29_49_360p_224.mp4 (385.0s - 60 fps) | Progress: 36/48\n",
            "â–¶ï¸ Start: 29_5_360p_224.mp4\n",
            "âœ… Saved: 28_7_360p_224.mp4 (520.9s - 63 fps) | Progress: 37/48\n",
            "â–¶ï¸ Start: 27_18_360p_224.mp4\n",
            "âœ… Saved: 27_31_360p_224.mp4 (329.0s - 67 fps) | Progress: 38/48\n",
            "â–¶ï¸ Start: 27_34_360p_224.mp4\n",
            "âœ… Saved: 27_34_360p_224.mp4 (349.3s - 54 fps) | Progress: 39/48\n",
            "â–¶ï¸ Start: 29_18_360p_224.mp4\n",
            "âœ… Saved: 27_18_360p_224.mp4 (429.4s - 62 fps) | Progress: 40/48\n",
            "â–¶ï¸ Start: 28_25_360p_224.mp4\n",
            "âœ… Saved: 29_5_360p_224.mp4 (467.3s - 69 fps) | Progress: 41/48\n",
            "â–¶ï¸ Start: 28_38_360p_224.mp4\n",
            "âœ… Saved: 29_18_360p_224.mp4 (423.1s - 61 fps) | Progress: 42/48\n",
            "â–¶ï¸ Start: 29_7_360p_224.mp4\n",
            "âœ… Saved: 28_38_360p_224.mp4 (359.0s - 57 fps) | Progress: 43/48\n",
            "â–¶ï¸ Start: 27_45_360p_224.mp4\n",
            "âœ… Saved: 28_25_360p_224.mp4 (449.0s - 70 fps) | Progress: 44/48\n",
            "â–¶ï¸ Start: 28_2_360p_224.mp4\n",
            "âœ… Saved: 27_45_360p_224.mp4 (502.5s - 61 fps) | Progress: 45/48\n",
            "â–¶ï¸ Start: 27_38_360p_224.mp4\n",
            "âœ… Saved: 28_2_360p_224.mp4 (460.3s - 58 fps) | Progress: 46/48\n",
            "âœ… Saved: 29_7_360p_224.mp4 (602.0s - 70 fps) | Progress: 47/48\n",
            "âœ… Saved: 27_38_360p_224.mp4 (153.6s - 114 fps) | Progress: 48/48\n",
            "ðŸŽ‰ Tutti i video elaborati.\n"
          ]
        }
      ],
      "source": [
        "!pip install einops\n",
        "!pip install humanize psutil\n",
        "!pip install ffmpeg-python\n",
        "!pip install av dominate scikit-learn tensorboardx\n",
        "\n",
        "# Crea cartella output\n",
        "!mkdir -p data/features\n",
        "\n",
        "# Esegui lo script\n",
        "# Assicurati che VIDEO_ROOT punti alla cartella dove ci sono effettivamente i file .mp4\n",
        "!python extract_custom.py \\\n",
        "  --video_dir \"data/videos/captain_cook_4d_gopro_resized_8\" \\\n",
        "  --output_dir \"data/features\" \\\n",
        "  --weights \"pretrained/model.pth\" \\\n",
        "  --batch_size 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gMpkKYBmA0X",
        "outputId": "2615b07a-f5a1-4793-ca7d-7131fb18be03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creazione archivio zip...\n",
            "Upload su Drive: /content/drive/MyDrive/MistakeDetection/egovlp_8.zip\n",
            "âœ… Completato!\n"
          ]
        }
      ],
      "source": [
        "print(\"Creazione archivio zip...\")\n",
        "!zip -r -q egovlp_features.zip data/features\n",
        "\n",
        "print(f\"Upload su Drive: {OUTPUT_ZIP_PATH}\")\n",
        "shutil.copy(\"egovlp_features.zip\", OUTPUT_ZIP_PATH)\n",
        "print(\"âœ… Completato!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PARTE 2: TEXT ENCODER"
      ],
      "metadata": {
        "id": "InfYsExY1Xuy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ðŸš€ Setup Dipendenze Substep 3 (Versione Pulita)\n",
        "import sys\n",
        "import os\n",
        "import traceback\n",
        "\n",
        "print(\"ðŸ”„ Preparazione ambiente per Text Encoder...\")\n",
        "\n",
        "# 1. BINARI DI SISTEMA (Video/Audio)\n",
        "#    Usiamo Mamba per installare le librerie C++ sottostanti (ffmpeg, av, decord)\n",
        "print(\"ðŸ”§ Installazione librerie di sistema (av, decord, ffmpeg)...\")\n",
        "get_ipython().system('mamba install -y av decord ffmpeg -c conda-forge')\n",
        "\n",
        "# 2. PACCHETTI PYTHON\n",
        "#    Installiamo direttamente la lista di pacchetti necessari\n",
        "print(\"ðŸ”§ Installazione pacchetti Python...\")\n",
        "get_ipython().system(f'{sys.executable} -m pip install av decord ftfy regex tqdm simplejson pandas transformers ffmpeg-python')\n",
        "\n",
        "print(\"âœ… Installazioni completate.\")\n",
        "\n",
        "# 3. CONFIGURAZIONE PATH\n",
        "repo_path = \"/content/EgoVLP\"\n",
        "if os.path.exists(repo_path):\n",
        "    if repo_path not in sys.path:\n",
        "        sys.path.append(repo_path)\n",
        "    os.chdir(repo_path)\n",
        "    print(f\"ðŸ“‚ Directory di lavoro: {os.getcwd()}\")\n",
        "else:\n",
        "    print(f\"âš ï¸ Cartella {repo_path} non trovata. Assicurati di aver scaricato la repo!\")\n",
        "\n",
        "# 4. TEST IMPORTAZIONE MODELLO\n",
        "print(\"\\nâš™ï¸ Test Finale Importazione...\")\n",
        "try:\n",
        "    import av\n",
        "    import decord\n",
        "    from model.model import FrozenInTime\n",
        "    print(\"\\nðŸŽ‰ TUTTO OK! Il modello Ã¨ pronto.\")\n",
        "    print(\"   Puoi procedere con le celle 2 (Caricamento Pesi) e 3 (Generazione).\")\n",
        "except ImportError as e:\n",
        "    print(f\"\\nâŒ ERRORE: {e}\")\n",
        "    print(\"âš ï¸ Se l'errore riguarda 'av' o 'decord', riavvia la sessione (Runtime -> Restart Session) e riesegui questa cella.\")"
      ],
      "metadata": {
        "id": "dKDKEi02GIkS",
        "outputId": "0d6911ea-33bf-4630-c359-0fda0291524f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”„ Preparazione ambiente per Text Encoder...\n",
            "ðŸ”§ Installazione librerie di sistema (av, decord, ffmpeg)...\n",
            "\n",
            "Looking for: ['av', 'decord', 'ffmpeg']\n",
            "\n",
            "conda-forge/linux-64                                        Using cache\n",
            "conda-forge/noarch                                          Using cache\n",
            "Your pinning does not match what's currently installed. Please remove the pin and fix your installation\n",
            "  Pin: python=3.12\n",
            "  Currently installed: conda-forge/linux-64::python==3.11.11=h9e4cc4f_1_cpython\n",
            "ðŸ”§ Installazione pacchetti Python...\n",
            "Requirement already satisfied: av in /usr/local/lib/python3.12/dist-packages (16.0.1)\n",
            "Requirement already satisfied: decord in /usr/local/lib/python3.12/dist-packages (0.6.0)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.12/dist-packages (6.3.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: simplejson in /usr/local/lib/python3.12/dist-packages (3.20.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: ffmpeg-python in /usr/local/lib/python3.12/dist-packages (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from decord) (2.0.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy) (0.2.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.12/dist-packages (from ffmpeg-python) (1.0.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "âœ… Installazioni completate.\n",
            "ðŸ“‚ Directory di lavoro: /content/EgoVLP\n",
            "\n",
            "âš™ï¸ Test Finale Importazione...\n",
            "\n",
            "ðŸŽ‰ TUTTO OK! Il modello Ã¨ pronto.\n",
            "   Puoi procedere con le celle 2 (Caricamento Pesi) e 3 (Generazione).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title â¬‡ï¸ Download Pesi Modello (Esegui prima di caricare il modello)\n",
        "import os\n",
        "import gdown\n",
        "import urllib.request\n",
        "\n",
        "# Crea cartella pesi\n",
        "if not os.path.exists(\"pretrained\"):\n",
        "    os.makedirs(\"pretrained\")\n",
        "\n",
        "print(\"ðŸ“¦ Controllo file dei pesi...\")\n",
        "\n",
        "# --- 1. DOWNLOAD BACKBONE ViT (Quello che ha causato il tuo errore) ---\n",
        "# Questo file Ã¨ richiesto dalla classe SpaceTimeTransformer per inizializzarsi\n",
        "vit_path = \"pretrained/jx_vit_base_p16_224-80ecf9dd.pth\"\n",
        "vit_url = \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_p16_224-80ecf9dd.pth\"\n",
        "\n",
        "if not os.path.exists(vit_path):\n",
        "    print(f\"â¬‡ï¸ Scaricando backbone ViT (jx_vit_base_p16_224-80ecf9dd.pth)...\")\n",
        "    try:\n",
        "        urllib.request.urlretrieve(vit_url, vit_path)\n",
        "        print(\"âœ… Backbone ViT scaricato.\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Errore download ViT: {e}\")\n",
        "else:\n",
        "    print(\"âœ… Backbone ViT giÃ  presente.\")\n",
        "\n",
        "# --- 2. DOWNLOAD CHECKPOINT EGOVLP (model.pth) ---\n",
        "model_path = \"pretrained/model.pth\"\n",
        "if not os.path.exists(model_path):\n",
        "    print(f\"â¬‡ï¸ Scaricando checkpoint EgoVLP (model.pth)...\")\n",
        "    # ID del file drive ufficiale EgoVLP\n",
        "    file_id = '1-cP3Gcg0NGDcMZalgJ_615BQdbFIbcj7'\n",
        "    url = f'https://drive.google.com/uc?id={file_id}'\n",
        "\n",
        "    try:\n",
        "        gdown.download(url, model_path, quiet=False)\n",
        "        print(\"âœ… Checkpoint EgoVLP scaricato.\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Errore download model.pth: {e}\")\n",
        "else:\n",
        "    print(\"âœ… Checkpoint EgoVLP giÃ  presente.\")\n",
        "\n",
        "print(\"\\nðŸŽ‰ Pesi pronti! Ora puoi eseguire la Cella 2.\")"
      ],
      "metadata": {
        "id": "8-eb-zZ9LMYh",
        "outputId": "7a8874dc-0a23-4b95-c71f-e991fcce026d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“¦ Controllo file dei pesi...\n",
            "â¬‡ï¸ Scaricando backbone ViT (jx_vit_base_p16_224-80ecf9dd.pth)...\n",
            "âœ… Backbone ViT scaricato.\n",
            "â¬‡ï¸ Scaricando checkpoint EgoVLP (model.pth)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1-cP3Gcg0NGDcMZalgJ_615BQdbFIbcj7\n",
            "From (redirected): https://drive.google.com/uc?id=1-cP3Gcg0NGDcMZalgJ_615BQdbFIbcj7&confirm=t&uuid=bc3f90b1-b2a9-4b0a-9f09-756bde454c9f\n",
            "To: /content/EgoVLP/pretrained/model.pth\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.17G/2.17G [00:22<00:00, 95.2MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Checkpoint EgoVLP scaricato.\n",
            "\n",
            "ðŸŽ‰ Pesi pronti! Ora puoi eseguire la Cella 2.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. Inizializzazione Modello (Fix LOCAL_RANK)\n",
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# --- FIX CRITICO PER EGOVLP ---\n",
        "# Simuliamo le variabili d'ambiente distribuite per evitare l'errore\n",
        "os.environ[\"LOCAL_RANK\"] = \"0\"\n",
        "os.environ[\"RANK\"] = \"0\"\n",
        "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
        "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
        "os.environ[\"MASTER_PORT\"] = \"12345\"\n",
        "# ------------------------------\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"âš™ï¸ Setup su device: {device}\")\n",
        "\n",
        "# 1. Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "# 2. Configurazione\n",
        "config = {\n",
        "    \"video_params\": {\n",
        "        \"model\": \"SpaceTimeTransformer\",\n",
        "        \"arch_config\": \"base_patch16_224\",\n",
        "        \"num_frames\": 16,\n",
        "        \"pretrained\": True,\n",
        "        \"time_init\": \"zeros\",\n",
        "        \"vit_ckpt\": \"pretrained/jx_vit_base_p16_224-80ecf9dd.pth\"\n",
        "    },\n",
        "    \"text_params\": {\n",
        "        \"model\": \"distilbert-base-uncased\",\n",
        "        \"pretrained\": True,\n",
        "        \"input\": \"text\"\n",
        "    },\n",
        "    \"projection\": \"minimal\",\n",
        "    \"load_checkpoint\": \"pretrained/model.pth\"\n",
        "}\n",
        "\n",
        "# 3. Caricamento Pesi\n",
        "print(\"âš™ï¸ Caricamento pesi modello...\")\n",
        "try:\n",
        "    # Importiamo qui per essere sicuri che prenda le env vars settate sopra\n",
        "    from model.model import FrozenInTime\n",
        "\n",
        "    model = FrozenInTime(**config)\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    print(\"âœ… SUCCESSO! Modello EgoVLP caricato correttamente.\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Errore caricamento: {e}\")\n",
        "    # Stampa dettagliata per debug\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeIqu2iJ1csH",
        "outputId": "6988b995-fddc-474f-afd3-e1eb2511aa81"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âš™ï¸ Setup su device: cpu\n",
            "âš™ï¸ Caricamento pesi modello...\n",
            "######USING ATTENTION STYLE:  frozen-in-time\n",
            "âŒ Errore caricamento: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-4118718561.py\", line 46, in <cell line: 0>\n",
            "    model = FrozenInTime(**config)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/EgoVLP/model/model.py\", line 91, in __init__\n",
            "    checkpoint = torch.load(load_checkpoint, map_location='cuda:{}'.format(local_rank))\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 1521, in load\n",
            "    return _load(\n",
            "           ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 2122, in _load\n",
            "    result = unpickler.load()\n",
            "             ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/_weights_only_unpickler.py\", line 535, in load\n",
            "    self.append(self.persistent_load(pid))\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 2086, in persistent_load\n",
            "    typed_storage = load_tensor(\n",
            "                    ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 2052, in load_tensor\n",
            "    wrap_storage = restore_location(storage, location)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 1859, in restore_location\n",
            "    return default_restore_location(storage, map_location)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 698, in default_restore_location\n",
            "    result = fn(storage, location)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 636, in _deserialize\n",
            "    device = _validate_device(location, backend_name)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 605, in _validate_device\n",
            "    raise RuntimeError(\n",
            "RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. Generazione Text Embeddings (Esecuzione Finale)\n",
        "import numpy as np\n",
        "import json\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- CONFIGURAZIONE ---\n",
        "INPUT_GRAPHS_DIR = \"/content/drive/MyDrive/MistakeDetection/task_graphs\" # <--- VERIFICA QUESTO PERCORSO\n",
        "OUTPUT_TEXT_EMB_DIR = \"/content/drive/MyDrive/MistakeDetection/text_embeddings\"\n",
        "\n",
        "# Funzione helper\n",
        "def encode_descriptions(descriptions, model, tokenizer, device):\n",
        "    tokens = tokenizer(descriptions, padding=True, truncation=True, max_length=32, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        text_embeddings = model.compute_text(tokens)\n",
        "    return text_embeddings.cpu().numpy()\n",
        "\n",
        "# Creazione cartella output\n",
        "if not os.path.exists(OUTPUT_TEXT_EMB_DIR):\n",
        "    os.makedirs(OUTPUT_TEXT_EMB_DIR)\n",
        "\n",
        "# Trova file\n",
        "json_files = glob.glob(os.path.join(INPUT_GRAPHS_DIR, \"*.json\"))\n",
        "print(f\"ðŸ“‚ Trovati {len(json_files)} file JSON in: {INPUT_GRAPHS_DIR}\")\n",
        "\n",
        "if len(json_files) == 0:\n",
        "    print(\"âš ï¸ NESSUN FILE TROVATO! Controlla il percorso INPUT_GRAPHS_DIR.\")\n",
        "else:\n",
        "    count = 0\n",
        "    for json_path in tqdm(json_files, desc=\"Processando Ricette\"):\n",
        "        try:\n",
        "            filename = os.path.basename(json_path)\n",
        "            recipe_name = filename.replace(\".json\", \"\")\n",
        "            save_path = os.path.join(OUTPUT_TEXT_EMB_DIR, f\"{recipe_name}_text.npz\")\n",
        "\n",
        "            with open(json_path, 'r') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            if \"steps\" not in data: continue\n",
        "\n",
        "            steps_dict = data[\"steps\"]\n",
        "            # Ordina per ID numerico\n",
        "            sorted_ids = sorted(steps_dict.keys(), key=lambda x: int(x) if x.isdigit() else x)\n",
        "\n",
        "            step_ids = []\n",
        "            descriptions = []\n",
        "\n",
        "            for sid in sorted_ids:\n",
        "                desc = steps_dict[sid]\n",
        "                # Gestione formato stringa o dizionario\n",
        "                desc_text = desc['description'] if isinstance(desc, dict) and 'description' in desc else str(desc)\n",
        "                step_ids.append(sid)\n",
        "                descriptions.append(desc_text.strip())\n",
        "\n",
        "            if not descriptions: continue\n",
        "\n",
        "            # Encoding\n",
        "            emb_matrix = encode_descriptions(descriptions, model, tokenizer, device)\n",
        "\n",
        "            # Salvataggio\n",
        "            np.savez_compressed(save_path, features=emb_matrix, step_ids=step_ids, text=descriptions)\n",
        "            count += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Errore su {filename}: {e}\")\n",
        "\n",
        "    print(f\"\\nðŸŽ‰ COMPLETATO! Generati {count} file .npz in {OUTPUT_TEXT_EMB_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRnd0_3d1jgJ",
        "outputId": "3ee6c062-6600-490b-8e6e-c59caeb70a97"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âš™ï¸ Setup su device: cpu\n",
            "âš™ï¸ Caricamento pesi modello...\n",
            "######USING ATTENTION STYLE:  frozen-in-time\n",
            "âŒ Errore caricamento: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-4118718561.py\", line 46, in <cell line: 0>\n",
            "    model = FrozenInTime(**config)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/EgoVLP/model/model.py\", line 91, in __init__\n",
            "    checkpoint = torch.load(load_checkpoint, map_location='cuda:{}'.format(local_rank))\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 1521, in load\n",
            "    return _load(\n",
            "           ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 2122, in _load\n",
            "    result = unpickler.load()\n",
            "             ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/_weights_only_unpickler.py\", line 535, in load\n",
            "    self.append(self.persistent_load(pid))\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 2086, in persistent_load\n",
            "    typed_storage = load_tensor(\n",
            "                    ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 2052, in load_tensor\n",
            "    wrap_storage = restore_location(storage, location)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 1859, in restore_location\n",
            "    return default_restore_location(storage, map_location)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 698, in default_restore_location\n",
            "    result = fn(storage, location)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 636, in _deserialize\n",
            "    device = _validate_device(location, backend_name)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 605, in _validate_device\n",
            "    raise RuntimeError(\n",
            "RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. Generazione Text Embeddings\n",
        "# --- CONFIGURAZIONE ---\n",
        "# Cartella INPUT: Dove sono i tuoi file .json (es. blenderbananapancakes.json)\n",
        "INPUT_GRAPHS_DIR = \"/content/drive/MyDrive/MistakeDetection/task_graphs\"\n",
        "\n",
        "# Cartella OUTPUT: Dove salvare gli embedding testuali\n",
        "OUTPUT_TEXT_EMB_DIR = \"/content/drive/MyDrive/MistakeDetection/text_embeddings\"\n",
        "# ----------------------\n",
        "\n",
        "# Funzione helper per l'encoding\n",
        "def encode_descriptions(descriptions, model, tokenizer, device):\n",
        "    # Tokenizzazione con padding/truncation\n",
        "    tokens = tokenizer(\n",
        "        descriptions,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=32, # EgoVLP Ã¨ addestrato su clip brevi, 32-64 token bastano\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # model.compute_text restituisce l'embedding proiettato (dimensione 256)\n",
        "        text_embeddings = model.compute_text(tokens)\n",
        "    return text_embeddings.cpu().numpy()\n",
        "\n",
        "# Creazione cartella output\n",
        "os.makedirs(OUTPUT_TEXT_EMB_DIR, exist_ok=True)\n",
        "\n",
        "# Trova tutti i file JSON\n",
        "json_files = glob.glob(os.path.join(INPUT_GRAPHS_DIR, \"*.json\"))\n",
        "print(f\"ðŸ“‚ Trovati {len(json_files)} file JSON in: {INPUT_GRAPHS_DIR}\")\n",
        "\n",
        "if len(json_files) == 0:\n",
        "    print(\"âŒ ERRORE: Nessun file trovato. Controlla il percorso INPUT_GRAPHS_DIR!\")\n",
        "else:\n",
        "    count = 0\n",
        "    for json_path in tqdm(json_files, desc=\"Processando Ricette\"):\n",
        "        filename = os.path.basename(json_path)\n",
        "        recipe_name = filename.replace(\".json\", \"\")\n",
        "        save_path = os.path.join(OUTPUT_TEXT_EMB_DIR, f\"{recipe_name}_text.npz\")\n",
        "\n",
        "        try:\n",
        "            with open(json_path, 'r') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            # Estrazione Steps dal JSON\n",
        "            # Ci aspettiamo una struttura: { \"steps\": { \"0\": \"desc...\", \"1\": \"desc...\" } }\n",
        "            if \"steps\" not in data:\n",
        "                print(f\"â© Salto {filename}: chiave 'steps' mancante.\")\n",
        "                continue\n",
        "\n",
        "            steps_dict = data[\"steps\"]\n",
        "\n",
        "            # Ordiniamo gli step per ID (importante per mantenere la coerenza col grafo)\n",
        "            sorted_ids = sorted(steps_dict.keys(), key=lambda x: int(x) if x.isdigit() else x)\n",
        "\n",
        "            step_ids = []\n",
        "            descriptions = []\n",
        "\n",
        "            for sid in sorted_ids:\n",
        "                desc = steps_dict[sid]\n",
        "                # A volte la descrizione Ã¨ dentro un oggetto, a volte Ã¨ stringa diretta\n",
        "                desc_text = desc['description'] if isinstance(desc, dict) and 'description' in desc else str(desc)\n",
        "\n",
        "                # Rimuoviamo eventuali ID hardcodati nel testo (es. \"1. Wash\") per pulizia\n",
        "                desc_text = desc_text.strip()\n",
        "\n",
        "                step_ids.append(sid)\n",
        "                descriptions.append(desc_text)\n",
        "\n",
        "            if not descriptions:\n",
        "                continue\n",
        "\n",
        "            # Generazione Embedding (Text Encoder)\n",
        "            emb_matrix = encode_descriptions(descriptions, model, tokenizer, device)\n",
        "\n",
        "            # Salvataggio .npz\n",
        "            # Salviamo tutto ciÃ² che serve per il matching successivo\n",
        "            np.savez_compressed(\n",
        "                save_path,\n",
        "                features=emb_matrix,    # Matrice (Numero_Step x 256)\n",
        "                step_ids=step_ids,      # Lista ID (es. ['0', '1', '2'])\n",
        "                text=descriptions       # Testo originale (per debug e visualizzazione)\n",
        "            )\n",
        "            count += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Errore su {filename}: {e}\")\n",
        "\n",
        "    print(f\"\\nðŸŽ‰ COMPLETATO!\")\n",
        "    print(f\"   Generati {count} file di embedding testuali.\")\n",
        "    print(f\"   Salvato in: {OUTPUT_TEXT_EMB_DIR}\")"
      ],
      "metadata": {
        "id": "jGsaZfKZ1rzq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}